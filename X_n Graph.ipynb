{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.spatial import distance_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from fermat import Fermat\n",
    "from sklearn.datasets import load_iris\n",
    "import networkx as nx\n",
    "import igraph as ig\n",
    "import neo4j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = load_iris(return_X_y=True, as_frame=True)\n",
    "N = len(X)  # # observaciones\n",
    "K = len(set(y))  # # clases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.complete_graph(len(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.set_node_attributes(G, X.to_dict(orient=\"index\"))\n",
    "nx.set_node_attributes(G, y, name=\"y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha, path_method = 2, \"FW\"\n",
    "fermat = Fermat(alpha=alpha, path_method=path_method)\n",
    "dists = distance_matrix(X, X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%timeit\n",
    "fermat.fit(dists)\n",
    "fdists = fermat.get_distances()\n",
    "nx.set_edge_attributes(\n",
    "    G, {(i, j): fdists[i, j] for i in range(N) for j in range(i)}, name=\"fermat_dist\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (i, j), d in nx.get_edge_attributes(G, \"fermat_dist\").items():\n",
    "    assert fdists[i, j] == d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reverse check\n",
    "assert np.all(\n",
    "    fdists[i, j] == d\n",
    "    for (i, j), d\n",
    "    in nx.get_edge_attributes(G, \"fermat_dist\").items()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%timeit\n",
    "nx.set_edge_attributes(\n",
    "    G, {(i, j): dists[i, j] ** alpha for i in range(N) for j in range(i)}, name=\"alpha_dist\"\n",
    ")\n",
    "nxdists = nx.shortest_paths.floyd_warshall_numpy(G, weight=\"alpha_dist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aparentemente las referencias locales (`nxdists`) dentro de %%timeit no perduran\n",
    "nxdists = nx.shortest_paths.floyd_warshall_numpy(G, weight=\"alpha_dist\")\n",
    "np.allclose(fdists, nxdists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff = np.abs(nxdists - fdists)\n",
    "plt.imshow(diff, cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_idx = diff.argmax()\n",
    "max_i, max_j = max_idx // N, max_idx % N\n",
    "diff[max_i, max_j]\n",
    "pd.concat([X.loc[[max_i, max_j]], y[[max_i, max_j]]], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Son dos observaciones idénticas! Probablemente uno de los dos algoritmos no está permitiendo caminos mínimos de longitud 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nxdists[max_i, max_j], fdists[max_i, max_j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdists[max_i, max_i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Curioso. La distancia de `max_i` a sí mismo es 0, pero no así a `max_j`. Asumimos que `Fermat` llegó a la distancia `fdists[max_i, max_j]` saltando al NN(1) de `max_i` que _no está en su mismo lugar_ (como `max_j`), y volviendo a `max_i`, así que conjeturamos que la distancia de `max_i` a NN(1) _según Fermat_, tiene que ser `fdists[max_i, max_j] / 2`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn1 = fdists[max_i].argsort()[1]  # el argmin absoluto es `max_i`, me interesa el segundo\n",
    "assert fdists[max_i, nn1] == fdists[max_i, max_j] / 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Era verdad! Qué cagada, a esta altura más bien tendemos a desconfiar de `Fermat.fit()`. Habremos de verificar que las `N` observaciones de X sean únicas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 9))\n",
    "nx.draw_networkx(\n",
    "    G,\n",
    "    width=1e-2 * np.sqrt([G.edges[e].get(\"alpha_dist\") for e in G.edges]),\n",
    "    node_color=[\n",
    "        {0: \"blue\", 1: \"green\", 2: \"red\"}[G.nodes[n].get(\"y\")] for n in G.nodes\n",
    "    ],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KernelDensity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import random as rnd\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = rnd.rand(100, 3)\n",
    "X.sort(axis=1)\n",
    "X = np.column_stack([np.zeros_like(X[:, 0]), X, np.ones_like(X[:, 0])])\n",
    "x = X[0]\n",
    "x, X[:5,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fermat_dist(x, alpha=1):\n",
    "    \"\"\" Fermat alpha-distance between `x_0` and `x_k`, in the line graph with nodes at `x = (x_1, ..., x_k)`.\"\"\"\n",
    "    return ((x[1:] - x[:-1]) ** alpha).sum()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.apply_along_axis(fermat_dist, axis=1, arr=X, alpha=3).mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size = 1000\n",
    "scales = [1/10, 1/2, 1, 2, 10]\n",
    "alphas = np.linspace(1, 4, 7)\n",
    "ks = [1, 2, 5, 10]\n",
    "results = []\n",
    "for k in ks:\n",
    "    X = rnd.rand(sample_size, k)\n",
    "    X.sort(axis=1)\n",
    "    X = np.column_stack([np.zeros_like(X[:, 0]), X, np.ones_like(X[:, 0])])\n",
    "    for scale in scales:\n",
    "        for alpha in alphas:\n",
    "            results.append(\n",
    "                dict(\n",
    "                    k=k,\n",
    "                    alpha=alpha,\n",
    "                    scale=scale,\n",
    "                    dists=np.apply_along_axis(\n",
    "                        fermat_dist, axis=1, arr=scale * X, alpha=alpha\n",
    "                    ),\n",
    "                )\n",
    "            )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"mean_dist\"] = df.dists.apply(np.mean)\n",
    "df[\"scaled_dist\"] = df.mean_dist / (df.scale ** df.alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.k == 1].pivot(\"alpha\", \"scale\", \"mean_dist\").round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.k == 1].pivot(\"alpha\", \"scale\", \"scaled_dist\").round(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Está claro que si $c$ es la constance de escale `scale`, las distancias de fermat escalan según $c^{\\alpha}$. Ahora, cuánto cambian con el tamaño de muestra $k$? \n",
    "\n",
    "Para $k=1$, se puede calcular exactamente la esperanza de la longitud del camino. Si hay un único punto entre 0 y 1 elegido al azar según $X \\sim \\text{Unif}(0, 1)$, entonces la longitud del camino de Fermat cuando $alpha=2$ será $E\\left(X^2 + (1-X)^2\\right) = 2/3$, lo cual se ve en la tabla anterior. Para $k=1$ y otros valores de $\\alpha$, la expresión no será tan bella pero es computable sin mucha dificultad. Para otros valores de $k$, sin embargo, ya entran en juego la distribución de los estadísticos de orden y no me resulta para nada evidente una fórmula cerrada.\n",
    "\n",
    "Aproximémosla. Sean $X^{(0)} = 0, X^{(k+1)} = 1$ y $X^{(i)}, i=1,\\dots,k$ las v.a. que surgen de ordenar una muestra de $k$. Para todo $k$ se cumple que cada \"segmentito de recta\", $\\mathbb{E}\\left(X^{(i+1)}-X^{(i)}\\right) = 1 / (k + 1)$. Luego, esperaríamos que \n",
    "\n",
    "$$\n",
    "\\begin{align} dist_{\\alpha}^k(0, 1) &= \\mathbb{E}\\ \\left(\\sum_{i=0}^k \\left[X^{(i+1)}-X^{(i)} \\right]^{\\alpha}\\right) \\\\\n",
    " &=  \\sum_{i=0}^k\\ \\left( \\mathbb{E}\\left[X^{(i+1)}-X^{(i)} \\right]^{\\alpha}\\right) \\\\\n",
    " &\\approx (???) (k + 1) \\left(\\frac{1}{k+1}\\right)^{\\alpha}\n",
    "   \\end{align}\n",
    "\n",
    "$$\n",
    "\n",
    "Pero no vale que $E(X^k) = E(X)^k$! Qué se hace en su lugar? Hay que conocer la densidad de $X^{(i+1)}-X^{(i)}$ y calcularlo? Empíricamente, veamos como cambia la distancia con $k$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.scale == 1].pivot(\"alpha\", \"k\", \"scaled_dist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.scale == 1].assign(k_scale = lambda x: (x.k) ** (1 - x.alpha)).pivot(\"alpha\", \"k\", \"k_scale\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size = 1000\n",
    "alphas = [1.5, 1.75, 2, 2.25, 3, 4, 5]\n",
    "ks = np.array([*range(1, 11), *rnd.choice(range(11, 2001), 50, replace=False)])\n",
    "\n",
    "results = []\n",
    "for k in ks:\n",
    "    X = rnd.rand(sample_size, k)\n",
    "    X.sort(axis=1)\n",
    "    X = np.column_stack([np.zeros_like(X[:, 0]), X, np.ones_like(X[:, 0])])\n",
    "    for alpha in alphas:\n",
    "        results.append(\n",
    "            dict(\n",
    "                k=k,\n",
    "                alpha=alpha,\n",
    "                dists=np.apply_along_axis(\n",
    "                    fermat_dist, axis=1, arr=X, alpha=alpha\n",
    "                ),\n",
    "            )\n",
    "        )\n",
    "\n",
    "df = pd.DataFrame(results)\n",
    "df[\"mean_dist\"] = df.dists.apply(np.mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.k < 50].pivot(\"alpha\", \"k\", \"mean_dist\").round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 7))\n",
    "sns.lineplot(x=\"k\", y=\"mean_dist\", hue=\"alpha\", data=df, marker=\"o\")\n",
    "plt.xscale(\"log\")\n",
    "plt.yscale(\"log\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para la elección del ancho de banda $h$ es necesario saber la escala de las distancias! Con qué se come esto???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for i in range(10000):\n",
    "    s = rnd.rand(100)\n",
    "    s.sort()\n",
    "    dists = (np.array([s[0], *(s[1:] - s[:-1]), 1 - s[-1]])) ** 2\n",
    "    results.append((dists.mean(), dists.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean([std / mean for mean, std in results])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementación KDEClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "from numpy.linalg import norm as euclidean_norm\n",
    "from scipy.spatial import distance_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "euclidean_norm([1, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scipy.spatial.distance.minkowski(X[0], X[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repr(fermat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = range(5)\n",
    "B = range(7)\n",
    "[[a * b for a in A] for b in B]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = []\n",
    "fhats = []\n",
    "for x in X_test:\n",
    "    fhat = {}\n",
    "    for cls in classes.keys():\n",
    "        klass = classes[cls]\n",
    "        verts, dists = klass[\"verts\"], klass[\"dists\"]\n",
    "        n = verts.shape[0]\n",
    "        to_verts = euclidean_distances(x.reshape(1, -1), verts)[0] ** alpha\n",
    "        fmt_dists = [min(to_verts + dists[:, i]) for i in range(n)]\n",
    "        # print(cls, np.mean(fmt_dists))\n",
    "        fhat[cls] = (1 / h**D) * np.mean([kern(d / h) for d in fmt_dists])\n",
    "    fhats.append(fhat)\n",
    "    preds.append(pd.Series(fhat).argmax())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.where((X[:] == 2 * X[2]).all(1))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape, np.zeros(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FermatDistance(Fermat):\n",
    "    def __init__(self, **kwargs):\n",
    "        super.__init__(**kwargs)\n",
    "\n",
    "    def _fit(self, X):\n",
    "        self.fit(X)\n",
    "        self.X_ = X\n",
    "        self.n_, self.d_ = X.shape\n",
    "        self.is_fitted_ = True\n",
    "        return self\n",
    "\n",
    "    def __call__(A, B):\n",
    "        if self.alpha == 1:\n",
    "            return distance_matrix(A, B)\n",
    "        else:\n",
    "            return [[self._get_distance(a, b) for a in A] for b in B]\n",
    "\n",
    "    def _get_distance(a, b):\n",
    "        if not self.is_fitted_:\n",
    "            self._fit()\n",
    "        if any((self.X_[:] == a).all(1)):  # `a` is a node from X_\n",
    "            to_X = np.zeros(self.n_)\n",
    "        else:\n",
    "            to_X = euclidean_distances(a.reshape(1, -1), verts)[0] ** self.alpha\n",
    "        \n",
    "\n",
    "        a_known = np.where((X[:] == 2 * X[2]).all(1))[0]\n",
    "        if known np.where((X[:] == 2 * X[2]).all(1))\n",
    "        to_nodes = euclidean_distances(a.reshape(1, -1), verts)[0] ** alpha\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new = np.random.rand(3, 4)\n",
    "sample = np.random.rand(5, 4)\n",
    "kernel = norm.pdf\n",
    "h = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel(distance_matrix(new, sample) / h).mean(axis=1) * (h ** -sample.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_distance(x, y):\n",
    "    return euclidean_norm(x - y)\n",
    "\n",
    "\n",
    "# https://scikit-learn.org/stable/developers/develop.html\n",
    "class EuclideanKDE:\n",
    "    def __init__(self, kernel=\"normal\", bandwith=1):\n",
    "        self.kernel = kernel\n",
    "        self.bandwith = bandwith\n",
    "\n",
    "    def fit(self, X):\n",
    "        self.X_ = X\n",
    "        self.n_, self.dim_ = X.shape\n",
    "        return self\n",
    "\n",
    "    def density(self, X=None, log=True):\n",
    "        X = X or self.X_\n",
    "        return np.log(\n",
    "            kernel(distance_matrix(X, self.X_) / h).mean(axis=1)\n",
    "            * (self.bandwith**-self.dim_)\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EuclideanKDEClassifier:\n",
    "    def __init__(self, kernel=\"normal\", bandwith=1):\n",
    "        self.kernel = kernel\n",
    "        self.bandwith = bandwith\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # Check that X and y have correct shape  \n",
    "        X, y = check_X_y(X, y)\n",
    "        # Store the classes seen during fit\n",
    "        self.classes_ = unique_labels(y)                                      \n",
    "        self.X_ = X\n",
    "        self.y_ = y\n",
    "        self.densities_ = {cls: EuclideanKDE.fit(X) for cls in self.classes_}\n",
    "\n",
    "    def predict(self, X):\n",
    "        self.densities_ nnnnnnnnnnnnnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ekde = EuclideanKDE()\n",
    "ekde.fit(X)\n",
    "ekde.density()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_distance(x, y):\n",
    "    return euclidean_norm(x - y)\n",
    "\n",
    "# https://scikit-learn.org/stable/developers/develop.html\n",
    "class FermatKDEstimator:\n",
    "    def __init__(self, kernel=\"normal\", bandwith=1, alpha=2, path_method=\"FW\"):\n",
    "        self.kernel = kernel\n",
    "        self.bandwith = bandwith\n",
    "        self.alpha = alpha\n",
    "        self.path_method = path_method\n",
    "\n",
    "    def fit(self, X):\n",
    "        self.X_ = X\n",
    "        self.n_, self.dim_ = X.shape\n",
    "        self.euclidean_distances_ = distance_matrix(X, X)\n",
    "        if self.alpha == 1:\n",
    "            self.distances_ = self.euclidean_distances_\n",
    "        elif:\n",
    "            self.distances_ = self.euclidean_distances_ ** self.alpha\n",
    "        self.fermat_ = Fermat(self.alpha, self.path_method)\n",
    "        self.fermat_.fit(X)\n",
    "\n",
    "\n",
    "    def             self.distances_ = self.\n",
    "        if self.distance == \"euclidean\":\n",
    "            self.distances_ = self.euclidean_distances_\n",
    "        if self.distance.\n",
    "\n",
    "    def predict(self, X):\n",
    "        y = anp.empty_like(X) * distances = rv = self.bandwith**-self.d * np.mean(\n",
    "            [self.kernel(self.distance(x, xi) / self.bandwith) for xi in self.X]\n",
    "        )\n",
    "        return rv\n",
    "\n",
    "    def distance(self, X, Y):\n",
    "        pass  # Implement your own. Should accept \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = load_iris(return_X_y=True)\n",
    "N = len(X)  # # observaciones\n",
    "K = len(set(y))  # # clases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kde = KDEstimator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kde.fit(X[y == 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kde.predict(X[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TemplateClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.utils.validation import check_X_y, check_array, check_is_fitted\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "from sklearn.metrics import euclidean_distances\n",
    "class TemplateClassifier(BaseEstimator, ClassifierMixin):\n",
    "\n",
    "    def __init__(self, demo_param='demo'):\n",
    "        self.demo_param = demo_param\n",
    "\n",
    "    def fit(self, X, y):\n",
    "\n",
    "        # Check that X and y have correct shape\n",
    "        X, y = check_X_y(X, y)\n",
    "        # Store the classes seen during fit\n",
    "        self.classes_ = unique_labels(y)\n",
    "\n",
    "        self.X_ = X\n",
    "        self.y_ = y\n",
    "        # Return the classifier\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "\n",
    "        # Check if fit has been called\n",
    "        check_is_fitted(self)\n",
    "\n",
    "        # Input validation\n",
    "        X = check_array(X)\n",
    "\n",
    "        closest = np.argmin(euclidean_distances(X, self.X_), axis=1)\n",
    "        return self.y_[closest]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fermat",
   "language": "python",
   "name": "fermat"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
