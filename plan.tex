%% LyX 2.3.6 created this file.  For more info, see http://www.lyx.org/.
%% Do not edit unless you really know what you are doing.
\documentclass[11pt]{article}
\usepackage[latin9]{inputenc}
\usepackage{textcomp}
\usepackage{amsmath}

\makeatletter
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% User specified LaTeX commands.
%\usepackage[utf8]{inputenc}
%\usepackage[latin1]{inputenc}
%\usepackage[spanish,activeacute]{babel}

\usepackage{amsfonts}\usepackage{color}
\usepackage{url}
\setlength{\textwidth}{6.70in}
\setlength{\oddsidemargin}{-0.2in}
\topmargin -0.4pt
\textheight 9.0in

\include{definiciones}

\makeatother

\begin{document}
\global\long\def\R{\mathbb{R}}%

\global\long\def\dimx{d_{x}}%

\global\long\def\Rdimx{\mathbb{R}^{\dimx}}%

\global\long\def\Rd{\Rdimx}%

\global\long\def\M{\mathcal{M}}%

\global\long\def\dimm{d_{\M}}%

\global\long\def\var{\mathcal{\M}}%

\global\long\def\itR{\mathcal{\R}}%

\pagestyle{myheadings} \markboth{}{{\small{}} \baselineskip12pt}{\small\par}
\begin{center}
\underline{\bf Proyecto de tesis}\\
 \underline{\bf para optar al t\'{\i}tulo de Magister en Estad\'{\i}stica Matem\'atica\rm} 
\par\end{center}

\noindent \textbf{Nombre del postulante:} Lic. Gonzalo Barrera Borla

\noindent \textbf{Directora:} Dr. Pablo Groisman

\noindent \textbf{Tema de trabajo: }Aprendizaje de distancia basada
en datos para clasificación por densidad de núcleos. // Distancia
de Fermat en Clasificadores de Densidad Nuclear.

\vspace{0.3cm}
\textbf{Lugar de trabajo:} Departamento de Matemática

\section{Antecedentes existentes sobre el tema}

El concepto de \emph{distancia }entre las observaciones disponibles,
es central a casi cualquier tarea estadística, tanto en descripción
como inferencia. Consideremos, por caso, un ejercicio de clasificación.
Sea $\boldsymbol{x}=\left(x_{i}\right)_{i=1}^{N}$una muestra de $N$
observaciones de vv.aa. i.i.d. ($X_{1},\dots,X_{n}:X\sim\mathcal{L}\left(X\right)\ \forall\ i\in\left[N\right]$),
con $x_{i}\in\mathbb{R}^{d_{x}}\ \forall\ i\in\left[N\right]\equiv\left\{ 1,\dots,N\right\} $,
donde cada observación pertenece a una de $M$ clases $C_{1},\dots,C_{M}$
mutuamente excluyentes y conjuntamente exhaustivas, codificadas en
un vector $\boldsymbol{y}=\left(y_{1},\dots,y_{N}\right)$ donde $y_{i}=j\iff x_{i}\in C_{j}$. 

Dada una nueva observación $x$ cuya clase es desconocida: ¿a qué
clase deberíamos asignarla? Cualquier respuesta a esta pregunta implicará
combinar toda la información muestral disponible, ponderando las $N$
observaciones de manera relativa a su cercanía o similitud con $x$.
Cuando el dominio de las $x_{i}$ es un espacio euclídeo $\R^{\dimx}$,
es costumbre tomar la \emph{distancia euclídea} para cuantificar la
cercanía entre elementos. Así, por ejemplo, k-vecinos más cercanos
(kNN) asignará la nueva observación $x$ a la clase modal entre las
$k$ observaciones de entrenamiento más cercanas\emph{ }(es decir,
que minimizan $\left\Vert x-\cdot\right\Vert $.

Una dificultad bien conocida con los métodos basados en distancias,
es la \emph{maldición de la dimensionalidad}: a medida que la dimensión
$\dimx$ del espacio euclídeo en consideración crece, el espacio se
vuelve tan grande, que todos los elementos de la muestra están indistinguiblemente
lejos entre sí; o lo que es equivalente, a igual $N,$ la densidad
de observaciones en el espacio cae exponencialmente con $\dimx$. 

En estos casos, es de suponer que el dominio de las $X$ no cubre
\emph{todo} $\Rd$, sino que éstas se encuentran embebidas en una
variedad $\M\subset\R^{\dimx}$ cuya dimensión intrínseca $\dim\left(\M\right)=\dimm$,
donde potencialmente $\dimm\ll\dimx$, y por ende la distancia \emph{en
la variedad} es más inormativa que la distancia (euclídea) en el espacio
ambiente $\Rd$. A este supuesto se lo suele llamar ``hipótesis de
la variedad'' (\emph{manifold hypothesis}), y suele ser particularmente
acertado cuando las observaciones provienen ``del mundo real'' (e.g.,
imágenes, sonido y texto). Según Bengio, \emph{aprender} la estructura
de $\M$ a partir de $\boldsymbol{x}$ es una forma (enre muchas)
de \emph{aprendizaje de representaciones} (representation learning),
donde la representación de $x_{i}$ en base a sus coordenadas en $\M$
(en $\mathbb{R}^{d}$) es tanto o más útil que la representación original
en $\Rd$ para tareas de descripción e inferencia.

La ganancia en reducción de dimensionalidad con la hipótesis de la
variedad, se compensa con la dificultad extra de tener que trabajar
en una variedad arbitraria $\M$ en lugar de $\Rd$, a priori desconocida
y que debemos estimar. Pelletier {[}2005{]} describe un estimador
``nuclear'' para la función de densidad de vv.aa. i.i.d. en variedades
compactas de Riemann sin fronteras, junto con resultados de consistencia
y convergencia; Henry y Rodriguez {[}2009{]} los amplían para probar
la consistencia uniforme fuerte y la distribución asintótica de $f_{N,k}$.

Sin embargo, tanto Pelletier Henry y Rodríguez discuten ``un estimador
de densidad {[}...{]} basado en núcleos que son funciones de \emph{la
distancia geodésica Riemanniana en la variedad}, cuya expresión es
consistente con su equivalente en el caso euclídeo''.{[}p. 298 intro,
Pelletier 2005{]}. Ahora, la distancia euclídea en $\Rdimx$, $\left|\cdot\right|_{\dimx}$
(que podemos calcular fácilmente) no tiene por qué coincidir con la
distancia geodésica en $\M,\ dg_{\M}$ (que nos interesa). Entre otros
factores, tanto la diferencia en dimensiones $\dimx-d_{\M}$ como
la curvatura de $\M$ afectan drásticamente la relación entre $\left|\cdot\right|_{\dimx}$
y $dg_{\M}$ .

Trabajos recientes {[}McKenzie 2019, Little 2021, Sapienza 2018, Groisman
2019{]} proponen aprender la distancia $dg_{\M}$ a partir del del
grafo (aleatorio) completo de la muestra $\mathbb{X}_{n}$, y las
geodésicas (caminos mínimos) entre sus elementos, que resultan de
pesar las aristas de acuerdo a cierta potencia $p$ de la distancia
euclídea entre sus extremos. Empíricamente, el uso de esta distancia
basada en datos que los autores llaman ``distancia de Fermat'' {[}Groisman
2019{]}.

\section{Naturaleza del aporte original sobre el tema y objetivos}

Uniendo los elementos enunciados anteriormente, nos proponemos una
comparación sistemáticamente entre el uso de la distancia de Fermat
$\mathcal{D}_{f}$ {[}McKenzie 2019, Little 2021, Sapienza 2018, Groisman
2019{]}, aprendiéndola de los datos disponibles, contra la elección
canónica (la distancia euclídea), para el cómputo de densidades por
núcleo \emph{en variedades de dimensión desconocida}, $\dimm\leq\dimx$
según propone {[}Pelletier 2005{]} y expande {[}Henry y Rodriguez
2009{]}. Como prueba para comparar sus ventajas relativas, aplicaremos
los estimadores de densidad (KDEs, ``kernel density estimator(s)'')
resultantes, a tareas varias de \emph{clasificación }bajo una amplia
gama de condiciones:
\begin{itemize}
\item datasets ``reales'' y ``sintéticos'',
\item tanto en relacióna la dimensión $D$ de $X\in\itR^{D}$ , como
\item las $k$ categorías posibles para $Y\in\left\{ C_{1},\dots,C_{k}\right\} )$
\item distintas razones entre $d:D$
\end{itemize}
La KDE con distancia euclídea es un método bastante eficiente en términos
de cómputo. En cambio, un calculo exacto del estimador muestral de
$\mathcal{D}_{f}$ requiere $n^{3}$ pasos. La pregunta al respecto
de su eficacia, entonces, debe considerar además comparativamente
los costos computacionales de ambos métodos, que en datasets lo suficientemnete
grandes podrían no justificar ganancias de \emph{performance }relativamente
menores. En este aspecto, y para poner en contexto la capacidad predictiva
de KDE, incluiremos como método de referencia (tanto estadística como
computacionalmente) a los \emph{gradient boosting trees}, un método
reconocido en a actualidad por su simplicidad de uso y escasez de
requerimientos {[}ESL Cap. 9, Additive Trees and Boosting Methods{]}.

En el orden teórico, nos proponemos ???????
\begin{thebibliography}{Gr19}
\bibitem[Gr19]{Groisman 2019}NONHOMOGENEOUS EUCLIDEAN FIRST-PASSAGE
PERCOLATION AND DISTANCE LEARNING P. GROISMAN, M. JONCKHEERE, AND
F. SAPIENZA

\bibitem{key-17}W EIGHTED G EODESIC D ISTANCE F OLLOWING F ERMAT
\textquoteright{} S P RINCIPLE Pablo Groisman IMAS-CONICET, NYU-ECNU
IMS at NYU Shanghai, and Universidad de Buenos Aires, Argentina. pgroisma@dm.uba.ar
Facundo Sapienza Aristas SRL, Buenos Aires, Argentina. f.sapienza@aristas.com.ar
Matthieu Jonckheere

\bibitem{key-9}2019 Power Weighted Shortest Paths for Clustering
Euclidean Data Daniel Mckenzie 1 \textasteriskcentered 1 and Steven
Damelin

\bibitem{key-10}Balancing Geometry and Density: Path Distances on
High-Dimensional Data Anna Little \textasteriskcentered{} Daniel McKenzie
\dag{} James M. Murphy \ddag{} June 9, 2021

\bibitem{key-7}ESL II

\bibitem{key-8}2014 Representation Learning: A Review and New Perspectives
Yoshua Bengio \dag{} , Aaron Courville, and Pascal Vincent \dag{}

\bibitem{key-13}Kernel Density Estimation on Riemannian Manifolds:
Asymptotic Results Guillermo Henry · Daniela Rodriguez Published online:
21 February 2009

\bibitem{key-14}Kernel density estimation on Riemannian manifolds
Bruno Pelletier 2005

\bibitem{key-1}ACA PRA ABAJO NO SE USARON

\bibitem{key-11}Data-driven density derivative estimation, with applications
to nonparametric clustering and bump hunting José E. Chacón and Tarn
Duong

\bibitem{key-12}A Comprehensive Approach to Mode Clustering Yen-Chi
Chen, and Christopher R. Genovese, and Larry Wasserman 2015

\bibitem{key-15}Nonparametric Density Estimation for High-Dimensional
Data - Algorithms and Applications Zhipeng Wang \textasteriskcentered{}
and David W. Scott \dag{}
\end{thebibliography}

\end{document}
