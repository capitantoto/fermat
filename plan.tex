%% LyX 2.3.6 created this file.  For more info, see http://www.lyx.org/.
%% Do not edit unless you really know what you are doing.
\documentclass[11pt]{article}
\usepackage[latin9]{inputenc}
\usepackage{amsmath}
\usepackage[authoryear]{natbib}

\makeatletter
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% User specified LaTeX commands.
%\usepackage[utf8]{inputenc}
%\usepackage[latin1]{inputenc}
%\usepackage[spanish,activeacute]{babel}

\usepackage{amsfonts}\usepackage{color}
\usepackage{url}
\setlength{\textwidth}{6.70in}
\setlength{\oddsidemargin}{-0.2in}
\topmargin -0.4pt
\textheight 9.0in

\include{definiciones}

\makeatother

\begin{document}
\global\long\def\R{\mathbb{R}}%
\global\long\def\dimx{d_{x}}%
\global\long\def\Rdimx{\mathbb{R}^{\dimx}}%
\global\long\def\Rd{\Rdimx}%
\global\long\def\M{\mathcal{M}}%
\global\long\def\dimm{d_{\M}}%
\global\long\def\var{\mathcal{\M}}%
\global\long\def\itR{\mathcal{\R}}%
\pagestyle{myheadings} \markboth{}{{\small{}{}{}} \baselineskip12pt}
\begin{center}
\underline{\bf Proyecto de tesis}\\
 \underline{\bf para optar al t\'{\i}tulo de Magister en Estad\'{\i}stica Matem\'atica\rm} 
\par\end{center}

\noindent \textbf{Nombre del postulante:} Lic. Gonzalo Barrera Borla

\noindent \textbf{Directora:} Dr. Pablo Groisman

\noindent \textbf{Tema de trabajo: }Aprendizaje de distancia basada
en datos para clasificación por densidad de núcleos. // Distancia
de Fermat en Clasificadores de Densidad Nuclear.

\vspace{0.3cm}
 \textbf{Lugar de trabajo:} Departamento de Matemática

\section{Antecedentes existentes sobre el tema}

El concepto de \emph{distancia }entre las observaciones disponibles,
es central a casi cualquier tarea estadística, tanto en descripción
como inferencia. Consideremos, por caso, un ejercicio de clasificación.
Sea $\boldsymbol{x}=\left(x_{i}\right)_{i=1}^{N}$una muestra de $N$
observaciones de vv.aa. i.i.d. ($X_{1},\dots,X_{n}:X_{i}\sim\mathcal{L}\left(X\right)\ \forall\ i\in\left[N\right]$),
con $x_{i}\in\mathbb{R}^{d_{x}}\ \forall\ i\in\left[N\right]\equiv\left\{ 1,\dots,N\right\} $,
donde cada observación pertenece a una de $M$ clases $C_{1},\dots,C_{M}$
mutuamente excluyentes y conjuntamente exhaustivas.

Dada una nueva observación $x$ cuya clase es desconocida: ¿a qué
clase deberíamos asignarla? Cualquier respuesta a esta pregunta implicará
combinar toda la información muestral disponible, ponderando las $N$
observaciones de manera relativa a su cercanía o similitud con $x$.
Cuando el dominio de las $x_{i}$ es un espacio euclídeo $\R^{\dimx}$,
es costumbre tomar la \emph{distancia euclídea} para cuantificar la
cercanía entre elementos. Así, por ejemplo, $k-$vecinos más cercanos
($k$NN) asignará la nueva observación $x$ a la clase modal entre
las $k$ observaciones de entrenamiento más cercanas\emph{ }(es decir,
que minimizan $\left\Vert x-\cdot\right\Vert )$.

Una dificultad bien conocida con los métodos basados en distancias,
es la \emph{maldición de la dimensionalidad}: a medida que la dimensión
$\dimx$ del espacio euclídeo en consideración crece, el espacio se
vuelve tan grande, que todos los elementos de la muestra están indistinguiblemente
lejos entre sí; o lo que es equivalente, a igual $N,$ la densidad
de observaciones en el espacio cae exponencialmente con $\dimx$.

En estos casos, es de suponer que el dominio de las $X$ no cubre
\emph{todo} $\Rd$, sino que éstas se encuentran embebidas en una
variedad $\M\subset\R^{\dimx}$ cuya dimensión intrínseca $\dim\left(\M\right)$,
es potencialmente mucho menor a $\dimx$, y por ende la distancia
\emph{en la variedad} es más informativa que la distancia (euclídea)
en el espacio ambiente $\Rd$. A este supuesto se lo suele llamar
``hipótesis de la variedad'' (\emph{manifold hypothesis}), y suele
ser particularmente acertado cuando las observaciones provienen ``del
mundo real'' (e.g., imágenes, sonido y texto). Según \citet{Bengio2013},
\emph{aprender} la estructura de $\M$ a partir de $\boldsymbol{x}$
es una forma (enre muchas) de \emph{aprendizaje de representaciones}
(representation learning), donde la representación de $x_{i}$ en
base a sus coordenadas en $\M$ (en $\mathbb{R}^{d}$) es tanto o
más útil que la representación original en $\Rd$ para tareas de descripción
e inferencia.

La ganancia en reducción de dimensionalidad con la hipótesis de la
variedad, se compensa con la dificultad extra de tener que trabajar
en una variedad arbitraria $\M$ en lugar de $\Rd$, a priori desconocida
y que debemos estimar. \citet{Pelletier2005} describe un estimador
``nuclear'' para la función de densidad de vv.aa. i.i.d. en variedades
compactas de Riemann sin borde, junto con resultados de consistencia
y convergencia; \citet*{Henry2009} los amplían para probar la consistencia
uniforme fuerte y la distribución asintótica de estos estimadores.

Tanto \citet{Pelletier2005} como \citet*{Henry2009} asumen que la
distancia geodésica es conocida. Trabajos recientes (\citet{Sapienza2018,Groisman2022,Mckenzie2019,Little2022})
proponen aprender la distancia geodésica $\mathcal{D}_{f}^{p}$ entre
los nodos del grafo (aleatorio) completo de la muestra $\mathbb{X}_{n}$\footnote{O por simplicidad de cómputo, su aproximación por el grafo de k-vecinos
más cercanos}, con cada arista pesada por una potencia $p$ de la distancia euclídea
entre sus\emph{ }extremos. En \citet{Sapienza2018}, el uso de esta
distancia - que los autores llaman ``de Fermat'', por su analogía
con el fenómeno óptico -, parece rendir considerables mejoras de \emph{performance
}empírica en tareas de clasificación. Cuando $p=1$, el estimador
de la distancia geodésica $\mathcal{D}_{f}^{1}$ resultante es idéntico
al que usa \emph{Isomap }(\citet{Tenenbaum2000}) para construir los
\emph{embeddings} de dimensión reducida.

\section{Naturaleza del aporte original sobre el tema y objetivos}

Uniendo los elementos enunciados anteriormente, nos proponemos estudiar
sistemáticamente qué valor aporta el uso de una distancia basada en
datos (la distancia de Fermat $\mathcal{D}_{f}$) frente a la elección
canónica (la distancia euclídea$\left\Vert x-\cdot\right\Vert $),
en el aprendizaje de \emph{estimadores de densidad nuclear} (KDEs,
por sus siglas en inglés). Nos proponemos luego comparar comparar
sus bondades relativas usándolos en tareas de \emph{clasificación
}bajo una amplia gama de condiciones: 
\begin{itemize}
\item en datasets ``reales'' y ``sintéticos'', 
\item en relación a la dimensión $D$ del espacio ambiente y
\item en relación a las $k$ categorías posibles para $Y\in\left\{ C_{1},\dots,C_{k}\right\} )$ 
\end{itemize}
Aprender un clasificador a partir de KDEs con distancia euclídea (\citet[cap. 6.6,][]{Hastie2009})
es un método bastante eficiente en términos de cómputo. En cambio,
un calculo exacto del estimador muestral de $\mathcal{D}_{f}$ requiere
$n^{3}$ pasos. La pregunta al respecto de su eficacia, entonces,
debe considerar además comparativamente los costos computacionales
de ambas distancias, que en datasets ``grandes'' podrían ser demasiado
altos para obtener ganancias de \emph{performance }relativamente menores.
Para poner en contexto la capacidad predictiva de estos clasificadores
y su costo computacional, incluiremos como métodos de referencias 
\begin{itemize}
\item clasificadores de \emph{Naive Bayes }(\citet[cap. 6.6.3,][]{Hastie2009}),
que usan $d$ KDEs unidimensionales en lugar de un KDE $d-$dimensional
por clase),
\item \emph{gradient boosting trees} (GBTs, \citet[cap. 10,][]{Hastie2009}),
un método reconocido en la actualidad por su simplicidad de uso y
escasez de requerimientos y
\item \emph{random forests} \citet[cap. 15,][]{Hastie2009}, que capturan
buena parte de las bondades de los GBTs con una estructura sencilla.
\end{itemize}
\bibliographystyle{plainnat}
\bibliography{bib/references}

\end{document}
