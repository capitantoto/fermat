% cb2Bib 1.9.9
@article{Groisman11,
author       = {P. Groisman and F. Sapienza and M. Jonckheere},
journal      = {Work},
year         = {2011},
file         = {/home/gonzalo/maestria/fermat/bib/Weighted Geodesic Distance Following Fermat's Principle.pdf}
}

% cb2Bib 1.9.9
@article{Wang04,
title        = {{R Graphics Output}},
author       = {Z. Wang and D. W. Scott},
journal      = {arXiv:1904.00176},
year         = {1904},
abstract     = {Density Estimation is one of the central areas of statistics whose purpose is to estimate the probability density function underlying the observed data. It serves as a building block for many tasks},
keywords     = {density estimation, high-dimensional data, clustering, neural networks, data partitioning},
eprint       = {1904.00176},
file         = {/home/gonzalo/maestria/fermat/bib/Wang \& Scott (2019) Nonparametric Density Estimation for High-Dimensional Data - Algorithms and Applications.pdf},
url          = {https://arxiv.org/abs/1904.00176}
}

% cb2Bib 1.9.9
@article{Pelletier05p297,
title        = {{Doi:10.1016/J.Spl.2005.04.004}},
author       = {B. Pelletier},
journal      = {Stat. Probab. Lett.},
pages        = {297 - 304},
volume       = {73},
year         = {2005},
abstract     = {The estimation of the underlying probability density of n i.i.d. random objects on a compact Riemannian},
keywords     = {nonparametric density estimation, kernel density estimation, riemannian manifolds, l2 convergence},
doi          = {10.1016/j.spl.2005.04.004},
file         = {/home/gonzalo/maestria/fermat/bib/pelletier2005.pdf},
url          = {www.elsevier.com/locate/stapro}
}

% cb2Bib 1.9.9
@article{Groisman19,
author       = {P. Groisman and M. Jonckheere and F. Sapienza},
journal      = {arXiv:1810.09398},
year         = {2019},
abstract     = {Consider an i.i.d. sample from an unknown density function supported on an unknown manifold embedded in a high dimensional Euclidean space. We tackle the problem of learning a distance between points, able to capture both the geometry of the manifold and the underlying density. We define such a sample distance and prove the convergence, as the sample size goes to infinity, to a macroscopic one that we call Fermat distance as it minimizes a path functional, resembling Fermat principle in optics. The proof boils down to the study of geodesics in Euclidean first-passage percolation for nonhomogeneous Poisson point processes.},
keywords     = {and phrases, distance learning, euclidean first-passage percolation, nonhomogeneous point},
eprint       = {1810.09398},
file         = {/home/gonzalo/maestria/fermat/bib/Nonhomogeneous Euclidean first-passage percolation and distance learning.pdf},
url          = {https://arxiv.org/abs/1810.09398}
}

% cb2Bib 1.9.9
@article{Chac13p499,
title        = {{Data-driven density derivative estimation, with applications to nonparametric clustering and bump hunting}},
author       = {J. E. Chac and T. Duong},
journal      = {Electron. J. Stat.},
pages        = {499 - 532},
volume       = {7},
year         = {2013},
abstract     = {Important information concerning a multivariate data set, such as clusters and modal regions, is contained in the derivatives of the probability density function. Despite this importance, nonparametric estimation of higher order derivatives of the density functions have received only relatively scant attention. Kernel estimators of density functions are widely used as they exhibit excellent theoretical and practical properties, though their generalization to density derivatives has progressed more slowly due to the mathematical intractabilities encountered in the crucial problem of bandwidth (or smoothing parameter) selection. This paper presents the first fully automatic, data-based bandwidth selectors for multivariate kernel density derivative estimators. This is achieved by synthesizing recent advances in matrix analytic theory which allow mathematically and computationally tractable representations of higher order derivatives of multivariate vector valued functions. The theoretical asymptotic properties as well as the finite sample behaviour of the proposed selectors are studied. In addition, we explore in detail the applications of the new data-driven methods for two other statistical problems: clustering and bump hunting. The introduced techniques are combined with the mean shift algorithm to develop novel automatic, nonparametric clustering procedures which are shown to outperform mixture-model cluster analysis and other recent nonparametric approaches in practice. Furthermore, the advantage of the use of smoothing parameters designed for density derivative estimation for feature significance analysis for bump hunting is illustrated with a real data example. AMS 2000 subject classifications: Primary 62G05; secondary 62H30. Keywords and phrases: Adjusted Rand index, cross validation, feature significance, nonparametric kernel method, mean integrated squared error, mean shift algorithm, plug-in choice. Received August 2012. 499},
keywords     = {62g05, 62h30adjusted rand index, cross validation, feature significance, nonparametric kernel method, mean integrated squared error, mean shift algorithm, plug-in choice},
doi          = {10.1214/13-EJS781},
file         = {/home/gonzalo/maestria/fermat/bib/chacon-2013.pdf}
}

% cb2Bib 1.9.9
@article{Little21,
author       = {A. Little and D. McKenzie and J. M. Murphy},
journal      = {arXiv:2012.09385},
volume       = {9},
year         = {2021},
abstract     = {New geometric and computational analyses of power-weighted shortest-path distances (PWSPDs) are presented. By illuminating the way these metrics balance geometry and density in the underlying data, we clarify their key parameters and illustrate how they provide multiple perspectives for data analysis. Comparisons are made with related data-driven metrics, which illustrate the broader role of density in kernel-based unsupervised and semi-supervised machine learning. Computationally, we relate PWSPDs on complete weighted graphs to their analogues on weighted nearest neighbor graphs, providing high probability guarantees on their equivalence that are near-optimal. Connections with percolation theory are developed to establish estimates on the bias and variance of PWSPDs in the finite sample setting. The theoretical results are bolstered by illustrative experiments, demonstrating the versatility of PWSPDs for a wide range of data settings. Throughout the paper, our results generally require only that the underlying data is sampled from a compact low-dimensional manifold, and depend most crucially on the intrinsic dimension of this manifold, rather than its ambient dimension.},
eprint       = {2012.09385},
file         = {/home/gonzalo/maestria/fermat/bib/2012.09385(1).pdf},
url          = {https://arxiv.org/abs/2012.09385}
}

