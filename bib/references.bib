@misc{bengioConsciousnessPrior2019,
  title = {The {{Consciousness Prior}}},
  author = {Bengio, Yoshua},
  year = {2019},
  month = dec,
  number = {arXiv:1709.08568},
  eprint = {arXiv:1709.08568},
  publisher = {{arXiv}},
  abstract = {A new prior is proposed for learning representations of high-level concepts of the kind we manipulate with language. This prior can be combined with other priors in order to help disentangling abstract factors from each other. It is inspired by cognitive neuroscience theories of consciousness, seen as a bottleneck through which just a few elements, after having been selected by attention from a broader pool, are then broadcast and condition further processing, both in perception and decision-making. The set of recently selected elements one becomes aware of is seen as forming a low-dimensional conscious state. This conscious state is combining the few concepts constituting a conscious thought, i.e., what one is immediately conscious of at a particular moment. We claim that this architectural and information-processing constraint corresponds to assumptions about the joint distribution between high-level concepts. To the extent that these assumptions are generally true (and the form of natural language seems consistent with them), they can form a useful prior for representation learning. A low-dimensional thought or conscious state is analogous to a sentence: it involves only a few variables and yet can make a statement with very high probability of being true. This is consistent with a joint distribution (over high-level concepts) which has the form of a sparse factor graph, i.e., where the dependencies captured by each factor of the factor graph involve only very few variables while creating a strong dip in the overall energy function. Instead of making predictions in the sensory (e.g. pixel) space, one can thus make predictions in this high-level abstract space, which do not have to be limited to just the next time step but can relate events far away from each other in time. The consciousness prior also makes it natural to map conscious states to natural language utterances or to express classical AI knowledge in a form similar to facts and rules, albeit capturing uncertainty as well as efficient search mechanisms implemented by attention mechanisms.},
  archiveprefix = {arxiv},
  howpublished = {\url{http://arxiv.org/abs/1709.08568}},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/gonzalo/Zotero/storage/K4X43RFZ/Bengio - 2019 - The Consciousness Prior.pdf}
}

@inproceedings{bengioCurseHighlyVariable2005,
  title = {The {{Curse}} of {{Highly Variable Functions}} for {{Local Kernel Machines}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Bengio, Yoshua and Delalleau, Olivier and Roux, Nicolas},
  year = {2005},
  volume = {18},
  publisher = {{MIT Press}},
  abstract = {We present a series of theoretical arguments supporting the claim that a large class of modern learning algorithms that rely solely on the smoothness prior  with similarity between examples expressed with a local kernel  are sensitive to the curse of dimensionality, or more precisely to the variability of the target. Our discussion covers supervised, semisupervised and unsupervised learning algorithms. These algorithms are found to be local in the sense that crucial properties of the learned function at x depend mostly on the neighbors of x in the training set. This makes them sensitive to the curse of dimensionality, well studied for classical non-parametric statistical learning. We show in the case of the Gaussian kernel that when the function to be learned has many variations, these algorithms require a number of training examples proportional to the number of variations, which could be large even though there may exist short descriptions of the target function, i.e. their Kolmogorov complexity may be low. This suggests that there exist non-local learning algorithms that at least have the potential to learn about such structured but apparently complex functions (because locally they have many variations), while not using very specific prior domain knowledge.},
  note = {\url{https://proceedings.neurips.cc/paper/2005/hash/663772ea088360f95bac3dc7ffb841be-Abstract.html}},
  file = {/Users/gonzalo/Zotero/storage/NYEDLXTC/Bengio et al. - 2005 - The Curse of Highly Variable Functions for Local K.pdf}
}

@article{bengioLearningEigenfunctionsLinks2004,
  title = {Learning {{Eigenfunctions Links Spectral Embedding}} and {{Kernel PCA}}},
  author = {Bengio, Yoshua and Delalleau, Olivier and Roux, Nicolas Le and Paiement, Jean-Fran{\c c}ois and Vincent, Pascal and Ouimet, Marie},
  year = {2004},
  month = oct,
  journal = {Neural Computation},
  volume = {16},
  number = {10},
  pages = {2197--2219},
  issn = {0899-7667, 1530-888X},
  doi = {10.1162/0899766041732396},
  abstract = {In this paper, we show a direct relation between spectral embedding methods and kernel PCA, and how both are special cases of a more general learning problem, that of learning the principal eigenfunctions of an operator defined from a kernel and the unknown data generating density.},
  langid = {english},
  note = {\url{https://direct.mit.edu/neco/article/16/10/2197-2219/6863}},
  file = {/Users/gonzalo/Zotero/storage/G9QCDMAH/Bengio et al. - 2004 - Learning Eigenfunctions Links Spectral Embedding a.pdf}
}

@inproceedings{bengioNonLocalManifoldParzen2005,
  title = {Non-{{Local Manifold Parzen Windows}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Bengio, Yoshua and Larochelle, Hugo and Vincent, Pascal},
  year = {2005},
  volume = {18},
  publisher = {{MIT Press}},
  abstract = {To escape from the curse of dimensionality, we claim that one can learn non-local functions, in the sense that the value and shape of the learned function at x must be inferred using examples that may be far from x. With this objective, we present a non-local non-parametric density esti- mator. It builds upon previously proposed Gaussian mixture models with regularized covariance matrices to take into account the local shape of the manifold. It also builds upon recent work on non-local estimators of the tangent plane of a manifold, which are able to generalize in places with little training data, unlike traditional, local, non-parametric models.},
  note = {\url{https://proceedings.neurips.cc/paper/2005/hash/17eb7ecc4c38e4705361cccd903ad8c6-Abstract.html}},
  file = {/Users/gonzalo/Zotero/storage/8KTRUNGT/Bengio et al. - 2005 - Non-Local Manifold Parzen Windows.pdf}
}

@misc{bengioRepresentationLearningReview2014,
  title = {Representation {{Learning}}: {{A Review}} and {{New Perspectives}}},
  shorttitle = {Representation {{Learning}}},
  author = {Bengio, Yoshua and Courville, Aaron and Vincent, Pascal},
  year = {2014},
  month = apr,
  number = {arXiv:1206.5538},
  eprint = {arXiv:1206.5538},
  publisher = {{arXiv}},
  abstract = {The success of machine learning algorithms generally depends on data representation, and we hypothesize that this is because different representations can entangle and hide more or less the different explanatory factors of variation behind the data. Although specific domain knowledge can be used to help design representations, learning with generic priors can also be used, and the quest for AI is motivating the design of more powerful representation-learning algorithms implementing such priors. This paper reviews recent work in the area of unsupervised feature learning and deep learning, covering advances in probabilistic models, auto-encoders, manifold learning, and deep networks. This motivates longer-term unanswered questions about the appropriate objectives for learning good representations, for computing representations (i.e., inference), and the geometrical connections between representation learning, density estimation and manifold learning.},
  archiveprefix = {arxiv},
  howpublished = {\url{http://arxiv.org/abs/1206.5538}},
  langid = {english},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/gonzalo/Zotero/storage/I3TJUBUZ/Bengio et al. - 2014 - Representation Learning A Review and New Perspect.pdf}
}

@inproceedings{beyerWhenNearestNeighbor1999,
  title = {When Is "{{Nearest Neighbor}}" {{Meaningful}}?},
  booktitle = {Database {{Theory}} - {{ICDT}}'99},
  author = {Beyer, Kevin and Goldstein, Jonathan and Ramakrishnan, Raghu and Shaft, Uri},
  editor = {Beeri, Catriel and Buneman, Peter},
  year = {1999},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {217--235},
  publisher = {{Springer}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/3-540-49257-7_15},
  abstract = {We explore the effect of dimensionality on the ``nearest neighbor'' problem. We show that under a broad set of conditions (much broader than independent and identically distributed dimensions), as dimensionality increases, the distance to the nearest data point approaches the distance to the farthest data point. To provide a practical perspective, we present empirical results on both real and synthetic data sets that demonstrate that this effect can occur for as few as 10\textendash 15 dimensions.},
  isbn = {978-3-540-49257-3},
  langid = {english},
  file = {/Users/gonzalo/Zotero/storage/VCFTVUSQ/Beyer et al. - 1999 - When Is “Nearest Neighbor” Meaningful.pdf}
}

@misc{bijralSemisupervisedLearningDensity2012,
  title = {Semi-Supervised {{Learning}} with {{Density Based Distances}}},
  author = {Bijral, Avleen S. and Ratliff, Nathan and Srebro, Nathan},
  year = {2012},
  month = feb,
  number = {arXiv:1202.3702},
  eprint = {arXiv:1202.3702},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1202.3702},
  abstract = {We present a simple, yet effective, approach to Semi-Supervised Learning. Our approach is based on estimating density-based distances (DBD) using a shortest path calculation on a graph. These Graph-DBD estimates can then be used in any distance-based supervised learning method, such as Nearest Neighbor methods and SVMs with RBF kernels. In order to apply the method to very large data sets, we also present a novel algorithm which integrates nearest neighbor computations into the shortest path search and can find exact shortest paths even in extremely large dense graphs. Significant runtime improvement over the commonly used Laplacian regularization method is then shown on a large scale dataset.},
  archiveprefix = {arxiv},
  howpublished = {\url{http://arxiv.org/abs/1202.3702}},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/gonzalo/Zotero/storage/67RXZ5QX/Bijral et al. - 2012 - Semi-supervised Learning with Density Based Distan.pdf;/Users/gonzalo/Zotero/storage/ESN4PLVZ/1202.html}
}

@inproceedings{brandChartingManifold2002,
  title = {Charting a {{Manifold}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Brand, Matthew},
  year = {2002},
  volume = {15},
  publisher = {{MIT Press}},
  abstract = {We construct a nonlinear mapping from a high-dimensional sample space to a low-dimensional vector space, effectively recovering a Cartesian coordinate system for the manifold from which the data is sampled. The mapping preserves local geometric relations in the manifold and is pseudo-invertible. We show how to estimate the intrinsic dimensionality of the manifold from samples, decompose the sample data into locally linear low-dimensional patches, merge these patches into a single low- dimensional coordinate system, and compute forward and reverse map- pings between the sample and coordinate spaces. The objective functions are convex and their solutions are given in closed form.},
  note = {\url{https://proceedings.neurips.cc/paper/2002/hash/8929c70f8d710e412d38da624b21c3c8-Abstract.html}},
  file = {/Users/gonzalo/Zotero/storage/HEWYRQCF/Brand - 2002 - Charting a Manifold.pdf}
}

@article{calderHamiltonJacobiEquationsGraphs2022,
  title = {Hamilton-{{Jacobi}} Equations on Graphs with Applications to Semi-Supervised Learning and Data Depth},
  author = {Calder, Jeff and Ettehad, Mahmood},
  year = {2022},
  journal = {Journal of Machine Learning Research},
  volume = {23},
  number = {318},
  pages = {1--62},
  abstract = {Shortest path graph distances are widely used in data science and machine learning, since they can approximate the underlying geodesic distance on the data manifold. However, the shortest path distance is highly sensitive to the addition of corrupted edges in the graph, either through noise or an adversarial perturbation. In this paper we study a family of Hamilton-Jacobi equations on graphs that we call the p-eikonal equation. We show that the p-eikonal equation with p = 1 is a provably robust distance-type function on a graph, and the p \textrightarrow{} {$\infty$} limit recovers shortest path distances. While the p-eikonal equation does not correspond to a shortest-path graph distance, we nonetheless show that the continuum limit of the p-eikonal equation on a random geometric graph recovers a geodesic density weighted distance in the continuum. We consider applications of the p-eikonal equation to data depth and semi-supervised learning, and use the continuum limit to prove asymptotic consistency results for both applications. Finally, we show the results of experiments with data depth and semi-supervised learning on real image datasets, including MNIST, FashionMNIST and CIFAR-10, which show that the p-eikonal equation offers significantly better results compared to shortest path distances.},
  file = {/Users/gonzalo/Zotero/storage/8C8NGYJL/Calder and Ettehad - 2022 - Hamilton-Jacobi equations on graphs with applicati.pdf}
}

@techreport{carpioFingerprintsCancerPersistent2019,
  type = {Preprint},
  title = {Fingerprints of Cancer by Persistent Homology},
  author = {Carpio, A. and Bonilla, L. L. and Mathews, J. C. and Tannenbaum, A. R.},
  year = {2019},
  month = sep,
  institution = {{Cancer Biology}},
  doi = {10.1101/777169},
  abstract = {We have carried out a topological data analysis of gene expressions for different databases based on the Fermat distance between the z scores of different tissue samples. There is a critical value of the filtration parameter at which all clusters collapse in a single one. This critical value for healthy samples is gapless and smaller than that for cancerous ones. After collapse in a single cluster, topological holes persist for larger filtration parameter values in cancerous samples. Barcodes, persistence diagrams and Betti numbers as functions of the filtration parameter are different for different types of cancer and constitute fingerprints thereof.},
  langid = {english},
  note = {\url{http://biorxiv.org/lookup/doi/10.1101/777169}},
  file = {/Users/gonzalo/Zotero/storage/8ZN8N9N9/Carpio et al. - 2019 - Fingerprints of cancer by persistent homology.pdf}
}

@article{caytonAlgorithmsManifoldLearning2005,
  title = {Algorithms for Manifold Learning},
  author = {Cayton, Lawrence},
  year = {2005},
  abstract = {Manifold learning is a popular recent approach to nonlinear dimensionality reduction. Algorithms for this task are based on the idea that the dimensionality of many data sets is only artificially high; though each data point consists of perhaps thousands of features, it may be described as a function of only a few underlying parameters. That is, the data points are actually samples from a low-dimensional manifold that is embedded in a high-dimensional space. Manifold learning algorithms attempt to uncover these parameters in order to find a low-dimensional representation of the data. In this paper, we discuss the motivation, background, and algorithms proposed for manifold learning. Isomap, Locally Linear Embedding, Laplacian Eigenmaps, Semidefinite Embedding, and a host of variants of these algorithms are examined.},
  langid = {english},
  file = {/Users/gonzalo/Zotero/storage/Q3RCYSTM/Cayton - Algorithms for manifold learning.pdf}
}

@article{chaconDatadrivenDensityDerivative2013,
  title = {Data-Driven Density Derivative Estimation, with Applications to Nonparametric Clustering and Bump Hunting},
  author = {Chac{\'o}n, Jos{\'e} E. and Duong, Tarn},
  year = {2013},
  month = jan,
  journal = {Electronic Journal of Statistics},
  volume = {7},
  number = {none},
  issn = {1935-7524},
  doi = {10.1214/13-EJS781},
  abstract = {Important information concerning a multivariate data set, such as clusters and modal regions, is contained in the derivatives of the probability density function. Despite this importance, nonparametric estimation of higher order derivatives of the density functions have received only relatively scant attention. Kernel estimators of density functions are widely used as they exhibit excellent theoretical and practical properties, though their generalization to density derivatives has progressed more slowly due to the mathematical intractabilities encountered in the crucial problem of bandwidth (or smoothing parameter) selection. This paper presents the first fully automatic, data-based bandwidth selectors for multivariate kernel density derivative estimators. This is achieved by synthesizing recent advances in matrix analytic theory which allow mathematically and computationally tractable representations of higher order derivatives of multivariate vector valued functions. The theoretical asymptotic properties as well as the finite sample behaviour of the proposed selectors are studied. In addition, we explore in detail the applications of the new data-driven methods for two other statistical problems: clustering and bump hunting. The introduced techniques are combined with the mean shift algorithm to develop novel automatic, nonparametric clustering procedures which are shown to outperform mixture-model cluster analysis and other recent nonparametric approaches in practice. Furthermore, the advantage of the use of smoothing parameters designed for density derivative estimation for feature significance analysis for bump hunting is illustrated with a real data example.},
  langid = {english},
  note = {\url{https://projecteuclid.org/journals/electronic-journal-of-statistics/volume-7/issue-none/Data-driven-density-derivative-estimation-with-applications-to-nonparametric-clustering/10.1214/13-EJS781.full}},
  file = {/Users/gonzalo/Zotero/storage/52VG4PRB/Chacón and Duong - 2013 - Data-driven density derivative estimation, with ap.pdf}
}

@book{chavelRiemannianGeometryModern2006,
  title = {Riemannian Geometry: A Modern Introduction},
  shorttitle = {Riemannian Geometry},
  author = {Chavel, Isaac},
  year = {2006},
  series = {Cambridge Studies in Advanced Mathematics},
  edition = {2nd ed},
  number = {98},
  publisher = {{Cambridge University Press}},
  address = {{New York}},
  isbn = {978-0-521-85368-2 978-0-521-61954-7},
  langid = {english},
  lccn = {QA649 .C45 2006},
  keywords = {Geometry; Riemannian},
  annotation = {OCLC: ocm62089870},
  file = {/Users/gonzalo/Zotero/storage/IVWATM2C/Chavel - 2006 - Riemannian geometry a modern introduction.pdf}
}

@misc{chenComprehensiveApproachMode2015,
  title = {A {{Comprehensive Approach}} to {{Mode Clustering}}},
  author = {Chen, Yen-Chi and Genovese, Christopher R. and Wasserman, Larry},
  year = {2015},
  month = dec,
  number = {arXiv:1406.1780},
  eprint = {arXiv:1406.1780},
  publisher = {{arXiv}},
  abstract = {Mode clustering is a nonparametric method for clustering that defines clusters using the basins of attraction of a density estimator's modes. We provide several enhancements to mode clustering: (i) a soft variant of cluster assignment, (ii) a measure of connectivity between clusters, (iii) a technique for choosing the bandwidth, (iv) a method for denoising small clusters, and (v) an approach to visualizing the clusters. Combining all these enhancements gives us a complete procedure for clustering in multivariate problems. We also compare mode clustering to other clustering methods in several examples.},
  archiveprefix = {arxiv},
  howpublished = {\url{http://arxiv.org/abs/1406.1780}},
  langid = {english},
  keywords = {62H30 (Primary); 62G07; 62G99 (Secondary),Statistics - Machine Learning,Statistics - Methodology},
  file = {/Users/gonzalo/Zotero/storage/2M3P8733/Chen et al. - 2015 - A Comprehensive Approach to Mode Clustering.pdf}
}

@incollection{chuExactComputationManifold2019,
  title = {Exact Computation of a Manifold Metric, via {{Lipschitz Embeddings}} and {{Shortest Paths}} on a {{Graph}}},
  booktitle = {Proceedings of the 2020 {{ACM-SIAM Symposium}} on {{Discrete Algorithms}} ({{SODA}})},
  author = {Chu, Timothy and Miller, Gary L. and Sheehy, Donald R.},
  year = {2019},
  month = dec,
  series = {Proceedings},
  pages = {411--425},
  publisher = {{Society for Industrial and Applied Mathematics}},
  doi = {10.1137/1.9781611975994.25},
  abstract = {Data-sensitive metrics adapt distances locally based the density of data points with the goal of aligning distances and some notion of similarity. In this paper, we give the first exact algorithm for computing a data-sensitive metric called the nearest neighbor metric. In fact, we prove the surprising result that a previously published 3-approximation is an exact algorithm. The nearest neighbor metric can be viewed as a special case of a density-based distance used in machine learning, or it can be seen as an example of a manifold metric. Previous computational research on such metrics despaired of computing exact distances on account of the apparent difficulty of minimizing over all continuous paths between a pair of points. We leverage the exact computation of the nearest neighbor metric to compute sparse spanners and persistent homology. We also explore the behavior of the metric built from point sets drawn from an underlying distribution and consider the more general case of inputs that are finite collections of path-connected compact sets. The main results connect several classical theories such as the conformal change of Riemannian metrics, the theory of positive definite functions of Schoenberg, and screw function theory of Schoenberg and Von Neumann. We also develop some novel proof techniques based on the combination of screw functions and Lipschitz extensions that may be of independent interest.},
  note = {\url{https://epubs.siam.org/doi/abs/10.1137/1.9781611975994.25}},
  file = {/Users/gonzalo/Zotero/storage/PYHYGYEZ/Chu et al. - 2019 - Exact computation of a manifold metric, via Lipsch.pdf;/Users/gonzalo/Zotero/storage/YJVHNGZG/arXiv v3 - Exploration of a Graph-based Density-Sensitive Metric.pdf}
}

@misc{davenportBayesRuleRandom2014,
  title = {Bayes Rule for Random Variables},
  author = {Davenport, M. and Romberg, J. and Rozell, J},
  year = {2014},
  annotation = {ECE 3077 Notes by M. Davenport, J. Romberg and C. Rozell. Last updated 21:27, June 25, 2014},
  file = {/Users/gonzalo/Zotero/storage/83NTKU79/16_BayesRVs-su14.pdf}
}

@book{devroyeProbabilisticTheoryPattern1996,
  title = {A Probabilistic Theory of Pattern Recognition},
  author = {Devroye, Luc and Gy{\"o}rfi, L{\'a}szl{\'o} and Lugosi, G{\'a}bor},
  year = {1996},
  series = {Applications of Mathematics},
  edition = {Repr},
  number = {31},
  publisher = {{Springer}},
  address = {{New York Berlin Heidelberg}},
  isbn = {978-1-4612-0711-5 978-1-4612-6877-2},
  langid = {english},
  file = {/Users/gonzalo/Zotero/storage/AFEZXQ7W/Devroye et al. - 1996 - A probabilistic theory of pattern recognition.pdf}
}

@article{duongCrossvalidationBandwidthMatrices2005,
  title = {Cross-Validation {{Bandwidth Matrices}} for {{Multivariate Kernel Density Estimation}}},
  author = {Duong, Tarn and Hazelton, Martin L.},
  year = {2005},
  month = sep,
  journal = {Scandinavian Journal of Statistics},
  volume = {32},
  number = {3},
  pages = {485--506},
  issn = {0303-6898, 1467-9469},
  doi = {10.1111/j.1467-9469.2005.00445.x},
  abstract = {The performance of multivariate kernel density estimates depends crucially on the choice of bandwidth matrix, but progress towards developing good bandwidth matrix selectors has been relatively slow. In particular, previous studies of cross-validation (CV) methods have been restricted to biased and unbiased CV selection of diagonal bandwidth matrices. However, for certain types of target density the use of full (i.e. unconstrained) bandwidth matrices offers the potential for significantly improved density estimation. In this paper, we generalize earlier work from diagonal to full bandwidth matrices, and develop a smooth cross-validation (SCV) methodology for multivariate data. We consider optimization of the SCV technique with respect to a pilot bandwidth matrix. All the CV methods are studied using asymptotic analysis, simulation experiments and real data analysis. The results suggest that SCV for full bandwidth matrices is the most reliable of the CV methods. We also observe that experience from the univariate setting can sometimes be a misleading guide for understanding bandwidth selection in the multivariate case.},
  langid = {english},
  note = {\url{https://onlinelibrary.wiley.com/doi/10.1111/j.1467-9469.2005.00445.x}},
  file = {/Users/gonzalo/Zotero/storage/SNZ4M2YX/Duong and Hazelton - 2005 - Cross-validation Bandwidth Matrices for Multivaria.pdf}
}

@article{fisherDispersionSphere1957,
  title = {Dispersion on a Sphere},
  author = {Fisher, Ronald Aylmer},
  year = {1957},
  journal = {Proceedings of the Royal Society of London. Series A. Mathematical and Physical Sciences},
  volume = {217},
  number = {1130},
  pages = {295--305},
  publisher = {{Royal Society}},
  doi = {10.1098/rspa.1953.0064},
  abstract = {Any topological framework requires the development of a theory of errors of characteristic and appropriate mathematical form. The paper develops a form of theory which appears to be appropriate to measurements of position on a sphere. The primary problems of estimation as applied to the true direction, and the precision of observations, are discussed in the subcases which arise. The simultaneous distribution of the amplitude and direction of the vector sum of a number of random unit vectors of given precision, is demonstrated. From this is derived the test of significance appropriate to a worker whose knowledge of precision lies entirely in the internal evidence of the sample. This is the analogue of `Student's' test in the Gaussian theory of errors. The general formulae obtained are illustrated using measurements of the direction of remanent magnetization in the directly and inversely magnetized lava flows obtained in Iceland by Mr J. Hospers.},
  note = {\url{https://royalsocietypublishing.org/doi/abs/10.1098/rspa.1953.0064}},
  file = {/Users/gonzalo/Zotero/storage/Q43MB6FW/Fisher - 1997 - Dispersion on a sphere.pdf}
}

@book{fukunagaIntroductionStatisticalPattern2013,
  title = {Introduction to {{Statistical Pattern Recognition}}},
  author = {Fukunaga, Keinosuke},
  year = {2013},
  month = oct,
  publisher = {{Elsevier}},
  abstract = {This completely revised second edition presents an introduction to statistical pattern recognition. Pattern recognition in general covers a wide range of problems: it is applied to engineering problems, such as character readers and wave form analysis as well as to brain modeling in biology and psychology. Statistical decision and estimation, which are the main subjects of this book, are regarded as fundamental to the study of pattern recognition. This book is appropriate as a text for introductory courses in pattern recognition and as a reference book for workers in the field. Each chapter contains computer projects as well as exercises.},
  googlebooks = {BIJZTGjTxBgC},
  isbn = {978-0-08-047865-4},
  langid = {english},
  keywords = {Computers / Artificial Intelligence / Computer Vision \& Pattern Recognition},
  file = {/Users/gonzalo/Zotero/storage/TMT4IZRS/Fukunaga - Introduction to Statistical Pattern Recognition Second Edition.pdf}
}

@book{garcia-portuguesShortCourseNonparametric2022,
  title = {A {{Short Course}} on {{Nonparametric Curve Estimation}}},
  author = {{Garc{\'i}a-Portugu{\'e}s}, Eduardo},
  year = {2022},
  month = apr,
  abstract = {A Short Course on Nonparametric Curve Estimation. MSc in Applied Mathematics. EAFIT University (Colombia).},
  annotation = {Last updated: 2022-04-26, v2.1.1},
  note = {\url{https://bookdown.org/egarpor/NP-EAFIT/}},
  file = {/Users/gonzalo/Zotero/storage/U7DYZWK4/NP-EAFIT.html}
}

@misc{groismanNonhomogeneousEuclideanFirstpassage2019,
  title = {Nonhomogeneous {{Euclidean}} First-Passage Percolation and Distance Learning},
  author = {Groisman, Pablo and Jonckheere, Matthieu and Sapienza, Facundo},
  year = {2019},
  month = dec,
  number = {arXiv:1810.09398},
  eprint = {arXiv:1810.09398},
  publisher = {{arXiv}},
  abstract = {Consider an i.i.d. sample from an unknown density function supported on an unknown manifold embedded in a high dimensional Euclidean space. We tackle the problem of learning a distance between points, able to capture both the geometry of the manifold and the underlying density. We define such a sample distance and prove the convergence, as the sample size goes to infinity, to a macroscopic one that we call Fermat distance as it minimizes a path functional, resembling Fermat principle in optics. The proof boils down to the study of geodesics in Euclidean first-passage percolation for nonhomogeneous Poisson point processes.},
  archiveprefix = {arxiv},
  howpublished = {\url{http://arxiv.org/abs/1810.09398}},
  langid = {english},
  keywords = {G.3,Mathematics - Probability},
  file = {/Users/gonzalo/Zotero/storage/4AEYHE2D/Groisman et al. - 2019 - Nonhomogeneous Euclidean first-passage percolation.pdf}
}

@article{hallBandwidthChoiceNonparametric2005,
  title = {Bandwidth Choice for Nonparametric Classification},
  author = {Hall, Peter and Kang, Kee-Hoon},
  year = {2005},
  month = feb,
  journal = {The Annals of Statistics},
  volume = {33},
  number = {1},
  issn = {0090-5364},
  doi = {10.1214/009053604000000959},
  langid = {english},
  note = {\url{https://projecteuclid.org/journals/annals-of-statistics/volume-33/issue-1/Bandwidth-choice-for-nonparametric-classification/10.1214/009053604000000959.full}},
  file = {/Users/gonzalo/Zotero/storage/RI6RY8ZG/Hall and Kang - 2005 - Bandwidth choice for nonparametric classification.pdf}
}

@book{hastieElementsStatisticalLearning2009,
  title = {Elements of {{Statistical Learning Data Mining}}, {{Inference}}, and {{Prediction}}},
  author = {Hastie, Trevor and Tibshirani, Robert and Friedman, Jerome},
  year = {2009},
  publisher = {{Springer London, Limited}},
  isbn = {978-0-387-21606-5},
  file = {/Users/gonzalo/Zotero/storage/MPWZK9V3/Hastie et al. - 2009 - Elements of Statistical Learning Data Mining, Infe.pdf}
}

@article{henryKernelDensityEstimation2009,
  title = {Kernel {{Density Estimation}} on {{Riemannian Manifolds}}: {{Asymptotic Results}}},
  shorttitle = {Kernel {{Density Estimation}} on {{Riemannian Manifolds}}},
  author = {Henry, Guillermo and Rodriguez, Daniela},
  year = {2009},
  month = jul,
  journal = {Journal of Mathematical Imaging and Vision},
  volume = {34},
  number = {3},
  pages = {235--239},
  issn = {0924-9907, 1573-7683},
  doi = {10.1007/s10851-009-0145-2},
  abstract = {The paper concerns the strong uniform consistency and the asymptotic distribution of the kernel density estimator of random objects on a Riemannian manifolds, proposed by Pelletier (Stat. Probab. Lett., 73(3):297\textendash 304, 2005). The estimator is illustrated via one example based on a real data.},
  langid = {english},
  note = {\url{http://link.springer.com/10.1007/s10851-009-0145-2}},
  file = {/Users/gonzalo/Zotero/storage/CUVPHEPW/Henry and Rodriguez - 2009 - Kernel Density Estimation on Riemannian Manifolds.pdf}
}

@article{huberProjectionPursuit1985,
  title = {Projection {{Pursuit}}},
  author = {Huber, Peter J.},
  year = {1985},
  journal = {The Annals of Statistics},
  volume = {13},
  number = {2},
  eprint = {2241175},
  eprinttype = {jstor},
  pages = {435--475},
  langid = {english},
  note = {\url{http://www.jstor.org/stable/2241175}},
  file = {/Users/gonzalo/Zotero/storage/8FGJUMHJ/Huber - 1985 - Projection Pursuit.pdf}
}

@article{jenq-nenghwangNonparametricMultivariateDensity1994,
  title = {Nonparametric Multivariate Density Estimation: A Comparative Study},
  shorttitle = {Nonparametric Multivariate Density Estimation},
  author = {{Jenq-Neng Hwang} and {Shyh-Rong Lay} and Lippman, A.},
  year = {Oct./1994},
  journal = {IEEE Transactions on Signal Processing},
  volume = {42},
  number = {10},
  pages = {2795--2810},
  issn = {1053587X},
  doi = {10.1109/78.324744},
  abstract = {This paper algorithmically and empirically studies two major types of nonparametricmultivariatedensity estimation techniques, where no assumption is made about the data being drawn from any of known parametric families of distribution. The first type is the popular kernel method (and several of its variants) which uses locally tuned radial basis (e.g., Gaussian) functions to interpolate the multidimensional density; the second type is based on an exploratory projection pursuit technique which interprets the multidimensionaldensity through the construction of several 1-D densities along highly ``interesting'' projections of multidimensional data. Performance evaluations using training data from mixture Gaussian and mixture Cauchy densities are presented. The results show that the curse of dimensionality and the sensitivity of control parameters have a much more adverse impact on the kernel density estimators than on the projection pursuit density estimators.},
  langid = {english},
  note = {\url{http://ieeexplore.ieee.org/document/324744/}},
  file = {/Users/gonzalo/Zotero/storage/8MBX5UI9/Jenq-Neng Hwang et al. - 1994 - Nonparametric multivariate density estimation a c.pdf}
}

@article{juppUnifiedViewTheory1989,
  title = {A {{Unified View}} of the {{Theory}} of {{Directional Statistics}}, 1975-1988},
  author = {Jupp, P. E. and Mardia, K. V.},
  year = {1989},
  journal = {International Statistical Review / Revue Internationale de Statistique},
  volume = {57},
  number = {3},
  eprint = {1403799},
  eprinttype = {jstor},
  pages = {261--294},
  publisher = {{[Wiley, International Statistical Institute (ISI)]}},
  issn = {0306-7734},
  doi = {10.2307/1403799},
  abstract = {Numerous articles on the theory and practice of directional statistics have appeared since Mardia's (1975a) survey. This paper aims to present a coherent view of the theory of the topic by relating these developments to the following key ideas: exponential families, transformation models, 'tangent-normal' decompositions, transformations to multivariate problems and the central limit theorem. Further unification is attained by identifying three basic approaches to directional statistics, in which the basic sample space, the sphere, is regarded respectively as a subset of Euclidean space, an object in its own right and as something approximated by a tangent plane. Parametric, nonparametric and informal methods are considered. The discussion is mainly of observations on the circle or sphere but a section on non-spherical sample spaces is included. /// De nombreux articles sur la th\'eorie et la pratique de la statistique directionnelle ont paru depuis la revue du sujet par Mardia (1975a). Cet article pr\'esente une approche coh\'erente de ces d\'evelopments. L'attention est port\'ee sur les rapports entre la th\'eorie sous-jacente et quelques id\'ees clefs: familles exponentielles, mod\`eles de transformation, d\'ecompositions tangentes-normales, transformations en probl\`emes multidimensionnels, et le th\'eor\`eme limite centrale. De plus on identifie trois points de vue fondamentaux de la statistique directionelle, dans lesquels l'espace d'\'echantillonage de base, la sph\`ere, est consid\'er\'e respectivement comme \'etant un sous-ensemble de l'espace euclidien, un objet en lui-m\^eme et une entit\'e approxim\'ee par une surface tangente. On consid\`ere des m\'ethodes param\'etriques, non-param\'etriques et informelles. La discussion se fonde surtout sur le cas o\`u les observations proviennent du cercle ou de la sph\`ere, mais une section sur les espaces d'\'echantillonage non-sph\'eriques est incluse.},
  note = {\url{https://www.jstor.org/stable/1403799}},
  file = {/Users/gonzalo/Zotero/storage/3HZEWRUS/Jupp and Mardia - 1989 - A Unified View of the Theory of Directional Statis.pdf}
}

@article{korolyukAsymptoticTheoryUstatistics1988,
  title = {Asymptotic Theory of {{U-statistics}}},
  author = {Korolyuk, V. S. and Borovskikh, {\relax Yu. V}},
  year = {1988},
  month = mar,
  journal = {Ukrainian Mathematical Journal},
  volume = {40},
  number = {2},
  pages = {142--154},
  issn = {1573-9376},
  doi = {10.1007/BF01056469},
  langid = {english},
  keywords = {Asymptotic Theory},
  note = {\url{https://doi.org/10.1007/BF01056469}},
  file = {/Users/gonzalo/Zotero/storage/TQJDTLYC/Korolyuk and Borovskikh - 1988 - Asymptotic theory of U-statistics.pdf}
}

@article{LinearDiscriminantAnalysis2022,
  title = {Linear Discriminant Analysis},
  year = {2022},
  month = dec,
  journal = {Wikipedia},
  abstract = {Linear discriminant analysis (LDA), normal discriminant analysis (NDA), or discriminant function analysis is a generalization of Fisher's linear discriminant, a method used in statistics and other fields, to find a linear combination of features that characterizes or separates two or more classes of objects or events. The resulting combination may be used as a linear classifier, or, more commonly, for dimensionality reduction before later classification. LDA is closely related to analysis of variance (ANOVA) and regression analysis, which also attempt to express one dependent variable as a linear combination of other features or measurements. However, ANOVA uses categorical independent variables and a continuous dependent variable, whereas discriminant analysis has continuous independent variables and a categorical dependent variable (i.e. the class label). Logistic regression and probit regression are more similar to LDA than ANOVA is, as they also explain a categorical variable by the values of continuous independent variables. These other methods are preferable in applications where it is not reasonable to assume that the independent variables are normally distributed, which is a fundamental assumption of the LDA method. LDA is also closely related to principal component analysis (PCA) and factor analysis in that they both look for linear combinations of variables which best explain the data. LDA explicitly attempts to model the difference between the classes of data. PCA, in contrast, does not take into account any difference in class, and factor analysis builds the feature combinations based on differences rather than similarities. Discriminant analysis is also different from factor analysis in that it is not an interdependence technique: a distinction between independent variables and dependent variables (also called criterion variables) must be made. LDA works when the measurements made on independent variables for each observation are continuous quantities. When dealing with categorical independent variables, the equivalent technique is discriminant correspondence analysis.Discriminant analysis is used when groups are known a priori (unlike in cluster analysis). Each case must have a score on one or more quantitative predictor measures, and a score on a group measure. In simple terms, discriminant function analysis is classification - the act of distributing things into groups, classes or categories of the same type.},
  copyright = {Creative Commons Attribution-ShareAlike License},
  langid = {english},
  annotation = {Page Version ID: 1130599337},
  note = {\url{https://en.wikipedia.org/w/index.php?title=Linear_discriminant_analysis&oldid=1130599337#cite_note-cohen-8}},
  file = {/Users/gonzalo/Zotero/storage/YRFULRH6/Linear_discriminant_analysis.html}
}

@article{linShellTheoryStatistical2021,
  title = {Shell {{Theory}}: {{A Statistical Model}} of {{Reality}}},
  shorttitle = {Shell {{Theory}}},
  author = {Lin, Wen-Yan and Liu, Siying and Ren, Changhao and Cheung, Ngai-Man and Li, Hongdong and Matsushita, Yasuyuki},
  year = {2021},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  pages = {1--1},
  issn = {0162-8828, 2160-9292, 1939-3539},
  doi = {10.1109/TPAMI.2021.3084598},
  abstract = {The foundational assumption of machine learning is that the data under consideration is separable into classes; while intuitively reasonable, separability constraints have proven remarkably difficult to formulate mathematically. We believe this problem is rooted in the mismatch between existing statistical techniques and commonly encountered data; object representations are typically high dimensional but statistical techniques tend to treat high dimensions a degenerate case. To address this problem, we develop a dedicated statistical framework for machine learning in high dimensions. The framework derives from the observation that object relations form a natural hierarchy; this leads us to model objects as instances of a high dimensional, hierarchal generative processes. Using a distance based statistical technique, also developed in this paper, we show that in such generative processes, instances of each process in the hierarchy, are almost-always encapsulated by a distinctive-shell that excludes almost-all other instances. The result is shell theory, a statistical machine learning framework in which separability constraints (distinctive-shells) are formally derived from the assumed generative process.},
  langid = {english},
  note = {\url{https://ieeexplore.ieee.org/document/9444188/}},
  file = {/Users/gonzalo/Zotero/storage/3L999SHM/Lin et al. - 2021 - Shell Theory A Statistical Model of Reality.pdf}
}

@misc{littleBalancingGeometryDensity2021,
  title = {Balancing {{Geometry}} and {{Density}}: {{Path Distances}} on {{High-Dimensional Data}}},
  shorttitle = {Balancing {{Geometry}} and {{Density}}},
  author = {Little, Anna and McKenzie, Daniel and Murphy, James},
  year = {2021},
  month = jun,
  number = {arXiv:2012.09385},
  eprint = {arXiv:2012.09385},
  publisher = {{arXiv}},
  abstract = {New geometric and computational analyses of power-weighted shortest-path distances (PWSPDs) are presented. By illuminating the way these metrics balance geometry and density in the underlying data, we clarify their key parameters and illustrate how they provide multiple perspectives for data analysis. Comparisons are made with related data-driven metrics, which illustrate the broader role of density in kernel-based unsupervised and semi-supervised machine learning. Computationally, we relate PWSPDs on complete weighted graphs to their analogues on weighted nearest neighbor graphs, providing high probability guarantees on their equivalence that are near-optimal. Connections with percolation theory are developed to establish estimates on the bias and variance of PWSPDs in the finite sample setting. The theoretical results are bolstered by illustrative experiments, demonstrating the versatility of PWSPDs for a wide range of data settings. Throughout the paper, our results generally require only that the underlying data is sampled from a compact low-dimensional manifold, and depend most crucially on the intrinsic dimension of this manifold, rather than its ambient dimension.},
  archiveprefix = {arxiv},
  howpublished = {\url{http://arxiv.org/abs/2012.09385}},
  langid = {english},
  keywords = {05C85; 05C80,Computer Science - Data Structures and Algorithms,Computer Science - Machine Learning,I.5.3,Statistics - Machine Learning},
  file = {/Users/gonzalo/Zotero/storage/6XMA593D/Little et al. - 2021 - Balancing Geometry and Density Path Distances on .pdf}
}

@article{loubesKernelbasedClassifierRiemannian2008,
  title = {A Kernel-Based Classifier on a {{Riemannian}} Manifold},
  author = {Loubes, Jean-Michel and Pelletier, Bruno},
  year = {2008},
  month = mar,
  journal = {Statistics \& Decisions},
  volume = {26},
  number = {1},
  pages = {35--51},
  issn = {0721-2631},
  doi = {10.1524/stnd.2008.0911},
  abstract = {Let X be a random variable taking values in a compact Riemannian manifold without boundary, and let Y be a discrete random variable valued in \{0; 1\} which represents a classification label. We introduce a kernel rule for classification on the manifold based on n independent copies of (X, Y ). Under mild assumptions on the bandwidth sequence, it is shown that this kernel rule is consistent in the sense that its probability of error converges to the Bayes risk with probability one.},
  langid = {english},
  note = {\url{https://www.degruyter.com/document/doi/10.1524/stnd.2008.0911/html}},
  file = {/Users/gonzalo/Zotero/storage/QLSVS3PR/Loubes and Pelletier - 2008 - A kernel-based classifier on a Riemannian manifold.pdf}
}

@inproceedings{mardiaDistributionTheoryMisesFisher1975,
  title = {Distribution {{Theory}} for the {{Von Mises-Fisher Distribution}} and {{Its Application}}},
  booktitle = {A {{Modern Course}} on {{Statistical Distributions}} in {{Scientific Work}}},
  author = {Mardia, K. V.},
  editor = {Patil, G. P. and Kotz, S. and Ord, J. K.},
  year = {1975},
  series = {{{NATO Advanced Study Institutes Series}}},
  pages = {113--130},
  publisher = {{Springer Netherlands}},
  address = {{Dordrecht}},
  doi = {10.1007/978-94-010-1842-5_10},
  abstract = {The von Mises-Fisher distribution is the most important distribution in directional data analysis. We derive the sampling distributions of the sample resultant length, the sample mean direction and the component lengths. For the multi-sample case, the conditional distribution of the individual sample resultant lengths given the combined sample resultant length is derived. These results depend heavily on the corresponding distributions for the isotropic random walk on hypersphere. Using these results we investigate some optimum properties of various important tests. Most of these tests were formulated intuitively by Watson and Williams (1956). Mardia (1972) in his book concentrated on the optimum properties of the circular and spherical cases, and this paper extends and unifies some of the parametric work.},
  isbn = {978-94-010-1842-5},
  langid = {english},
  keywords = {Directional data analysis,multi-sample problems,von Mises-Fisher distribution},
  file = {/Users/gonzalo/Zotero/storage/2DBPSTZQ/Mardia - 1975 - Distribution Theory for the Von Mises-Fisher Distr.pdf}
}

@misc{mckenziePowerWeightedShortest2019,
  title = {Power {{Weighted Shortest Paths}} for {{Clustering Euclidean Data}}},
  author = {Mckenzie, Daniel and Damelin, Steven},
  year = {2019},
  month = sep,
  number = {arXiv:1905.13345},
  eprint = {arXiv:1905.13345},
  publisher = {{arXiv}},
  abstract = {We study the use of power weighted shortest path metrics for clustering high dimensional Euclidean data, under the assumption that the data is drawn from a collection of disjoint low dimensional manifolds. We argue, theoretically and experimentally, that this leads to higher clustering accuracy. We also present a fast algorithm for computing these distances.},
  archiveprefix = {arxiv},
  howpublished = {\url{http://arxiv.org/abs/1905.13345}},
  langid = {english},
  keywords = {05C85; 05C80,Computer Science - Machine Learning,I.5.3,Statistics - Machine Learning},
  file = {/Users/gonzalo/Zotero/storage/JY9XJQQX/Mckenzie and Damelin - 2019 - Power Weighted Shortest Paths for Clustering Eucli.pdf}
}

@phdthesis{munozEstimacionNoParametrica2011,
  title = {{Estimaci\'on no param\'etrica de la densidad en variedades Riemannianas}},
  author = {Mu{\~n}oz, Andres Leandro},
  year = {2011},
  langid = {spanish},
  school = {Universidad de Buenos Aires},
  file = {/Users/gonzalo/Zotero/storage/8NLUQYY3/Munoz - Estimacio´n no param´etrica de la densidad en vari.pdf}
}

@article{parzenEstimationProbabilityDensity1962,
  title = {On Estimation of a Probability Density Function and Mode},
  author = {Parzen, Emanuel},
  year = {1962},
  journal = {The annals of mathematical statistics},
  volume = {33},
  number = {3},
  pages = {1065--1076},
  publisher = {{JSTOR}},
  file = {/Users/gonzalo/Zotero/storage/YCCG8PGX/Parzen - 1962 - On estimation of a probability density function an.pdf;/Users/gonzalo/Zotero/storage/35R8VYH2/2237880.html}
}

@article{pelletierKernelDensityEstimation2005,
  title = {Kernel Density Estimation on {{Riemannian}} Manifolds},
  author = {Pelletier, Bruno},
  year = {2005},
  month = jul,
  journal = {Statistics \& Probability Letters},
  volume = {73},
  number = {3},
  pages = {297--304},
  issn = {01677152},
  doi = {10.1016/j.spl.2005.04.004},
  abstract = {The estimation of the underlying probability density of n i.i.d. random objects on a compact Riemannian manifold without boundary is considered. The proposed methodology adapts the technique of kernel density estimation on Euclidean sample spaces to this nonEuclidean setting. Under sufficient regularity assumptions on the underlying density, L2 convergence rates are obtained.},
  langid = {english},
  note = {\url{https://linkinghub.elsevier.com/retrieve/pii/S0167715205001239}},
  file = {/Users/gonzalo/Zotero/storage/DEZE5FJQ/Pelletier - 2005 - Kernel density estimation on Riemannian manifolds.pdf}
}

@inproceedings{rifaiManifoldTangentClassifier2011,
  title = {The {{Manifold Tangent Classifier}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Rifai, Salah and Dauphin, Yann N and Vincent, Pascal and Bengio, Yoshua and Muller, Xavier},
  year = {2011},
  volume = {24},
  publisher = {{Curran Associates, Inc.}},
  abstract = {We combine three important ideas present in previous work for building classi- fiers: the semi-supervised hypothesis (the input distribution contains information about the classifier), the unsupervised manifold hypothesis (data density concen- trates near low-dimensional manifolds), and the manifold hypothesis for classifi- cation (different classes correspond to disjoint manifolds separated by low den- sity). We exploit a novel algorithm for capturing manifold structure (high-order contractive auto-encoders) and we show how it builds a topological atlas of charts, each chart being characterized by the principal singular vectors of the Jacobian of a representation mapping. This representation learning algorithm can be stacked to yield a deep architecture, and we combine it with a domain knowledge-free version of the TangentProp algorithm to encourage the classifier to be insensitive to local directions changes along the manifold. Record-breaking classification results are obtained.},
  note = {\url{https://papers.nips.cc/paper/2011/hash/d1f44e2f09dc172978a4d3151d11d63e-Abstract.html}},
  file = {/Users/gonzalo/Zotero/storage/D2MEYF5H/Rifai et al. - 2011 - The Manifold Tangent Classifier.pdf}
}

@article{rosenblattRemarksNonparametricEstimates1956,
  title = {Remarks on {{Some Nonparametric Estimates}} of a {{Density Function}}},
  author = {Rosenblatt, Murray},
  year = {1956},
  journal = {The Annals of Mathematical Statistics},
  volume = {27},
  number = {3},
  eprint = {2237390},
  eprinttype = {jstor},
  pages = {832--837},
  publisher = {{Institute of Mathematical Statistics}},
  issn = {0003-4851},
  abstract = {This note discusses some aspects of the estimation of the density function of a univariate probability distribution. All estimates of the density function satisfying relatively mild conditions are shown to be biased. The asymptotic mean square error of a particular class of estimates is evaluated.},
  note = {\url{https://www.jstor.org/stable/2237390}},
  file = {/Users/gonzalo/Zotero/storage/NGJQQ8SZ/Rosenblatt - 1956 - Remarks on Some Nonparametric Estimates of a Density Function.pdf}
}

@inproceedings{sapienzaWeightedGeodesicDistance2018,
  title = {Weighted {{Geodesic Distance Following Fermat}}'s {{Principle}}},
  author = {Sapienza, Facundo and Jonckheere, Matthieu and Groisman, Pablo},
  year = {2018},
  abstract = {We propose a density-based estimator for weighted geodesic distances suitable for data lying on a manifold of lower dimension than ambient space and sampled from a possibly nonuniform distribution. After discussing its properties and implementation, we evaluate its performance as a tool for clustering tasks. A discussion on the consistency of the estimator is also given.},
  langid = {english},
  note = {\url{https://openreview.net/forum?id=BJfaMIJwG}},
  file = {/Users/gonzalo/Zotero/storage/983EUZCU/Sapienza et al. - 2018 - WEIGHTED GEODESIC DISTANCE FOLLOWING FERMAT’S PRIN.pdf}
}

@article{silvermanUsingKernelDensity1981,
  title = {Using {{Kernel Density Estimates}} to {{Investigate Multimodality}}},
  author = {Silverman, B. W.},
  year = {1981},
  month = sep,
  journal = {Journal of the Royal Statistical Society: Series B (Methodological)},
  volume = {43},
  number = {1},
  pages = {97--99},
  issn = {00359246},
  doi = {10.1111/j.2517-6161.1981.tb01155.x},
  abstract = {A technique for using kernel density estimates to investigate the number of modes in a population is described and discussed. The amount of smoothing is chosen automatically in a natural way.},
  langid = {english},
  note = {\url{https://onlinelibrary.wiley.com/doi/10.1111/j.2517-6161.1981.tb01155.x}},
  file = {/Users/gonzalo/Zotero/storage/GY2IAI9M/Silverman - 1981 - Using Kernel Density Estimates to Investigate Mult.pdf}
}

@inproceedings{simardTangentPropFormalism1991,
  title = {Tangent {{Prop}} - {{A}} Formalism for Specifying Selected Invariances in an Adaptive Network},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Simard, Patrice and Victorri, Bernard and LeCun, Yann and Denker, John},
  year = {1991},
  volume = {4},
  publisher = {{Morgan-Kaufmann}},
  abstract = {In many machine learning applications, one has access, not only to training  data, but also to some high-level a priori knowledge about the desired be(cid:173) havior of the system. For example, it is known in advance that the output  of a character recognizer should be invariant with respect to small spa(cid:173) tial distortions of the input images (translations, rotations, scale changes,  etcetera).  We have implemented a scheme that allows a network to learn the deriva(cid:173) tive of its outputs with respect to distortion operators of our choosing.  This not only reduces the learning time and the amount of training data,  but also provides a powerful language for specifying what generalizations  we wish the network to perform.},
  note = {\url{https://proceedings.neurips.cc/paper/1991/hash/65658fde58ab3c2b6e5132a39fae7cb9-Abstract.html}},
  file = {/Users/gonzalo/Zotero/storage/ZFIBR7BL/Simard et al. - 1991 - Tangent Prop - A formalism for specifying selected.pdf}
}

@misc{tangTutorialTangentPropagation2009,
  title = {Tutorial on {{Tangent Propagation}}},
  author = {Tang, Yichuan},
  year = {2009},
  month = feb,
  langid = {english},
  file = {/Users/gonzalo/Zotero/storage/DMFPWIXT/Tang - Tutorial on Tangent Propagation.pdf}
}

@article{taylorClassificationKernelDensity1997,
  title = {Classification and Kernel Density Estimation},
  author = {Taylor, Charles},
  year = {1997},
  month = jan,
  journal = {Vistas in Astronomy},
  volume = {41},
  number = {3},
  pages = {411--417},
  issn = {00836656},
  doi = {10.1016/S0083-6656(97)00046-9},
  abstract = {The method of kernel density estimation can be readily used for the purposes of classification, and an easy-to-use package (ALLOCBO) is now in wide circulation. It is known that this method performs well (at least in relative terms) in the case of bimodal, or heavily skewed distributions.},
  langid = {english},
  note = {\url{https://linkinghub.elsevier.com/retrieve/pii/S0083665697000469}},
  file = {/Users/gonzalo/Zotero/storage/AKKJZT3R/Taylor - 1997 - Classification and kernel density estimation.pdf}
}

@article{tenenbaumGlobalGeometricFramework2000,
  title = {A {{Global Geometric Framework}} for {{Nonlinear Dimensionality Reduction}}},
  author = {Tenenbaum, Joshua B. and de Silva, Vin and Langford, John C.},
  year = {2000},
  month = dec,
  journal = {Science},
  volume = {290},
  number = {5500},
  pages = {2319--2323},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.290.5500.2319},
  abstract = {Scientists working with large volumes of high-dimensional data, such as global climate patterns, stellar spectra, or human gene distributions, regularly confront the problem of dimensionality reduction: finding meaningful low-dimensional structures hidden in their high-dimensional observations. The human brain confronts the same problem in everyday perception, extracting from its high-dimensional sensory inputs\textemdash 30,000 auditory nerve fibers or 10               6               optic nerve fibers\textemdash a manageably small number of perceptually relevant features. Here we describe an approach to solving dimensionality reduction problems that uses easily measured local metric information to learn the underlying global geometry of a data set. Unlike classical techniques such as principal component analysis (PCA) and multidimensional scaling (MDS), our approach is capable of discovering the nonlinear degrees of freedom that underlie complex natural observations, such as human handwriting or images of a face under different viewing conditions. In contrast to previous algorithms for nonlinear dimensionality reduction, ours efficiently computes a globally optimal solution, and, for an important class of data manifolds, is guaranteed to converge asymptotically to the true structure.},
  langid = {english},
  note = {\url{https://www.science.org/doi/10.1126/science.290.5500.2319}},
  file = {/Users/gonzalo/Zotero/storage/T82F3Z69/Tenenbaum et al. - 2000 - A Global Geometric Framework for Nonlinear Dimensi.pdf}
}

@inproceedings{tenenbaumMappingManifoldPerceptual1997,
  title = {Mapping a {{Manifold}} of {{Perceptual Observations}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Tenenbaum, Joshua},
  year = {1997},
  volume = {10},
  publisher = {{MIT Press}},
  abstract = {Nonlinear dimensionality reduction is formulated here as the problem of trying to  find a Euclidean feature-space embedding of a set of observations that preserves  as closely as possible their intrinsic metric structure - the distances between points  on  the  observation manifold as  measured along geodesic paths.  Our isometric  feature mapping procedure, or isomap, is able to reliably recover low-dimensional  nonlinear structure in  realistic  perceptual data  sets,  such as  a manifold  of face  images,  where  conventional global  mapping  methods  find  only  local  minima.  The  recovered  map  provides  a canonical  set  of globally  meaningful  features,  which allows perceptual transformations such as interpolation, extrapolation, and  analogy - highly nonlinear transformations in the original observation space - to  be computed with simple linear operations in feature space.},
  note = {\url{https://proceedings.neurips.cc/paper/1997/hash/28e209b61a52482a0ae1cb9f5959c792-Abstract.html}},
  file = {/Users/gonzalo/Zotero/storage/ZMNFLLEB/Tenenbaum - 1997 - Mapping a Manifold of Perceptual Observations.pdf}
}

@book{tuIntroductionManifolds2011,
  title = {An {{Introduction}} to {{Manifolds}}},
  author = {Tu, Loring W.},
  year = {2011},
  series = {Universitext},
  publisher = {{Springer New York}},
  address = {{New York, NY}},
  doi = {10.1007/978-1-4419-7400-6},
  isbn = {978-1-4419-7399-3 978-1-4419-7400-6},
  langid = {english},
  note = {\url{https://link.springer.com/10.1007/978-1-4419-7400-6}},
  file = {/Users/gonzalo/Zotero/storage/TRQJZ6B6/Tu - 2011 - An Introduction to Manifolds.pdf}
}

@inproceedings{vincentDensitySensitiveMetrics2003,
  title = {Density Sensitive Metrics and Kernels},
  booktitle = {Proceedings of the {{Snowbird Workshop}}},
  author = {Vincent, P. and Bengio, Y.},
  year = {2003}
}

@inproceedings{vincentManifoldParzenWindows2002,
  title = {Manifold {{Parzen Windows}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Vincent, Pascal and Bengio, Yoshua},
  year = {2002},
  volume = {15},
  publisher = {{MIT Press}},
  abstract = {The similarity between objects is a fundamental element of many learn- ing algorithms. Most non-parametric methods take this similarity to be fixed, but much recent work has shown the advantages of learning it, in particular to exploit the local invariances in the data or to capture the possibly non-linear manifold on which most of the data lies. We propose a new non-parametric kernel density estimation method which captures the local structure of an underlying manifold through the leading eigen- vectors of regularized local covariance matrices. Experiments in density estimation show significant improvements with respect to Parzen density estimators. The density estimators can also be used within Bayes classi- fiers, yielding classification rates similar to SVMs and much superior to the Parzen classifier.},
  note = {\url{https://proceedings.neurips.cc/paper/2002/hash/2d969e2cee8cfa07ce7ca0bb13c7a36d-Abstract.html}},
  file = {/Users/gonzalo/Zotero/storage/BAKIAKHX/Vincent and Bengio - 2002 - Manifold Parzen Windows.pdf}
}

@article{vonmisesUberGanzzahligkeitAtomgewicht1918,
  title = {Uber Die" {{Ganzzahligkeit}}" Der {{Atomgewicht}} Und Verwandte {{Fragen}}},
  author = {{von Mises}, Richard},
  year = {1918},
  journal = {Physikal. Z.},
  volume = {19},
  pages = {490--500},
  file = {/Users/gonzalo/Zotero/storage/IXWD935I/von Mises - 1918 - Uber die Ganzzahligkeit der Atomgewicht und verw.pdf;/Users/gonzalo/Zotero/storage/U99ZVMSV/1571135650500724992.html}
}

@article{wandComparisonSmoothingParameterizations1993,
  title = {Comparison of {{Smoothing Parameterizations}} in {{Bivariate Kernel Density Estimation}}},
  author = {Wand, M. P. and Jones, M. C.},
  year = {1993},
  month = jun,
  journal = {Journal of the American Statistical Association},
  volume = {88},
  number = {422},
  pages = {520--528},
  issn = {0162-1459, 1537-274X},
  doi = {10.1080/01621459.1993.10476303},
  langid = {english},
  note = {\url{http://www.tandfonline.com/doi/abs/10.1080/01621459.1993.10476303}},
  file = {/Users/gonzalo/Zotero/storage/WHDX44I9/Wand and Jones - 1993 - Comparison of Smoothing Parameterizations in Bivar.pdf}
}

@book{wandKernelSmoothing1995,
  title = {Kernel {{Smoothing}}},
  author = {Wand, M. P. and Jones, M. C.},
  year = {1995},
  publisher = {{Springer US}},
  address = {{Boston, MA}},
  doi = {10.1007/978-1-4899-4493-1},
  isbn = {978-0-412-55270-0 978-1-4899-4493-1},
  langid = {english},
  note = {\url{http://link.springer.com/10.1007/978-1-4899-4493-1}},
  file = {/Users/gonzalo/Zotero/storage/662G7HUL/Wand and Jones - 1995 - Kernel Smoothing.pdf}
}

@article{wandMultivariatePluginBandwidth1994,
  title = {Multivariate Plug-in Bandwidth Selection},
  author = {Wand, Matt P. and Jones, M. Chris},
  year = {1994},
  journal = {Computational Statistics},
  volume = {9},
  number = {2},
  pages = {97--116},
  publisher = {{Heidelberg: Physica-Verlag,[1992-}},
  file = {/Users/gonzalo/Zotero/storage/EHBPWPM5/Wand and Jones - 1994 - Multivariate plug-in bandwidth selection.pdf}
}

@article{wangNonparametricDensityEstimation2019,
  title = {Nonparametric {{Density Estimation}} for {{High-Dimensional Data}} - {{Algorithms}} and {{Applications}}},
  author = {Wang, Zhipeng and Scott, David W.},
  year = {2019},
  month = jul,
  journal = {WIREs Computational Statistics},
  volume = {11},
  number = {4},
  eprint = {1904.00176},
  primaryclass = {cs, stat},
  issn = {1939-5108, 1939-0068},
  doi = {10.1002/wics.1461},
  abstract = {Density Estimation is one of the central areas of statistics whose purpose is to estimate the probability density function underlying the observed data. It serves as a building block for many tasks in statistical inference, visualization, and machine learning. Density Estimation is widely adopted in the domain of unsupervised learning especially for the application of clustering. As big data become pervasive in almost every area of data sciences, analyzing high-dimensional data that have many features and variables appears to be a major focus in both academia and industry. Highdimensional data pose challenges not only from the theoretical aspects of statistical inference, but also from the algorithmic/computational considerations of machine learning and data analytics. This paper reviews a collection of selected nonparametric density estimation algorithms for high-dimensional data, some of them are recently published and provide interesting mathematical insights. The important application domain of nonparametric density estimation, such as modal clustering, are also included in this paper. Several research directions related to density estimation and high-dimensional data analysis are suggested by the authors.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Computation,Statistics - Machine Learning},
  note = {\url{http://arxiv.org/abs/1904.00176}},
  file = {/Users/gonzalo/Zotero/storage/5C9PF8NB/Wang and Scott - 2019 - Nonparametric Density Estimation for High-Dimensio.pdf}
}

@article{yuDensitybasedGeodesicDistance2016,
  title = {Density-Based Geodesic Distance for Identifying the Noisy and Nonlinear Clusters},
  author = {Yu, Jaehong and Kim, Seoung Bum},
  year = {2016},
  month = sep,
  journal = {Information Sciences},
  volume = {360},
  pages = {231--243},
  issn = {0020-0255},
  doi = {10.1016/j.ins.2016.04.032},
  abstract = {Clustering analysis can facilitate the extraction of implicit patterns in a dataset and elicit its natural groupings without requiring prior classification information. For superior clustering analysis results, a number of distance measures have been proposed. Recently, geodesic distance has been widely applied to clustering algorithms for nonlinear groupings. However, geodesic distance is sensitive to noise and hence, geodesic distance-based clustering may fail to discover nonlinear clusters in the region of the noise. In this study, we propose a density-based geodesic distance that can identify clusters in nonlinear and noisy situations. Experiments on various simulation and benchmark datasets are conducted to examine the properties of the proposed geodesic distance and to compare its performance with that of existing distance measures. The experimental results confirm that a clustering algorithm with the proposed distance measure demonstrated superior performance compared to the competitors; this was especially true when the cluster structures in the data were inherently noisy and nonlinearly patterned.},
  langid = {english},
  keywords = {Geodesic distance,Mutual neighborhood-based density coefficient,Noisy data clustering,Nonlinearity},
  note = {\url{https://www.sciencedirect.com/science/article/pii/S002002551630281X}},
  file = {/Users/gonzalo/Zotero/storage/LL74HFAV/Yu and Kim - 2016 - Density-based geodesic distance for identifying th.pdf;/Users/gonzalo/Zotero/storage/PK6FXSRE/S002002551630281X.html}
}
