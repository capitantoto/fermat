
@inproceedings{rifai_manifold_2011,
	title = {The Manifold Tangent Classifier},
	volume = {24},
	url = {https://papers.nips.cc/paper/2011/hash/d1f44e2f09dc172978a4d3151d11d63e-Abstract.html},
	abstract = {We combine three important ideas present in previous work for building classi- ﬁers: the semi-supervised hypothesis (the input distribution contains information about the classiﬁer), the unsupervised manifold hypothesis (data density concen- trates near low-dimensional manifolds), and the manifold hypothesis for classiﬁ- cation (different classes correspond to disjoint manifolds separated by low den- sity). We exploit a novel algorithm for capturing manifold structure (high-order contractive auto-encoders) and we show how it builds a topological atlas of charts, each chart being characterized by the principal singular vectors of the Jacobian of a representation mapping. This representation learning algorithm can be stacked to yield a deep architecture, and we combine it with a domain knowledge-free version of the {TangentProp} algorithm to encourage the classiﬁer to be insensitive to local directions changes along the manifold. Record-breaking classiﬁcation results are obtained.},
	booktitle = {Advances in Neural Information Processing Systems},
	publisher = {Curran Associates, Inc.},
	author = {Rifai, Salah and Dauphin, Yann N and Vincent, Pascal and Bengio, Yoshua and Muller, Xavier},
	urldate = {2023-03-02},
	date = {2011},
	file = {Full Text PDF:/Users/gonzalo/Zotero/storage/D2MEYF5H/Rifai et al. - 2011 - The Manifold Tangent Classifier.pdf:application/pdf},
}

@book{hastie_elements_2009,
	title = {Elements of Statistical Learning Data Mining, Inference, and Prediction},
	isbn = {978-0-387-21606-5},
	publisher = {Springer London, Limited},
	author = {Hastie, Trevor and Tibshirani, Robert and Friedman, Jerome},
	date = {2009},
	file = {Hastie et al. - 2009 - Elements of Statistical Learning Data Mining, Infe.pdf:/Users/gonzalo/Zotero/storage/MPWZK9V3/Hastie et al. - 2009 - Elements of Statistical Learning Data Mining, Infe.pdf:application/pdf},
}

@book{chavel_riemannian_2006,
	location = {New York},
	edition = {2nd ed},
	title = {Riemannian geometry: a modern introduction},
	isbn = {978-0-521-85368-2 978-0-521-61954-7},
	series = {Cambridge studies in advanced mathematics},
	shorttitle = {Riemannian geometry},
	pagetotal = {471},
	number = {98},
	publisher = {Cambridge University Press},
	author = {Chavel, Isaac},
	date = {2006},
	langid = {english},
	note = {{OCLC}: ocm62089870},
	keywords = {Geometry, Riemannian},
	file = {Chavel - 2006 - Riemannian geometry a modern introduction.pdf:/Users/gonzalo/Zotero/storage/IVWATM2C/Chavel - 2006 - Riemannian geometry a modern introduction.pdf:application/pdf},
}

@book{tu_introduction_2011,
	location = {New York, {NY}},
	title = {An Introduction to Manifolds},
	isbn = {978-1-4419-7399-3 978-1-4419-7400-6},
	url = {https://link.springer.com/10.1007/978-1-4419-7400-6},
	series = {Universitext},
	publisher = {Springer New York},
	author = {Tu, Loring W.},
	urldate = {2023-03-02},
	date = {2011},
	langid = {english},
	doi = {10.1007/978-1-4419-7400-6},
	file = {Tu - 2011 - An Introduction to Manifolds.pdf:/Users/gonzalo/Zotero/storage/TRQJZ6B6/Tu - 2011 - An Introduction to Manifolds.pdf:application/pdf},
}

@misc{bengio_representation_2014,
	title = {Representation Learning: A Review and New Perspectives},
	url = {http://arxiv.org/abs/1206.5538},
	shorttitle = {Representation Learning},
	abstract = {The success of machine learning algorithms generally depends on data representation, and we hypothesize that this is because different representations can entangle and hide more or less the different explanatory factors of variation behind the data. Although specific domain knowledge can be used to help design representations, learning with generic priors can also be used, and the quest for {AI} is motivating the design of more powerful representation-learning algorithms implementing such priors. This paper reviews recent work in the area of unsupervised feature learning and deep learning, covering advances in probabilistic models, auto-encoders, manifold learning, and deep networks. This motivates longer-term unanswered questions about the appropriate objectives for learning good representations, for computing representations (i.e., inference), and the geometrical connections between representation learning, density estimation and manifold learning.},
	number = {{arXiv}:1206.5538},
	publisher = {{arXiv}},
	author = {Bengio, Yoshua and Courville, Aaron and Vincent, Pascal},
	urldate = {2023-03-02},
	date = {2014-04-23},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1206.5538 [cs]},
	keywords = {Computer Science - Machine Learning},
	file = {Bengio et al. - 2014 - Representation Learning A Review and New Perspect.pdf:/Users/gonzalo/Zotero/storage/I3TJUBUZ/Bengio et al. - 2014 - Representation Learning A Review and New Perspect.pdf:application/pdf},
}

@article{bengio_learning_2004,
	title = {Learning Eigenfunctions Links Spectral Embedding and Kernel {PCA}},
	volume = {16},
	issn = {0899-7667, 1530-888X},
	url = {https://direct.mit.edu/neco/article/16/10/2197-2219/6863},
	doi = {10.1162/0899766041732396},
	abstract = {In this paper, we show a direct relation between spectral embedding methods and kernel {PCA}, and how both are special cases of a more general learning problem, that of learning the principal eigenfunctions of an operator deﬁned from a kernel and the unknown data generating density.},
	pages = {2197--2219},
	number = {10},
	journaltitle = {Neural Computation},
	shortjournal = {Neural Computation},
	author = {Bengio, Yoshua and Delalleau, Olivier and Roux, Nicolas Le and Paiement, Jean-François and Vincent, Pascal and Ouimet, Marie},
	urldate = {2023-03-02},
	date = {2004-10-01},
	langid = {english},
	file = {Bengio et al. - 2004 - Learning Eigenfunctions Links Spectral Embedding a.pdf:/Users/gonzalo/Zotero/storage/G9QCDMAH/Bengio et al. - 2004 - Learning Eigenfunctions Links Spectral Embedding a.pdf:application/pdf},
}

@misc{bengio_consciousness_2019,
	title = {The Consciousness Prior},
	url = {http://arxiv.org/abs/1709.08568},
	abstract = {A new prior is proposed for learning representations of high-level concepts of the kind we manipulate with language. This prior can be combined with other priors in order to help disentangling abstract factors from each other. It is inspired by cognitive neuroscience theories of consciousness, seen as a bottleneck through which just a few elements, after having been selected by attention from a broader pool, are then broadcast and condition further processing, both in perception and decision-making. The set of recently selected elements one becomes aware of is seen as forming a low-dimensional conscious state. This conscious state is combining the few concepts constituting a conscious thought, i.e., what one is immediately conscious of at a particular moment. We claim that this architectural and information-processing constraint corresponds to assumptions about the joint distribution between high-level concepts. To the extent that these assumptions are generally true (and the form of natural language seems consistent with them), they can form a useful prior for representation learning. A low-dimensional thought or conscious state is analogous to a sentence: it involves only a few variables and yet can make a statement with very high probability of being true. This is consistent with a joint distribution (over high-level concepts) which has the form of a sparse factor graph, i.e., where the dependencies captured by each factor of the factor graph involve only very few variables while creating a strong dip in the overall energy function. Instead of making predictions in the sensory (e.g. pixel) space, one can thus make predictions in this high-level abstract space, which do not have to be limited to just the next time step but can relate events far away from each other in time. The consciousness prior also makes it natural to map conscious states to natural language utterances or to express classical {AI} knowledge in a form similar to facts and rules, albeit capturing uncertainty as well as efﬁcient search mechanisms implemented by attention mechanisms.},
	number = {{arXiv}:1709.08568},
	publisher = {{arXiv}},
	author = {Bengio, Yoshua},
	urldate = {2023-03-02},
	date = {2019-12-02},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1709.08568 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Statistics - Machine Learning},
	file = {Bengio - 2019 - The Consciousness Prior.pdf:/Users/gonzalo/Zotero/storage/K4X43RFZ/Bengio - 2019 - The Consciousness Prior.pdf:application/pdf},
}

@report{carpio_fingerprints_2019,
	title = {Fingerprints of cancer by persistent homology},
	url = {http://biorxiv.org/lookup/doi/10.1101/777169},
	abstract = {We have carried out a topological data analysis of gene expressions for diﬀerent databases based on the Fermat distance between the z scores of diﬀerent tissue samples. There is a critical value of the ﬁltration parameter at which all clusters collapse in a single one. This critical value for healthy samples is gapless and smaller than that for cancerous ones. After collapse in a single cluster, topological holes persist for larger ﬁltration parameter values in cancerous samples. Barcodes, persistence diagrams and Betti numbers as functions of the ﬁltration parameter are diﬀerent for diﬀerent types of cancer and constitute ﬁngerprints thereof.},
	institution = {Cancer Biology},
	type = {preprint},
	author = {Carpio, A. and Bonilla, L. L. and Mathews, J. C. and Tannenbaum, A. R.},
	urldate = {2023-03-02},
	date = {2019-09-23},
	langid = {english},
	doi = {10.1101/777169},
	file = {Carpio et al. - 2019 - Fingerprints of cancer by persistent homology.pdf:/Users/gonzalo/Zotero/storage/8ZN8N9N9/Carpio et al. - 2019 - Fingerprints of cancer by persistent homology.pdf:application/pdf},
}

@article{cayton_algorithms_2005,
	title = {Algorithms for manifold learning},
	abstract = {Manifold learning is a popular recent approach to nonlinear dimensionality reduction. Algorithms for this task are based on the idea that the dimensionality of many data sets is only artiﬁcially high; though each data point consists of perhaps thousands of features, it may be described as a function of only a few underlying parameters. That is, the data points are actually samples from a low-dimensional manifold that is embedded in a high-dimensional space. Manifold learning algorithms attempt to uncover these parameters in order to ﬁnd a low-dimensional representation of the data. In this paper, we discuss the motivation, background, and algorithms proposed for manifold learning. Isomap, Locally Linear Embedding, Laplacian Eigenmaps, Semideﬁnite Embedding, and a host of variants of these algorithms are examined.},
	author = {Cayton, Lawrence},
	date = {2005},
	langid = {english},
	file = {Cayton - Algorithms for manifold learning.pdf:/Users/gonzalo/Zotero/storage/Q3RCYSTM/Cayton - Algorithms for manifold learning.pdf:application/pdf},
}

@misc{chen_comprehensive_2015,
	title = {A Comprehensive Approach to Mode Clustering},
	url = {http://arxiv.org/abs/1406.1780},
	abstract = {Mode clustering is a nonparametric method for clustering that deﬁnes clusters using the basins of attraction of a density estimator’s modes. We provide several enhancements to mode clustering: (i) a soft variant of cluster assignment, (ii) a measure of connectivity between clusters, (iii) a technique for choosing the bandwidth, (iv) a method for denoising small clusters, and (v) an approach to visualizing the clusters. Combining all these enhancements gives us a complete procedure for clustering in multivariate problems. We also compare mode clustering to other clustering methods in several examples.},
	number = {{arXiv}:1406.1780},
	publisher = {{arXiv}},
	author = {Chen, Yen-Chi and Genovese, Christopher R. and Wasserman, Larry},
	urldate = {2023-03-02},
	date = {2015-12-22},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1406.1780 [stat]},
	keywords = {Statistics - Machine Learning, 62H30 (Primary), 62G07, 62G99 (Secondary), Statistics - Methodology},
	file = {Chen et al. - 2015 - A Comprehensive Approach to Mode Clustering.pdf:/Users/gonzalo/Zotero/storage/2M3P8733/Chen et al. - 2015 - A Comprehensive Approach to Mode Clustering.pdf:application/pdf},
}

@misc{chu_exact_2020,
	title = {Exact Computation of a Manifold Metric, via Lipschitz Embeddings and Shortest Paths on a Graph},
	url = {http://arxiv.org/abs/1709.07797},
	abstract = {We consider a simple graph-based metric on points in Euclidean space known as the edge-squared metric. This metric is deﬁned by squaring the Euclidean distance between points, and taking the shortest paths on the resulting graph. This metric has been studied before in wireless networks and machine learning, and has the densitysensitive property: distances between two points in the same cluster are short, even if their Euclidean distance is long. This property is desirable in machine learning.},
	number = {{arXiv}:1709.07797},
	publisher = {{arXiv}},
	author = {Chu, Timothy and Miller, Gary and Sheehy, Donald},
	urldate = {2023-03-02},
	date = {2020-04-21},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1709.07797 [cs, math]},
	keywords = {Computer Science - Computational Geometry, Computer Science - Data Structures and Algorithms, Mathematics - Functional Analysis},
	file = {Chu et al. - 2020 - Exact Computation of a Manifold Metric, via Lipsch.pdf:/Users/gonzalo/Zotero/storage/BFL9S4E8/Chu et al. - 2020 - Exact Computation of a Manifold Metric, via Lipsch.pdf:application/pdf},
}

@misc{groisman_nonhomogeneous_2019,
	title = {Nonhomogeneous Euclidean first-passage percolation and distance learning},
	url = {http://arxiv.org/abs/1810.09398},
	abstract = {Consider an i.i.d. sample from an unknown density function supported on an unknown manifold embedded in a high dimensional Euclidean space. We tackle the problem of learning a distance between points, able to capture both the geometry of the manifold and the underlying density. We deﬁne such a sample distance and prove the convergence, as the sample size goes to inﬁnity, to a macroscopic one that we call Fermat distance as it minimizes a path functional, resembling Fermat principle in optics. The proof boils down to the study of geodesics in Euclidean ﬁrst-passage percolation for nonhomogeneous Poisson point processes.},
	number = {{arXiv}:1810.09398},
	publisher = {{arXiv}},
	author = {Groisman, Pablo and Jonckheere, Matthieu and Sapienza, Facundo},
	urldate = {2023-03-02},
	date = {2019-12-26},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1810.09398 [math]},
	keywords = {G.3, Mathematics - Probability},
	file = {Groisman et al. - 2019 - Nonhomogeneous Euclidean first-passage percolation.pdf:/Users/gonzalo/Zotero/storage/4AEYHE2D/Groisman et al. - 2019 - Nonhomogeneous Euclidean first-passage percolation.pdf:application/pdf},
}

@article{huber_projection_1985,
	title = {Projection Pursuit},
	volume = {13},
	url = {http://www.jstor.org/stable/2241175},
	pages = {435--475},
	number = {2},
	journaltitle = {The Annals of Statistics},
	author = {Huber, Peter J.},
	date = {1985},
	langid = {english},
	file = {Huber - 1985 - Projection Pursuit.pdf:/Users/gonzalo/Zotero/storage/8FGJUMHJ/Huber - 1985 - Projection Pursuit.pdf:application/pdf},
}

@article{jenq-neng_hwang_nonparametric_1994,
	title = {Nonparametric multivariate density estimation: a comparative study},
	volume = {42},
	issn = {1053587X},
	url = {http://ieeexplore.ieee.org/document/324744/},
	doi = {10.1109/78.324744},
	shorttitle = {Nonparametric multivariate density estimation},
	abstract = {This paper algorithmically and empirically studies two major types of nonparametricmultivariatedensity estimation techniques, where no assumption is made about the data being drawn from any of known parametric families of distribution. The first type is the popular kernel method (and several of its variants) which uses locally tuned radial basis (e.g., Gaussian) functions to interpolate the multidimensional density; the second type is based on an exploratory projection pursuit technique which interprets the multidimensionaldensity through the construction of several 1-D densities along highly “interesting” projections of multidimensional data. Performance evaluations using training data from mixture Gaussian and mixture Cauchy densities are presented. The results show that the curse of dimensionality and the sensitivity of control parameters have a much more adverse impact on the kernel density estimators than on the projection pursuit density estimators.},
	pages = {2795--2810},
	number = {10},
	journaltitle = {{IEEE} Transactions on Signal Processing},
	shortjournal = {{IEEE} Trans. Signal Process.},
	author = {{Jenq-Neng Hwang} and {Shyh-Rong Lay} and Lippman, A.},
	urldate = {2023-03-02},
	date = {1994-10},
	langid = {english},
	file = {Jenq-Neng Hwang et al. - 1994 - Nonparametric multivariate density estimation a c.pdf:/Users/gonzalo/Zotero/storage/8MBX5UI9/Jenq-Neng Hwang et al. - 1994 - Nonparametric multivariate density estimation a c.pdf:application/pdf},
}

@article{loubes_kernel-based_2008,
	title = {A kernel-based classifier on a Riemannian manifold},
	volume = {26},
	issn = {0721-2631},
	url = {https://www.degruyter.com/document/doi/10.1524/stnd.2008.0911/html},
	doi = {10.1524/stnd.2008.0911},
	abstract = {Let X be a random variable taking values in a compact Riemannian manifold without boundary, and let Y be a discrete random variable valued in \{0; 1\} which represents a classiﬁcation label. We introduce a kernel rule for classiﬁcation on the manifold based on n independent copies of (X, Y ). Under mild assumptions on the bandwidth sequence, it is shown that this kernel rule is consistent in the sense that its probability of error converges to the Bayes risk with probability one.},
	pages = {35--51},
	number = {1},
	journaltitle = {Statistics \& Decisions},
	author = {Loubes, Jean-Michel and Pelletier, Bruno},
	urldate = {2023-03-02},
	date = {2008-03},
	langid = {english},
	file = {Loubes and Pelletier - 2008 - A kernel-based classifier on a Riemannian manifold.pdf:/Users/gonzalo/Zotero/storage/QLSVS3PR/Loubes and Pelletier - 2008 - A kernel-based classifier on a Riemannian manifold.pdf:application/pdf},
}

@misc{little_balancing_2021,
	title = {Balancing Geometry and Density: Path Distances on High-Dimensional Data},
	url = {http://arxiv.org/abs/2012.09385},
	shorttitle = {Balancing Geometry and Density},
	abstract = {New geometric and computational analyses of power-weighted shortest-path distances ({PWSPDs}) are presented. By illuminating the way these metrics balance geometry and density in the underlying data, we clarify their key parameters and illustrate how they provide multiple perspectives for data analysis. Comparisons are made with related data-driven metrics, which illustrate the broader role of density in kernel-based unsupervised and semi-supervised machine learning. Computationally, we relate {PWSPDs} on complete weighted graphs to their analogues on weighted nearest neighbor graphs, providing high probability guarantees on their equivalence that are near-optimal. Connections with percolation theory are developed to establish estimates on the bias and variance of {PWSPDs} in the ﬁnite sample setting. The theoretical results are bolstered by illustrative experiments, demonstrating the versatility of {PWSPDs} for a wide range of data settings. Throughout the paper, our results generally require only that the underlying data is sampled from a compact low-dimensional manifold, and depend most crucially on the intrinsic dimension of this manifold, rather than its ambient dimension.},
	number = {{arXiv}:2012.09385},
	publisher = {{arXiv}},
	author = {Little, Anna and {McKenzie}, Daniel and Murphy, James},
	urldate = {2023-03-02},
	date = {2021-06-07},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2012.09385 [cs, stat]},
	keywords = {I.5.3, Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Data Structures and Algorithms, 05C85, 05C80},
	file = {Little et al. - 2021 - Balancing Geometry and Density Path Distances on .pdf:/Users/gonzalo/Zotero/storage/6XMA593D/Little et al. - 2021 - Balancing Geometry and Density Path Distances on .pdf:application/pdf},
}

@misc{mckenzie_power_2019,
	title = {Power Weighted Shortest Paths for Clustering Euclidean Data},
	url = {http://arxiv.org/abs/1905.13345},
	abstract = {We study the use of power weighted shortest path metrics for clustering high dimensional Euclidean data, under the assumption that the data is drawn from a collection of disjoint low dimensional manifolds. We argue, theoretically and experimentally, that this leads to higher clustering accuracy. We also present a fast algorithm for computing these distances.},
	number = {{arXiv}:1905.13345},
	publisher = {{arXiv}},
	author = {Mckenzie, Daniel and Damelin, Steven},
	urldate = {2023-03-02},
	date = {2019-09-03},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1905.13345 [cs, stat]},
	keywords = {I.5.3, Computer Science - Machine Learning, Statistics - Machine Learning, 05C85, 05C80},
	file = {Mckenzie and Damelin - 2019 - Power Weighted Shortest Paths for Clustering Eucli.pdf:/Users/gonzalo/Zotero/storage/JY9XJQQX/Mckenzie and Damelin - 2019 - Power Weighted Shortest Paths for Clustering Eucli.pdf:application/pdf},
}

@thesis{munoz_estimacion_2011,
	title = {Estimación no paramétrica de la densidad en variedades Riemannianas},
	institution = {Universidad de Buenos Aires},
	type = {phdthesis},
	author = {Muñoz, Andres Leandro},
	date = {2011},
	langid = {spanish},
	file = {Munoz - Estimacio´n no param´etrica de la densidad en vari.pdf:/Users/gonzalo/Zotero/storage/8NLUQYY3/Munoz - Estimacio´n no param´etrica de la densidad en vari.pdf:application/pdf},
}

@inproceedings{sapienza_weighted_2018,
	title = {Weighted Geodesic Distance Following Fermat's Principle},
	url = {https://openreview.net/forum?id=BJfaMIJwG},
	abstract = {We propose a density-based estimator for weighted geodesic distances suitable for data lying on a manifold of lower dimension than ambient space and sampled from a possibly nonuniform distribution. After discussing its properties and implementation, we evaluate its performance as a tool for clustering tasks. A discussion on the consistency of the estimator is also given.},
	author = {Sapienza, Facundo and Jonckheere, Matthieu and Groisman, Pablo},
	date = {2018},
	langid = {english},
	file = {Sapienza et al. - 2018 - WEIGHTED GEODESIC DISTANCE FOLLOWING FERMAT’S PRIN.pdf:/Users/gonzalo/Zotero/storage/983EUZCU/Sapienza et al. - 2018 - WEIGHTED GEODESIC DISTANCE FOLLOWING FERMAT’S PRIN.pdf:application/pdf},
}

@article{silverman_using_1981,
	title = {Using Kernel Density Estimates to Investigate Multimodality},
	volume = {43},
	issn = {00359246},
	url = {https://onlinelibrary.wiley.com/doi/10.1111/j.2517-6161.1981.tb01155.x},
	doi = {10.1111/j.2517-6161.1981.tb01155.x},
	abstract = {A technique for using kernel density estimates to investigate the number of modes in a population is described and discussed. The amount of smoothing is chosen automatically in a natural way.},
	pages = {97--99},
	number = {1},
	journaltitle = {Journal of the Royal Statistical Society: Series B (Methodological)},
	shortjournal = {Journal of the Royal Statistical Society: Series B (Methodological)},
	author = {Silverman, B. W.},
	urldate = {2023-03-02},
	date = {1981-09},
	langid = {english},
	file = {Silverman - 1981 - Using Kernel Density Estimates to Investigate Mult.pdf:/Users/gonzalo/Zotero/storage/GY2IAI9M/Silverman - 1981 - Using Kernel Density Estimates to Investigate Mult.pdf:application/pdf},
}

@unpublished{tang_tutorial_2009,
	title = {Tutorial on Tangent Propagation},
	author = {Tang, Yichuan},
	date = {2009-02-05},
	langid = {english},
	file = {Tang - Tutorial on Tangent Propagation.pdf:/Users/gonzalo/Zotero/storage/DMFPWIXT/Tang - Tutorial on Tangent Propagation.pdf:application/pdf},
}

@article{taylor_classification_1997,
	title = {Classification and kernel density estimation},
	volume = {41},
	issn = {00836656},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0083665697000469},
	doi = {10.1016/S0083-6656(97)00046-9},
	abstract = {The method of kernel density estimation can be readily used for the purposes of classification, and an easy-to-use package ({ALLOCBO}) is now in wide circulation. It is known that this method performs well (at least in relative terms) in the case of bimodal, or heavily skewed distributions.},
	pages = {411--417},
	number = {3},
	journaltitle = {Vistas in Astronomy},
	shortjournal = {Vistas in Astronomy},
	author = {Taylor, Charles},
	urldate = {2023-03-02},
	date = {1997-01},
	langid = {english},
	file = {Taylor - 1997 - Classification and kernel density estimation.pdf:/Users/gonzalo/Zotero/storage/AKKJZT3R/Taylor - 1997 - Classification and kernel density estimation.pdf:application/pdf},
}

@article{tenenbaum_global_2000,
	title = {A Global Geometric Framework for Nonlinear Dimensionality Reduction},
	volume = {290},
	issn = {0036-8075, 1095-9203},
	url = {https://www.science.org/doi/10.1126/science.290.5500.2319},
	doi = {10.1126/science.290.5500.2319},
	abstract = {Scientists working with large volumes of high-dimensional data, such as global climate patterns, stellar spectra, or human gene distributions, regularly confront the problem of dimensionality reduction: finding meaningful low-dimensional structures hidden in their high-dimensional observations. The human brain confronts the same problem in everyday perception, extracting from its high-dimensional sensory inputs—30,000 auditory nerve fibers or 10
              6
              optic nerve fibers—a manageably small number of perceptually relevant features. Here we describe an approach to solving dimensionality reduction problems that uses easily measured local metric information to learn the underlying global geometry of a data set. Unlike classical techniques such as principal component analysis ({PCA}) and multidimensional scaling ({MDS}), our approach is capable of discovering the nonlinear degrees of freedom that underlie complex natural observations, such as human handwriting or images of a face under different viewing conditions. In contrast to previous algorithms for nonlinear dimensionality reduction, ours efficiently computes a globally optimal solution, and, for an important class of data manifolds, is guaranteed to converge asymptotically to the true structure.},
	pages = {2319--2323},
	number = {5500},
	journaltitle = {Science},
	shortjournal = {Science},
	author = {Tenenbaum, Joshua B. and Silva, Vin de and Langford, John C.},
	urldate = {2023-03-02},
	date = {2000-12-22},
	langid = {english},
	file = {Tenenbaum et al. - 2000 - A Global Geometric Framework for Nonlinear Dimensi.pdf:/Users/gonzalo/Zotero/storage/T82F3Z69/Tenenbaum et al. - 2000 - A Global Geometric Framework for Nonlinear Dimensi.pdf:application/pdf},
}

@article{wand_comparison_1993,
	title = {Comparison of Smoothing Parameterizations in Bivariate Kernel Density Estimation},
	volume = {88},
	issn = {0162-1459, 1537-274X},
	url = {http://www.tandfonline.com/doi/abs/10.1080/01621459.1993.10476303},
	doi = {10.1080/01621459.1993.10476303},
	pages = {520--528},
	number = {422},
	journaltitle = {Journal of the American Statistical Association},
	shortjournal = {Journal of the American Statistical Association},
	author = {Wand, M. P. and Jones, M. C.},
	urldate = {2023-03-02},
	date = {1993-06},
	langid = {english},
	file = {Wand and Jones - 1993 - Comparison of Smoothing Parameterizations in Bivar.pdf:/Users/gonzalo/Zotero/storage/WHDX44I9/Wand and Jones - 1993 - Comparison of Smoothing Parameterizations in Bivar.pdf:application/pdf},
}

@article{wang_nonparametric_2019,
	title = {Nonparametric Density Estimation for High-Dimensional Data - Algorithms and Applications},
	volume = {11},
	issn = {1939-5108, 1939-0068},
	url = {http://arxiv.org/abs/1904.00176},
	doi = {10.1002/wics.1461},
	abstract = {Density Estimation is one of the central areas of statistics whose purpose is to estimate the probability density function underlying the observed data. It serves as a building block for many tasks in statistical inference, visualization, and machine learning. Density Estimation is widely adopted in the domain of unsupervised learning especially for the application of clustering. As big data become pervasive in almost every area of data sciences, analyzing high-dimensional data that have many features and variables appears to be a major focus in both academia and industry. Highdimensional data pose challenges not only from the theoretical aspects of statistical inference, but also from the algorithmic/computational considerations of machine learning and data analytics. This paper reviews a collection of selected nonparametric density estimation algorithms for high-dimensional data, some of them are recently published and provide interesting mathematical insights. The important application domain of nonparametric density estimation, such as modal clustering, are also included in this paper. Several research directions related to density estimation and high-dimensional data analysis are suggested by the authors.},
	number = {4},
	journaltitle = {{WIREs} Computational Statistics},
	shortjournal = {{WIREs} Comp Stat},
	author = {Wang, Zhipeng and Scott, David W.},
	urldate = {2023-03-02},
	date = {2019-07},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1904.00176 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Statistics - Computation},
	file = {Wang and Scott - 2019 - Nonparametric Density Estimation for High-Dimensio.pdf:/Users/gonzalo/Zotero/storage/5C9PF8NB/Wang and Scott - 2019 - Nonparametric Density Estimation for High-Dimensio.pdf:application/pdf},
}

@article{chacon_data-driven_2013,
	title = {Data-driven density derivative estimation, with applications to nonparametric clustering and bump hunting},
	volume = {7},
	issn = {1935-7524},
	url = {https://projecteuclid.org/journals/electronic-journal-of-statistics/volume-7/issue-none/Data-driven-density-derivative-estimation-with-applications-to-nonparametric-clustering/10.1214/13-EJS781.full},
	doi = {10.1214/13-EJS781},
	abstract = {Important information concerning a multivariate data set, such as clusters and modal regions, is contained in the derivatives of the probability density function. Despite this importance, nonparametric estimation of higher order derivatives of the density functions have received only relatively scant attention. Kernel estimators of density functions are widely used as they exhibit excellent theoretical and practical properties, though their generalization to density derivatives has progressed more slowly due to the mathematical intractabilities encountered in the crucial problem of bandwidth (or smoothing parameter) selection. This paper presents the ﬁrst fully automatic, data-based bandwidth selectors for multivariate kernel density derivative estimators. This is achieved by synthesizing recent advances in matrix analytic theory which allow mathematically and computationally tractable representations of higher order derivatives of multivariate vector valued functions. The theoretical asymptotic properties as well as the ﬁnite sample behaviour of the proposed selectors are studied. In addition, we explore in detail the applications of the new data-driven methods for two other statistical problems: clustering and bump hunting. The introduced techniques are combined with the mean shift algorithm to develop novel automatic, nonparametric clustering procedures which are shown to outperform mixture-model cluster analysis and other recent nonparametric approaches in practice. Furthermore, the advantage of the use of smoothing parameters designed for density derivative estimation for feature significance analysis for bump hunting is illustrated with a real data example.},
	issue = {none},
	journaltitle = {Electronic Journal of Statistics},
	shortjournal = {Electron. J. Statist.},
	author = {Chacón, José E. and Duong, Tarn},
	urldate = {2023-03-02},
	date = {2013-01-01},
	langid = {english},
	file = {Chacón and Duong - 2013 - Data-driven density derivative estimation, with ap.pdf:/Users/gonzalo/Zotero/storage/52VG4PRB/Chacón and Duong - 2013 - Data-driven density derivative estimation, with ap.pdf:application/pdf},
}

@article{duong_cross-validation_2005,
	title = {Cross-validation Bandwidth Matrices for Multivariate Kernel Density Estimation},
	volume = {32},
	issn = {0303-6898, 1467-9469},
	url = {https://onlinelibrary.wiley.com/doi/10.1111/j.1467-9469.2005.00445.x},
	doi = {10.1111/j.1467-9469.2005.00445.x},
	abstract = {The performance of multivariate kernel density estimates depends crucially on the choice of bandwidth matrix, but progress towards developing good bandwidth matrix selectors has been relatively slow. In particular, previous studies of cross-validation ({CV}) methods have been restricted to biased and unbiased {CV} selection of diagonal bandwidth matrices. However, for certain types of target density the use of full (i.e. unconstrained) bandwidth matrices offers the potential for signiﬁcantly improved density estimation. In this paper, we generalize earlier work from diagonal to full bandwidth matrices, and develop a smooth cross-validation ({SCV}) methodology for multivariate data. We consider optimization of the {SCV} technique with respect to a pilot bandwidth matrix. All the {CV} methods are studied using asymptotic analysis, simulation experiments and real data analysis. The results suggest that {SCV} for full bandwidth matrices is the most reliable of the {CV} methods. We also observe that experience from the univariate setting can sometimes be a misleading guide for understanding bandwidth selection in the multivariate case.},
	pages = {485--506},
	number = {3},
	journaltitle = {Scandinavian Journal of Statistics},
	shortjournal = {Scand J Stat},
	author = {Duong, Tarn and Hazelton, Martin L.},
	urldate = {2023-03-02},
	date = {2005-09},
	langid = {english},
	file = {Duong and Hazelton - 2005 - Cross-validation Bandwidth Matrices for Multivaria.pdf:/Users/gonzalo/Zotero/storage/SNZ4M2YX/Duong and Hazelton - 2005 - Cross-validation Bandwidth Matrices for Multivaria.pdf:application/pdf},
}

@article{hall_bandwidth_2005,
	title = {Bandwidth choice for nonparametric classification},
	volume = {33},
	issn = {0090-5364},
	url = {https://projecteuclid.org/journals/annals-of-statistics/volume-33/issue-1/Bandwidth-choice-for-nonparametric-classification/10.1214/009053604000000959.full},
	doi = {10.1214/009053604000000959},
	number = {1},
	journaltitle = {The Annals of Statistics},
	shortjournal = {Ann. Statist.},
	author = {Hall, Peter and Kang, Kee-Hoon},
	urldate = {2023-03-02},
	date = {2005-02-01},
	langid = {english},
	file = {Hall and Kang - 2005 - Bandwidth choice for nonparametric classification.pdf:/Users/gonzalo/Zotero/storage/RI6RY8ZG/Hall and Kang - 2005 - Bandwidth choice for nonparametric classification.pdf:application/pdf},
}

@article{henry_kernel_2009,
	title = {Kernel Density Estimation on Riemannian Manifolds: Asymptotic Results},
	volume = {34},
	issn = {0924-9907, 1573-7683},
	url = {http://link.springer.com/10.1007/s10851-009-0145-2},
	doi = {10.1007/s10851-009-0145-2},
	shorttitle = {Kernel Density Estimation on Riemannian Manifolds},
	abstract = {The paper concerns the strong uniform consistency and the asymptotic distribution of the kernel density estimator of random objects on a Riemannian manifolds, proposed by Pelletier (Stat. Probab. Lett., 73(3):297–304, 2005). The estimator is illustrated via one example based on a real data.},
	pages = {235--239},
	number = {3},
	journaltitle = {Journal of Mathematical Imaging and Vision},
	shortjournal = {J Math Imaging Vis},
	author = {Henry, Guillermo and Rodriguez, Daniela},
	urldate = {2023-03-02},
	date = {2009-07},
	langid = {english},
	file = {Henry and Rodriguez - 2009 - Kernel Density Estimation on Riemannian Manifolds.pdf:/Users/gonzalo/Zotero/storage/CUVPHEPW/Henry and Rodriguez - 2009 - Kernel Density Estimation on Riemannian Manifolds.pdf:application/pdf},
}

@article{lin_shell_2021,
	title = {Shell Theory: A Statistical Model of Reality},
	issn = {0162-8828, 2160-9292, 1939-3539},
	url = {https://ieeexplore.ieee.org/document/9444188/},
	doi = {10.1109/TPAMI.2021.3084598},
	shorttitle = {Shell Theory},
	abstract = {The foundational assumption of machine learning is that the data under consideration is separable into classes; while intuitively reasonable, separability constraints have proven remarkably difﬁcult to formulate mathematically. We believe this problem is rooted in the mismatch between existing statistical techniques and commonly encountered data; object representations are typically high dimensional but statistical techniques tend to treat high dimensions a degenerate case. To address this problem, we develop a dedicated statistical framework for machine learning in high dimensions. The framework derives from the observation that object relations form a natural hierarchy; this leads us to model objects as instances of a high dimensional, hierarchal generative processes. Using a distance based statistical technique, also developed in this paper, we show that in such generative processes, instances of each process in the hierarchy, are almost-always encapsulated by a distinctive-shell that excludes almost-all other instances. The result is shell theory, a statistical machine learning framework in which separability constraints (distinctive-shells) are formally derived from the assumed generative process.},
	pages = {1--1},
	journaltitle = {{IEEE} Transactions on Pattern Analysis and Machine Intelligence},
	shortjournal = {{IEEE} Trans. Pattern Anal. Mach. Intell.},
	author = {Lin, Wen-Yan and Liu, Siying and Ren, Changhao and Cheung, Ngai-Man and Li, Hongdong and Matsushita, Yasuyuki},
	urldate = {2023-03-02},
	date = {2021},
	langid = {english},
	file = {Lin et al. - 2021 - Shell Theory A Statistical Model of Reality.pdf:/Users/gonzalo/Zotero/storage/3L999SHM/Lin et al. - 2021 - Shell Theory A Statistical Model of Reality.pdf:application/pdf},
}

@article{pelletier_kernel_2005,
	title = {Kernel density estimation on Riemannian manifolds},
	volume = {73},
	issn = {01677152},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0167715205001239},
	doi = {10.1016/j.spl.2005.04.004},
	abstract = {The estimation of the underlying probability density of n i.i.d. random objects on a compact Riemannian manifold without boundary is considered. The proposed methodology adapts the technique of kernel density estimation on Euclidean sample spaces to this {nonEuclidean} setting. Under sufﬁcient regularity assumptions on the underlying density, L2 convergence rates are obtained.},
	pages = {297--304},
	number = {3},
	journaltitle = {Statistics \& Probability Letters},
	shortjournal = {Statistics \& Probability Letters},
	author = {Pelletier, Bruno},
	urldate = {2023-03-02},
	date = {2005-07},
	langid = {english},
	file = {Pelletier - 2005 - Kernel density estimation on Riemannian manifolds.pdf:/Users/gonzalo/Zotero/storage/DEZE5FJQ/Pelletier - 2005 - Kernel density estimation on Riemannian manifolds.pdf:application/pdf},
}

@article{rosenblatt_remarks_1956,
	title = {Remarks on Some Nonparametric Estimates of a Density Function},
	volume = {27},
	issn = {0003-4851},
	url = {https://www.jstor.org/stable/2237390},
	abstract = {This note discusses some aspects of the estimation of the density function of a univariate probability distribution. All estimates of the density function satisfying relatively mild conditions are shown to be biased. The asymptotic mean square error of a particular class of estimates is evaluated.},
	pages = {832--837},
	number = {3},
	journaltitle = {The Annals of Mathematical Statistics},
	author = {Rosenblatt, Murray},
	urldate = {2023-03-02},
	date = {1956},
	note = {Publisher: Institute of Mathematical Statistics},
	file = {Rosenblatt - 1956 - Remarks on Some Nonparametric Estimates of a Density Function.pdf:/Users/gonzalo/Zotero/storage/NGJQQ8SZ/Rosenblatt - 1956 - Remarks on Some Nonparametric Estimates of a Density Function.pdf:application/pdf},
}

@inproceedings{beyer_when_1999,
	location = {Berlin, Heidelberg},
	title = {When is "Nearest Neighbor" Meaningful?},
	isbn = {978-3-540-49257-3},
	doi = {10.1007/3-540-49257-7_15},
	series = {Lecture Notes in Computer Science},
	abstract = {We explore the effect of dimensionality on the “nearest neighbor” problem. We show that under a broad set of conditions (much broader than independent and identically distributed dimensions), as dimensionality increases, the distance to the nearest data point approaches the distance to the farthest data point. To provide a practical perspective, we present empirical results on both real and synthetic data sets that demonstrate that this effect can occur for as few as 10–15 dimensions.},
	pages = {217--235},
	booktitle = {Database Theory - {ICDT}'99},
	publisher = {Springer},
	author = {Beyer, Kevin and Goldstein, Jonathan and Ramakrishnan, Raghu and Shaft, Uri},
	editor = {Beeri, Catriel and Buneman, Peter},
	date = {1999},
	langid = {english},
	file = {Submitted Version:/Users/gonzalo/Zotero/storage/VCFTVUSQ/Beyer et al. - 1999 - When Is “Nearest Neighbor” Meaningful.pdf:application/pdf},
}

@article{wand_multivariate_1994,
	title = {Multivariate plug-in bandwidth selection},
	volume = {9},
	pages = {97--116},
	number = {2},
	journaltitle = {Computational Statistics},
	author = {Wand, Matt P. and Jones, M. Chris},
	date = {1994},
	note = {Publisher: Heidelberg: Physica-Verlag,[1992-},
	file = {Full Text:/Users/gonzalo/Zotero/storage/EHBPWPM5/Wand and Jones - 1994 - Multivariate plug-in bandwidth selection.pdf:application/pdf},
}

@article{parzen_estimation_1962,
	title = {On estimation of a probability density function and mode},
	volume = {33},
	pages = {1065--1076},
	number = {3},
	journaltitle = {The annals of mathematical statistics},
	author = {Parzen, Emanuel},
	date = {1962},
	note = {Publisher: {JSTOR}},
	file = {Full Text:/Users/gonzalo/Zotero/storage/YCCG8PGX/Parzen - 1962 - On estimation of a probability density function an.pdf:application/pdf;Snapshot:/Users/gonzalo/Zotero/storage/35R8VYH2/2237880.html:text/html},
}

@unpublished{davenport_bayes_2014,
	title = {Bayes rule for random variables},
	author = {Davenport, M. and Romberg, J. and Rozell, J},
	date = {2014},
	note = {{ECE} 3077 Notes by M. Davenport, J. Romberg and C. Rozell. Last updated 21:27, June 25, 2014},
	file = {16_BayesRVs-su14.pdf:/Users/gonzalo/Zotero/storage/83NTKU79/16_BayesRVs-su14.pdf:application/pdf},
}

@inproceedings{vincent_manifold_2002,
	title = {Manifold Parzen Windows},
	volume = {15},
	url = {https://proceedings.neurips.cc/paper/2002/hash/2d969e2cee8cfa07ce7ca0bb13c7a36d-Abstract.html},
	abstract = {The similarity between objects is a fundamental element of many learn- ing algorithms. Most non-parametric methods take this similarity to be ﬁxed, but much recent work has shown the advantages of learning it, in particular to exploit the local invariances in the data or to capture the possibly non-linear manifold on which most of the data lies. We propose a new non-parametric kernel density estimation method which captures the local structure of an underlying manifold through the leading eigen- vectors of regularized local covariance matrices. Experiments in density estimation show signiﬁcant improvements with respect to Parzen density estimators. The density estimators can also be used within Bayes classi- ﬁers, yielding classiﬁcation rates similar to {SVMs} and much superior to the Parzen classiﬁer.},
	booktitle = {Advances in Neural Information Processing Systems},
	publisher = {{MIT} Press},
	author = {Vincent, Pascal and Bengio, Yoshua},
	urldate = {2023-03-02},
	date = {2002},
	file = {Full Text PDF:/Users/gonzalo/Zotero/storage/BAKIAKHX/Vincent and Bengio - 2002 - Manifold Parzen Windows.pdf:application/pdf},
}

@inproceedings{bengio_non-local_2005,
	title = {Non-Local Manifold Parzen Windows},
	volume = {18},
	url = {https://proceedings.neurips.cc/paper/2005/hash/17eb7ecc4c38e4705361cccd903ad8c6-Abstract.html},
	abstract = {To escape from the curse of dimensionality, we claim that one can learn non-local functions, in the sense that the value and shape of the learned function at x must be inferred using examples that may be far from x. With this objective, we present a non-local non-parametric density esti- mator. It builds upon previously proposed Gaussian mixture models with regularized covariance matrices to take into account the local shape of the manifold. It also builds upon recent work on non-local estimators of the tangent plane of a manifold, which are able to generalize in places with little training data, unlike traditional, local, non-parametric models.},
	booktitle = {Advances in Neural Information Processing Systems},
	publisher = {{MIT} Press},
	author = {Bengio, Yoshua and Larochelle, Hugo and Vincent, Pascal},
	urldate = {2023-03-02},
	date = {2005},
	file = {Full Text PDF:/Users/gonzalo/Zotero/storage/8KTRUNGT/Bengio et al. - 2005 - Non-Local Manifold Parzen Windows.pdf:application/pdf},
}

@inproceedings{tenenbaum_mapping_1997,
	title = {Mapping a Manifold of Perceptual Observations},
	volume = {10},
	url = {https://proceedings.neurips.cc/paper/1997/hash/28e209b61a52482a0ae1cb9f5959c792-Abstract.html},
	abstract = {Nonlinear dimensionality reduction is formulated here as the problem of trying to  find a Euclidean feature-space embedding of a set of observations that preserves  as closely as possible their intrinsic metric structure - the distances between points  on  the  observation manifold as  measured along geodesic paths.  Our isometric  feature mapping procedure, or isomap, is able to reliably recover low-dimensional  nonlinear structure in  realistic  perceptual data  sets,  such as  a manifold  of face  images,  where  conventional global  mapping  methods  find  only  local  minima.  The  recovered  map  provides  a canonical  set  of globally  meaningful  features,  which allows perceptual transformations such as interpolation, extrapolation, and  analogy - highly nonlinear transformations in the original observation space - to  be computed with simple linear operations in feature space.},
	booktitle = {Advances in Neural Information Processing Systems},
	publisher = {{MIT} Press},
	author = {Tenenbaum, Joshua},
	urldate = {2023-03-02},
	date = {1997},
	file = {Full Text PDF:/Users/gonzalo/Zotero/storage/ZMNFLLEB/Tenenbaum - 1997 - Mapping a Manifold of Perceptual Observations.pdf:application/pdf},
}

@article{calder_hamilton-jacobi_2022,
	title = {Hamilton-Jacobi equations on graphs with applications to semi-supervised learning and data depth},
	volume = {23},
	abstract = {Shortest path graph distances are widely used in data science and machine learning, since they can approximate the underlying geodesic distance on the data manifold. However, the shortest path distance is highly sensitive to the addition of corrupted edges in the graph, either through noise or an adversarial perturbation. In this paper we study a family of Hamilton-Jacobi equations on graphs that we call the p-eikonal equation. We show that the p-eikonal equation with p = 1 is a provably robust distance-type function on a graph, and the p → ∞ limit recovers shortest path distances. While the p-eikonal equation does not correspond to a shortest-path graph distance, we nonetheless show that the continuum limit of the p-eikonal equation on a random geometric graph recovers a geodesic density weighted distance in the continuum. We consider applications of the p-eikonal equation to data depth and semi-supervised learning, and use the continuum limit to prove asymptotic consistency results for both applications. Finally, we show the results of experiments with data depth and semi-supervised learning on real image datasets, including {MNIST}, {FashionMNIST} and {CIFAR}-10, which show that the p-eikonal equation oﬀers signiﬁcantly better results compared to shortest path distances.},
	pages = {1--62},
	number = {318},
	journaltitle = {Journal of Machine Learning Research},
	author = {Calder, Jeff and Ettehad, Mahmood},
	date = {2022},
	file = {Full Text:/Users/gonzalo/Zotero/storage/8C8NGYJL/Calder and Ettehad - 2022 - Hamilton-Jacobi equations on graphs with applicati.pdf:application/pdf},
}

@inproceedings{brand_charting_2002,
	title = {Charting a Manifold},
	volume = {15},
	url = {https://proceedings.neurips.cc/paper/2002/hash/8929c70f8d710e412d38da624b21c3c8-Abstract.html},
	abstract = {We construct a nonlinear mapping from a high-dimensional sample space to a low-dimensional vector space, effectively recovering a Cartesian coordinate system for the manifold from which the data is sampled. The mapping preserves local geometric relations in the manifold and is pseudo-invertible. We show how to estimate the intrinsic dimensionality of the manifold from samples, decompose the sample data into locally linear low-dimensional patches, merge these patches into a single low- dimensional coordinate system, and compute forward and reverse map- pings between the sample and coordinate spaces. The objective functions are convex and their solutions are given in closed form.},
	booktitle = {Advances in Neural Information Processing Systems},
	publisher = {{MIT} Press},
	author = {Brand, Matthew},
	urldate = {2023-03-02},
	date = {2002},
	file = {Full Text PDF:/Users/gonzalo/Zotero/storage/HEWYRQCF/Brand - 2002 - Charting a Manifold.pdf:application/pdf},
}

@book{fukunaga_introduction_2013,
	title = {Introduction to Statistical Pattern Recognition},
	isbn = {978-0-08-047865-4},
	abstract = {This completely revised second edition presents an introduction to statistical pattern recognition. Pattern recognition in general covers a wide range of problems: it is applied to engineering problems, such as character readers and wave form analysis as well as to brain modeling in biology and psychology. Statistical decision and estimation, which are the main subjects of this book, are regarded as fundamental to the study of pattern recognition. This book is appropriate as a text for introductory courses in pattern recognition and as a reference book for workers in the field. Each chapter contains computer projects as well as exercises.},
	pagetotal = {606},
	publisher = {Elsevier},
	author = {Fukunaga, Keinosuke},
	date = {2013-10-22},
	langid = {english},
	note = {Google-Books-{ID}: {BIJZTGjTxBgC}},
	keywords = {Computers / Artificial Intelligence / Computer Vision \& Pattern Recognition},
	file = {Fukunaga - Introduction to Statistical Pattern Recognition Second Edition.pdf:/Users/gonzalo/Zotero/storage/TMT4IZRS/Fukunaga - Introduction to Statistical Pattern Recognition Second Edition.pdf:application/pdf},
}

@inproceedings{bengio_curse_2005,
	title = {The Curse of Highly Variable Functions for Local Kernel Machines},
	volume = {18},
	url = {https://proceedings.neurips.cc/paper/2005/hash/663772ea088360f95bac3dc7ffb841be-Abstract.html},
	abstract = {We present a series of theoretical arguments supporting the claim that a large class of modern learning algorithms that rely solely on the smoothness prior  with similarity between examples expressed with a local kernel  are sensitive to the curse of dimensionality, or more precisely to the variability of the target. Our discussion covers supervised, semisupervised and unsupervised learning algorithms. These algorithms are found to be local in the sense that crucial properties of the learned function at x depend mostly on the neighbors of x in the training set. This makes them sensitive to the curse of dimensionality, well studied for classical non-parametric statistical learning. We show in the case of the Gaussian kernel that when the function to be learned has many variations, these algorithms require a number of training examples proportional to the number of variations, which could be large even though there may exist short descriptions of the target function, i.e. their Kolmogorov complexity may be low. This suggests that there exist non-local learning algorithms that at least have the potential to learn about such structured but apparently complex functions (because locally they have many variations), while not using very specific prior domain knowledge.},
	booktitle = {Advances in Neural Information Processing Systems},
	publisher = {{MIT} Press},
	author = {Bengio, Yoshua and Delalleau, Olivier and Roux, Nicolas},
	urldate = {2023-03-02},
	date = {2005},
	file = {Full Text PDF:/Users/gonzalo/Zotero/storage/NYEDLXTC/Bengio et al. - 2005 - The Curse of Highly Variable Functions for Local K.pdf:application/pdf},
}

@misc{bijral_semi-supervised_2012,
	title = {Semi-supervised Learning with Density Based Distances},
	url = {http://arxiv.org/abs/1202.3702},
	doi = {10.48550/arXiv.1202.3702},
	abstract = {We present a simple, yet effective, approach to Semi-Supervised Learning. Our approach is based on estimating density-based distances ({DBD}) using a shortest path calculation on a graph. These Graph-{DBD} estimates can then be used in any distance-based supervised learning method, such as Nearest Neighbor methods and {SVMs} with {RBF} kernels. In order to apply the method to very large data sets, we also present a novel algorithm which integrates nearest neighbor computations into the shortest path search and can find exact shortest paths even in extremely large dense graphs. Significant runtime improvement over the commonly used Laplacian regularization method is then shown on a large scale dataset.},
	number = {{arXiv}:1202.3702},
	publisher = {{arXiv}},
	author = {Bijral, Avleen S. and Ratliff, Nathan and Srebro, Nathan},
	urldate = {2023-03-02},
	date = {2012-02-14},
	eprinttype = {arxiv},
	eprint = {1202.3702 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/gonzalo/Zotero/storage/67RXZ5QX/Bijral et al. - 2012 - Semi-supervised Learning with Density Based Distan.pdf:application/pdf;arXiv.org Snapshot:/Users/gonzalo/Zotero/storage/ESN4PLVZ/1202.html:text/html},
}

@article{yu_density-based_2016,
	title = {Density-based geodesic distance for identifying the noisy and nonlinear clusters},
	volume = {360},
	issn = {0020-0255},
	url = {https://www.sciencedirect.com/science/article/pii/S002002551630281X},
	doi = {10.1016/j.ins.2016.04.032},
	abstract = {Clustering analysis can facilitate the extraction of implicit patterns in a dataset and elicit its natural groupings without requiring prior classification information. For superior clustering analysis results, a number of distance measures have been proposed. Recently, geodesic distance has been widely applied to clustering algorithms for nonlinear groupings. However, geodesic distance is sensitive to noise and hence, geodesic distance-based clustering may fail to discover nonlinear clusters in the region of the noise. In this study, we propose a density-based geodesic distance that can identify clusters in nonlinear and noisy situations. Experiments on various simulation and benchmark datasets are conducted to examine the properties of the proposed geodesic distance and to compare its performance with that of existing distance measures. The experimental results confirm that a clustering algorithm with the proposed distance measure demonstrated superior performance compared to the competitors; this was especially true when the cluster structures in the data were inherently noisy and nonlinearly patterned.},
	pages = {231--243},
	journaltitle = {Information Sciences},
	shortjournal = {Information Sciences},
	author = {Yu, Jaehong and Kim, Seoung Bum},
	urldate = {2023-03-02},
	date = {2016-09-10},
	langid = {english},
	keywords = {Geodesic distance, Mutual neighborhood-based density coefficient, Noisy data clustering, Nonlinearity},
	file = {ScienceDirect Snapshot:/Users/gonzalo/Zotero/storage/PK6FXSRE/S002002551630281X.html:text/html;Yu and Kim - 2016 - Density-based geodesic distance for identifying th.pdf:/Users/gonzalo/Zotero/storage/LL74HFAV/Yu and Kim - 2016 - Density-based geodesic distance for identifying th.pdf:application/pdf},
}

@inproceedings{simard_tangent_1991,
	title = {Tangent Prop - A formalism for specifying selected invariances in an adaptive network},
	volume = {4},
	url = {https://proceedings.neurips.cc/paper/1991/hash/65658fde58ab3c2b6e5132a39fae7cb9-Abstract.html},
	abstract = {In many machine learning applications, one has access, not only to training  data, but also to some high-level a priori knowledge about the desired be(cid:173) havior of the system. For example, it is known in advance that the output  of a character recognizer should be invariant with respect to small spa(cid:173) tial distortions of the input images (translations, rotations, scale changes,  etcetera).  We have implemented a scheme that allows a network to learn the deriva(cid:173) tive of its outputs with respect to distortion operators of our choosing.  This not only reduces the learning time and the amount of training data,  but also provides a powerful language for specifying what generalizations  we wish the network to perform.},
	booktitle = {Advances in Neural Information Processing Systems},
	publisher = {Morgan-Kaufmann},
	author = {Simard, Patrice and Victorri, Bernard and {LeCun}, Yann and Denker, John},
	urldate = {2023-03-02},
	date = {1991},
	file = {Full Text PDF:/Users/gonzalo/Zotero/storage/ZFIBR7BL/Simard et al. - 1991 - Tangent Prop - A formalism for specifying selected.pdf:application/pdf},
}

@book{wand_kernel_1995,
	location = {Boston, {MA}},
	title = {Kernel Smoothing},
	isbn = {978-0-412-55270-0 978-1-4899-4493-1},
	url = {http://link.springer.com/10.1007/978-1-4899-4493-1},
	publisher = {Springer {US}},
	author = {Wand, M. P. and Jones, M. C.},
	urldate = {2023-03-02},
	date = {1995},
	langid = {english},
	doi = {10.1007/978-1-4899-4493-1},
	file = {Wand and Jones - 1995 - Kernel Smoothing.pdf:/Users/gonzalo/Zotero/storage/662G7HUL/Wand and Jones - 1995 - Kernel Smoothing.pdf:application/pdf},
}
