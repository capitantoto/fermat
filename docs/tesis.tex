%% LyX 2.3.7 created this file.  For more info, see http://www.lyx.org/.
%% Do not edit unless you really know what you are doing.
\documentclass[spanish]{article}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{color}
\usepackage{babel}
\addto\shorthandsspanish{\spanishdeactivate{~<>}}

\usepackage{verbatim}
\usepackage{textcomp}
\usepackage{mathtools}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{wasysym}
\usepackage[unicode=true,pdfusetitle,
 bookmarks=true,bookmarksnumbered=false,bookmarksopen=false,
 breaklinks=false,pdfborder={0 0 1},backref=false,colorlinks=false]
 {hyperref}

\makeatletter
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Textclass specific LaTeX commands.
\theoremstyle{plain}
\newtheorem{thm}{\protect\theoremname}
\theoremstyle{definition}
\newtheorem{defn}[thm]{\protect\definitionname}
\theoremstyle{definition}
\newtheorem{example}[thm]{\protect\examplename}
\theoremstyle{remark}
\newtheorem{rem}[thm]{\protect\remarkname}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% User specified LaTeX commands.
\usepackage{babel}
\usepackage{enotez}      % <-- instead of \usepackage{endnotes}
\setenotez{backref=true}
% \let\footnote=\endnote
% \usepackage[symbol]{footmisc}
% \renewcommand{\thefootnote}{\fnsymbol{footnote}}

\makeatother

\providecommand{\definitionname}{Definición}
\providecommand{\examplename}{Ejemplo}
\providecommand{\remarkname}{Observación}
\providecommand{\theoremname}{Teorema}

\begin{document}
\noindent 
\global\long\def\R{\mathbb{R}}%
\global\long\def\dimx{d_{x}}%
\global\long\def\Rdimx{\mathbb{R}^{\dimx}}%
\global\long\def\Rd{\Rdimx}%
\global\long\def\M{\mathcal{M}}%
\global\long\def\dimm{d_{\M}}%
\global\long\def\var{\mathcal{\M}}%
\global\long\def\calR{\mathcal{R}}%
\global\long\def\Lj{\mathcal{L}_{j}}%
\global\long\def\H{\mathbf{H}}%

\global\long\def\norm#1{\left\Vert #1\right\Vert }%
\global\long\def\t#1{\text{#1}}%
\global\long\def\X{\left\{  \mathbf{X}\right\}  }%
\global\long\def\card#1{\left|#1\right|}%
\global\long\def\ind#1{\mathbb{\mathbf{1}}\left\{  #1\right\}  }%
\global\long\def\prob#1{Pr\left(#1\right)}%

\title{Distancia de Fermat en Clasificadores de Densidad por Núcleos}
\author{Lic. Gonzalo Barrera Borla}
\date{Buenos Aires, 02/03/23}

\maketitle

\begin{center}
\includegraphics[scale=0.3]{logofac} 
\par\end{center}

\begin{center}
\medskip{}
 UNIVERSIDAD DE BUENOS AIRES 
\par\end{center}

\begin{center}
Facultad de Ciencias Exactas y Naturales 
\par\end{center}

\begin{center}
Instituto del Cálculo 
\par\end{center}

\begin{center}
\vspace{1cm}
 
\par\end{center}

\begin{center}
Tesis presentada para optar al título de Magíster en Estadística Matemática
de la Universidad de Buenos Aires 
\par\end{center}

\begin{center}
\vspace{1cm}
 
\par\end{center}

\begin{center}
Director: Dr. Pablo Groisman 
\par\end{center}

\pagebreak{} 
\begin{abstract}
TODO 
\end{abstract}
\pagebreak{}

\tableofcontents{}

\pagebreak{}

\part{Forma Final}

\section*{Notación}

$\R$: los números reales

\section{Preliminares}

\subsection{El problema de clasificación}

\subsubsection{Definición del problema unidimensional}
\begin{defn}
[muestra aleatoria]Sea 
\end{defn}

Consideremos el problema de clasificación: 
\begin{defn}
[problema de clasificación]\label{def:prob-clf-1} Sea $\X=\left\{ X_{1},\dots,X_{N}\right\} ,\ X_{i}\in\R^{\dimx}\ \forall i\in\left[N\right]$
una muestra de $N$ observaciones aleatorias $\dimx-$dimensionales,
repartidas en $M$ clases $C_{1},\dots,C_{M}$ mutuamente excluyentes
y conjuntamente exhaustivas\footnote{es decir, $\forall\ i\in\left[N\right]\equiv\left\{ 1,\dots,N\right\} ,X_{i}\in C_{j}\iff X_{i}\notin C_{k},k\in\text{\ensuremath{\left[M\right]}},k\neq j$}.
Asumamos además que la muestra está compuesta de observaciones independientes
entre sí, y las observaciones de cada clase están idénticamente distribuidas
según su propia ley: si $\card{C_{j}}=N_{j}$ y $X_{i}^{\left(j\right)}$representa
la i-ésima observación de la clase $j$, resulta que $X_{i}^{(j)}\sim\mathcal{L}_{j}\left(X\right)\ \forall\ j\in\text{\ensuremath{\left[M\right]}},i\in\left[N_{j}\right]$.

Dada una nueva observación $x_{0}$ cuy
\end{defn}


\subsubsection{Clasificadores \guillemotleft duros\guillemotright{} y \guillemotleft suaves\guillemotright}
\begin{defn}
a clase es desconocida, 
\begin{enumerate}
\item (clasificación dura) ¿a qué clase deberíamos asignarla? 
\item (clasificación suave) ¿qué probabilidad tiene de pertenecer a cada
clase $C_{j},j\in\left[M\right]$ ? 
\end{enumerate}
\end{defn}

Cualquier método o algoritmo que pretenda responder el problema de
clasificación, prescribe un modo u otro de combinar toda la información
muestral disponible, ponderando las $N$ observaciones relativamente
a su cercanía o similitud con $x_{0}$. Por caso, $k-$vecinos más
cercanos ($k-$NN) asignará la nueva observación $x_{0}$ a la clase
modal - la más frecuente - entre las $k$ observaciones de entrenamiento
más cercanas\emph{ }en distancia euclídea $\norm{x_{0}-\cdot}$. $k-$NN
no menciona explícitamente las leyes de clase $\mathcal{L}_{j}$,
lo cual lo mantiene sencillo a costa de ignorar la estructura del
problema.

\subsubsection{Clasificador de Bayes}

\subsubsection{KDE: Estimación de la densidad por núcleos}

\subsection{La maldición de la (alta) dimensionalidad}

\subsubsection{NB: El clasificador de \guillemotleft ingenuo\guillemotright{} de
Bayes}

\paragraph{Una muestra adversa}

\subsubsection{KDC Multivariado}

\paragraph{El caso 2-dimensional}

\paragraph{Relación entre $\protect\H$ y la distancia de Mahalanobis}

\subsection{La hipótesis de la variedad}

\subsubsection{KDE en variedades de Riemann}

\subsubsection{Variedades desconocidas}

\subsection{Aprendizaje de distancias}

\subsubsection{Isomap}

\subsubsection{Distancias basadas en densidad}

\subsection{Distancia de Fermat}
\begin{itemize}
\item Groisman \& Jonckheere
\item Little \& Mackenzie
\item Bijral
\item Bengio
\end{itemize}

\section{Propuesta Original}

\subsection{KDC con Distancia de Fermat Muestral}

\subsection{f-KNN}

\section{Evaluación}

\subsection{Metodología}

\subsubsection{Tareas Puntuadas (acc \%, pseudo-R\textasciicircum 2 if poss.)}

\subsubsection{Algoritmos de referencia}

\paragraph{Uno complejo: SVC}

\paragraph{Uno sencillo: 1-NN - tal vez?}

\paragraph{Uno conocido: LR - tal vez?}

\subsubsection{Datasets}

\paragraph{Datasets sintéticos baja dimensión}

\paragraph{Datasets reales en \guillemotleft mediana\guillemotright{} dimensión}

\paragraph{Dígitos}

\paragraph{PCA-red MNIST}

\section{Análisis de Resultados}

\subsection{Datasets sintéticos, Baja dimensión}

\subsection{Datasets orgánicos, Mediana dimensión}

\subsection{Alta dimensión: Dígitos}

\subsection{Efecto de dimensiones \guillemotleft ruidosas\guillemotright}

\subsection{fKDC: Interrelación entre $h,\alpha$}

\subsection{fKNN: Comportamiento local-global}

\section{Comentarios finales}

\subsection{Conclusiones}

\subsection{Posibles líneas de desarrollo}

\subsection{Relación con el estado del arte}

\section{Referencias}

\section{Código Fuente}

\subsection{sklearn}

\subsection{fkdc}

\part{Disponibles}

\section{El problema de clasificación}

\subsection{El problema de clasificacion}

Consideremos el problema de clasificación: 
\begin{defn}
[problema de clasificación]\label{def:prob-clf} Sea $\X=\left\{ X_{1},\dots,X_{N}\right\} ,\ X_{i}\in\R^{\dimx}\ \forall i\in\left[N\right]$
una muestra de $N$ observaciones aleatorias $\dimx-$dimensionales,
repartidas en $M$ clases $C_{1},\dots,C_{M}$ mutuamente excluyentes
y conjuntamente exhaustivas\footnote{es decir, $\forall\ i\in\left[N\right]\equiv\left\{ 1,\dots,N\right\} ,X_{i}\in C_{j}\iff X_{i}\notin C_{k},k\in\text{\ensuremath{\left[M\right]}},k\neq j$}.
Asumamos además que la muestra está compuesta de observaciones independientes
entre sí, y las observaciones de cada clase están idénticamente distribuidas
según su propia ley: si $\card{C_{j}}=N_{j}$ y $X_{i}^{\left(j\right)}$representa
la i-ésima observación de la clase $j$, resulta que $X_{i}^{(j)}\sim\mathcal{L}_{j}\left(X\right)\ \forall\ j\in\text{\ensuremath{\left[M\right]}},i\in\left[N_{j}\right]$.

Dada una nueva observación $x_{0}$ cuya clase es desconocida, 
\begin{enumerate}
\item (clasificación dura) ¿a qué clase deberíamos asignarla? 
\item (clasificación suave) ¿qué probabilidad tiene de pertenecer a cada
clase $C_{j},j\in\left[M\right]$ ? 
\end{enumerate}
\end{defn}

Cualquier método o algoritmo que pretenda responder el problema de
clasificación, prescribe un modo u otro de combinar toda la información
muestral disponible, ponderando las $N$ observaciones relativamente
a su cercanía o similitud con $x_{0}$. Por caso, $k-$vecinos más
cercanos ($k-$NN) asignará la nueva observación $x_{0}$ a la clase
modal - la más frecuente - entre las $k$ observaciones de entrenamiento
más cercanas\emph{ }en distancia euclídea $\norm{x_{0}-\cdot}$. $k-$NN
no menciona explícitamente las leyes de clase $\mathcal{L}_{j}$,
lo cual lo mantiene sencillo a costa de ignorar la estructura del
problema.

\subsection{Clasificadores de densidad}

Una familia bastante genérica de métodos para resolver el problema
de calsificación, consisten aproximadamente de los siguientes pasos: 
\begin{enumerate}
\item Hacer algunos supuestos sobre la forma de las leyes $\Lj$ 
\item Hallar estimadores $\hat{\Lj}$ de cada ley $\Lj$ usando las muestras
de cada clase $\X^{\text{\ensuremath{\left(j\right)}}}=\left\{ X_{1}^{\left(j\right)},\dots,X_{N_{j}}^{\left(j\right)}\right\} $
y algún procedimiento estándar \footnote{e.g.: máxima verosimilitud, método de momentos, et cetera} 
\item [clasificador]\label{def:regla-clf} Definir una regla de decisión
- un \emph{clasificador} - $\mathcal{R}\left(x\vert\hat{\Lj},j\in\left[M\right]\right):x\in S\rightarrow\left[M\right]\ni j$
que dados los estimadores de (2), asigne la observación $x_{0}$ a
la clase $\mathcal{R}\left(x_{0}\right)$. 
\end{enumerate}
Esta familia de clasificadores, se distingue por una explícita \emph{estimación
de densidades} que más tarde se utilizarán para la tarea de clasificación
en sí. 
\begin{example}
\label{exa:lda}El análisis de discriminante lineal (LDA) de Fisher\cite{LinearDiscriminantAnalysis2022}
para clasificación binaria ($j\in\text{}$$\left\{ 0,1\right\} $)
se encuadra en esta familia de la siguiene manera:
\begin{enumerate}
\item Las las leyes $\Lj$ 
\begin{enumerate}
\item son todas distribuciones normales con media $\mu_{j}$ y 
\item homocedásticas: $\Sigma_{j}=\Sigma\ \forall\ j\in\text{\ensuremath{\left[M\right]})}$. 
\end{enumerate}
\item Estimamos $\hat{\Lj}=N\left(\hat{\mu_{j}},\hat{\Sigma}\right)$ como
normales de medias independientes y varianza única por máxima verosimilitud,
\begin{align*}
\hat{\mu_{j}} & =N_{j}^{-1}\sum_{i=1}^{N_{j}}x_{i}^{(j)},\ \forall j\in\left\{ 0,1\right\} \\
\hat{\Sigma} & =N^{-1}\sum_{j=1}^{M}\sum_{{i=1}}^{N_{j}}(x_{i}^{(j)}-\hat{\mu_{j}})(x_{i}^{(j)}-\hat{\mu_{j}}).
\end{align*}
\item El clasificador es simplemente la función indicadora $\ind{\cdot}$del
discriminante lineal
\[
\mathcal{R}\left(x\right)=\ind{w\cdot x>c},\ \t{donde}\ w=\hat{\Sigma}^{-1}(\hat{\mu_{1}}-\hat{\mu_{0}})\ \t y\ c={\displaystyle w\cdot{\frac{1}{2}}(\hat{\mu_{1}}+\hat{\mu_{0}})}
\]
\end{enumerate}
\end{example}

Generalmente, mientras más restrictivos sean los supuestos de (1),
más sencilla de computar será la regla (3), y viceversa. y la generalidad
del clasificador resultante: el trabajo del buen científico será encontrar
el compromiso óptimo.\footnote{N. del A.: En las famosas palabras de George Box, 
\begin{quotation}
Todos los modelos son incorrectos, pero algunos son útiles.
\end{quotation}
El modelado estadístico es, aún hoy, más arte que técnica, y en palabras
de Picasso,
\begin{quotation}
Todos sabemos que el arte no es la verdad. El arte es una mentira
que nos acerca a la verdad, al menos aquella que no es dado comprender.
El artista debe saber el modo de convencer a los demás de la verdad
de sus mentiras.
\end{quotation}
} 

En el ejemplo de \ref{exa:lda}, los supuestos (leyes normales y homocedasticidad)
son inverosímiles en casi cualquier escenario real, pero el clasificador
resultante es muy sencillo de computar. En general, este será el caso
para todos los métodos\emph{ paramétricos} de estimación de densidad,
que acotan el espacio de densidades posibles a aquellas que se pueden
expresar de forma cerrada con una expresión predefinida (en este caso,
la densidad normal), y $Q$ parámetros (aquí, $\mu_{0},\mu_{1},\Sigma$).

Alternativamente, existen métodos en que los supuestos de (1) se obvian
del todo, o al menos son lo suficientemente generales como para representar
todas salvo las más patológicas leyes de probabilidad\footnote{e.g.: asumir que la media y dispersión son finitas}.
A estos se los conoce como métodos \emph{no paramétricos} de estimación
de densidad.

\subsection{Estimación de densidad por núcleos}

La estimación de densidad por núcleos (KDE\footnote{Kernel Density Estimation},
por sus siglas en inglés), es uno de los métodos mejor estudiados
dentro del amplio universo no-paramétrico. Introducidos hacia 1960\cite{parzenEstimationProbabilityDensity1962,rosenblattRemarksNonparametricEstimates1956}
para variables aleatorias unidimensionales, han sido ampliamente desarrollados
y adaptados a espacios mucho más generales. El objetivo es encontrar
un estimador \emph{suave} de la densidad poblacional $f$ de una v.a.
$X$ a partir de una muestra discreta, usando una función no-negativa
$K$ llamada \emph{núcleo} (``kernel'') y un parámetro de suavización
$h$, el \emph{ancho de banda} (``bandwith''). La notación y nomenclatura
para KDE es heterogénea; en la exposición que sigue, tomaremos de
referencia el abarcador tratado de \cite{wandKernelSmoothing1995}
\begin{defn}
[función núcleo]\label{def:kern-univ}(función núcleo) Una función
$K:\R\rightarrow\R$ es un \emph{núcleo} (``kernel''), si
\begin{itemize}
\item toma únicamente valores reales no-negativos: $K\left(u\right)\geq0\ \forall\ u\in\t{sop}K$,
\item está normalizada: $\int_{-\infty}^{+\infty}K\left(u\right)du=1$ y
\item es simétrica: $K\left(u\right)=K\left(-u\right)\ \forall\ u\in\t{sop}K$
\end{itemize}
\end{defn}

\begin{rem}
La no-negatividad y simetría no son forzosamente necesarias, pero
van a otorgarles propiedades muy convenientes al estimador resultante.
Cualquier función de densidad univariada cumple con la no-negatividad
y normalización, y muchas, como la ley uniforme y la gaussiana, son
además simétricas, convierti´ndolas en núcleos bastante populares.
\end{rem}

\begin{rem}
Para todo núcleo $K$ y $\lambda\in\R,\ J\text{\ensuremath{\left(u\right)}}=\lambda K\left(\lambda u\right)$
también es un núcleo, lo cual permite construir núcleos adecuadamente
escalados a los datos. Usaremos la notación $K_{h}\text{\ensuremath{\left(u\right)}}=h^{-1}K\left(u/h\right)$
para referirnos a estos núcleos escalados.
\end{rem}

\begin{defn}
[KDE univariado]\label{def:kde-univ} Sea $\X$ una muestra de $N$
elementos aleatorios i.i.d. tomada de cierta distribución univariada
con densidad desconocida $f$. Su estimador de densidad por núcleos
(su ``KDE'') es

${\displaystyle \widehat{f}(x;h)=\frac{1}{n}\sum_{i=1}^{n}K_{h}(x-X_{i})=\frac{1}{nh}\sum_{i=1}^{n}K\Big(\frac{x-X_{i}}{h}\Big)}$
\end{defn}

Dejando por un momento de lado qué par $\left(K,h\right)$ usar, podemos
derivar un clasificador ``duro'' de manera directa para la versión
univariada del \nameref{def:prob-clf}:
\begin{defn}
[clasificador KDE univariado]\label{def:clf-kde-univ}. Sea $C:\Rdimx\rightarrow\left[M\right]$
la ``función de clase'', tal que $\forall\ x\in\Rdimx,\ C\left(x\right)=j\iff x\in C_{j}$.
Sean además $\hat{f}^{(1)},\dots,\hat{f}^{(M)}$ los $M$ estimadores
de densidad de cada clase obtenidos obtenidos según \nameref{def:kde-univ}.
El ``clasificador por estimación de densidad nuclear'' correspondiente
será:
\begin{align*}
\hat{C}\left(x\right) & =\mathrm{\arg\max_{j\in\left[M\right]}\ }\hat{f}_{h}^{(j)}\left(x\right)
\end{align*}

asignando cada observación a la clase en la que maximiza la densidad
estimada.
\end{defn}

Cuando las clases de las cuales se compone la población se encuentran
muy ``separadas'' entre sí \footnote{i.e., $\exists k\in\left[M\right]:f_{h}^{(k)}\text{\ensuremath{\left(x_{0}\right)}\ensuremath{\ensuremath{\gg}}0\ },\ f_{h}^{(j)}\simeq0\ \forall\ j\in\left[M\right]/k$},
la clasificación ``dura'' de \ref{def:clf-kde-univ} será suficiente.
Ahora bien, ¿cómo hacemos para cuantificar la incertidumbre asociada
a la clasificación, cuando existe más de una clase con densidad estimada
no despreciable? ¿Y si creemos que las clases no son equiprobables
a priori? Como las $\hat{f}_{h}^{(j)}$ estimadas identifican distribuciones,
podemos utilizar la regla de Bayes para construir un <<clasificador
suave>>. Sea $p\left(A\right)$ la probabilidad de $A$, y consideremos
que la proporción muestral de cada clase es una distribución \emph{a
priori} razonable para las clases bajo consideración (es decir, $\hat{p}\left(C_{j}\right)=N_{j}/N$
es un estimador insesgado de $p\left(C_{j}\right)$). Luego, 
\begin{defn}
[clasificador KDE univariado suave]\label{def:soft-clf-kde} Sea
el \nameref{def:prob-clf} y los estimadores \nameref{def:kde-univ}.
Por la regla de bayes, 
\[
\prob{C\left(x\right)=j}=\frac{f^{(j)}\left(x\right)\cdot\prob{C_{j}}}{\prob x}
\]

Reemplazando el a priori $p\text{\ensuremath{\left(C_{j}\right)}}$
por su estimación muestral, las densidades $f^{(j)}$ por sus estimadores
y usando la ley de la probabilidad total para expandir $p\left(x\right)$,
obtenemos:
\[
\hat{p}_{j}=\hat{p}\left(C\left(x\right)=j\right)=\frac{\hat{f}_{h}^{(j)}\left(x\right)\cdot N_{j}}{\sum_{i\in\text{\ensuremath{\left[M\right]}}}\hat{f}_{h}^{(i)}\left(x\right)\cdot N_{i}}
\]

El vector $M-$dimensional $\left(\hat{p}_{1},\dots,\hat{p}_{M}\right)$
es una <<clasificación suave>> de $x$ en las $M$ posibles clases
disponibles.
\end{defn}


\subsection{KDE multivariado}

En el contexto univariado, no hay direcciones en el espacio, sólo
sentido, positivo o negativo. Más aún, el peso de cada $X_{i}$ en
$\hat{f}\left(x\right)$ es $K\left(x-X_{i}\right)$, y como $K$
es simétrica respecto al 0, sólo importa el \emph{valor absoluto }-
la \emph{distancia -} entre el nuevo punto y cada observación. En
una dimensión al menos, el núcleo $K$ pondera - escalando por $h$
- la distancia (euclídea) entre el punto a clasificar y cada datum:

\[
K_{h}\left(x_{0}-x_{i}\right)=K_{h}\left(\left|x_{0}-x_{i}\right|\right)=\frac{1}{h}K\left(\frac{\norm{x_{0}-x_{i}}}{h}\right)
\]

En mayore dimensiones, la situación es más compleja, pero directamente
análoga.
\begin{defn}
[KDE multivariado]\label{def:kde-multiv}(Sección 4.2 en \cite{wandKernelSmoothing1995})
En su forma más general, el estimador de densidad nuclear $d-$dimensional
es 

\[
{\displaystyle \widehat{f}(x;\H)=N^{-1}\sum_{i=1}^{N}K_{\H}\left(x-X_{i}\right)}
\]

donde $\H$ es una matriz $d\times d$ simétrica positiva definida,
análoga al \emph{ancho de banda} unidimensional $h$,

\[
K_{\H}\left(x\right)=\left|\det\H\right|^{-1/2}K\left(\H^{-1/2}x\right)
\]

y $K:\R^{d}\rightarrow\R_{0}^{+}$ es una función núcleo que satisface
\[
\int_{\R^{d}}K\left(u\right)du=1
\]
\end{defn}

Como en el contexto escalar, el núcleo suele ser una funciones de
densidad aleatoria $d-$variada.\emph{ }Un núcleo muy popular es el
la densidad normal estándar 
\[
K\left(x\right)=\left(2\pi\right)^{-d/2}\exp\left(-\frac{\norm x^{2}}{2}\right)
\]
,

un núcleo \emph{esférico }o \emph{radialmente simétrico}. En este
caso, $K_{\H}\left(x-X_{i}\right)$ es equivalente a la densidad $\mathcal{N}\left(X_{i},\H\right)$
en el vector $x$.

Cuando $\H=h^{2}\mathbf{I}$, el estimador resultante es consistente
con el producto de $d$ estimadores \nameref{def:kde-univ}s:
\begin{align*}
{\displaystyle \widehat{f}(x;h^{2}\mathbf{I})} & =N^{-1}\sum_{i=1}^{N}\left|\det h^{2}\mathbf{I}\right|^{-1/2}K\left(\left(h^{2}\mathbf{I}\right)^{-1/2}\left(x-X_{i}\right)\right)\\
 & =N^{-1}h^{-d}\sum_{i=1}^{N}K\left(\left(x-X_{i}\right)/h\right)
\end{align*}

\begin{rem}
[distancia de Mahalanobis]\label{rem:mahalanobis-dist}Dada una distribución
de probabilidad $Q$ en $\R^{d},$ con media $\mu\in\R^{d}$ y matriz
de covarianza positiva definida $\mathbf{\Sigma}\in\R^{d\times d}$,
la \emph{distancia de Mahalanobis}\footnote{https://en.wikipedia.org/wiki/Mahalanobis\_distance}
de un punto $x$ a $Q$ es
\[
d_{M}\text{\ensuremath{\left(x,Q\right)}=\ensuremath{\sqrt{\left(x-\mu\right)^{T}\mathbf{\Sigma}^{-1}\left(x-\mu\right)}}}
\]

Dados dos puntos $x,y$ en $\R^{n}$, la distancia de Mahalanobis
\emph{entre si} con respecto a $Q$ es
\[
d_{M}\text{\ensuremath{\left(x,Q\right)}}=d_{M}\left(x,\mu;Q\right)
\]

Como $\mathbf{\Sigma}$ es definida positiva, también lo es $\mathbf{\Sigma}^{-1}$,
con lo que las raíces cuadradas están bien definidas. Por el teorema
espectral, $\mathbf{\Sigma}^{-1}$ se puede descomponer en $\mathbf{\Sigma}^{-1}=W^{T}W$
para alguna matriz real $d\times d$, lo cual sugiere una definición
equivalente

$d_{M}\left(x,y;Q\right)=\norm{W\left(x-y\right)}$

donde $\norm{\cdot}$ es la norma euclídea. Es decir, la distancia
de Mahalanobis es la distancia euclídea luego de una transformación
de blanqueo.

Reemplazando $W=\H,\ \mu=X_{1},\dots,X_{N}$, podemos redefinir el
estimador \nameref{def:kde-multiv} como un estimador de núcleos basado
en la distancia de Mahalanobis de $x$ a las distribuciones $\mathcal{N}\left(X_{i},\H\right)$.
\end{rem}

La relativa sencillez para el cómputo del método hasta aquí descrito
lo hace un perenne favorito entre los estimadores de densidad no paramétricos,
en particular cuando tomamos $\H=\text{\textbf{C}}$, donde \textbf{C}
es el estimador muestral de la covarianza de $\X$, lo cual simplifica
radicalmente la cantidad de parámetros a ajustar. Sin embargo, el
método posee algunas conocidas desventajas \cite{jenq-nenghwangNonparametricMultivariateDensity1994}:
\begin{enumerate}
\item \label{enu:dif-kde-esf}Salvo en casos excepcionalmente bien portados,
la dirección y dispersión \emph{local} de la muestra alrededor de
un cierto punto $x_{i}$ típicamente no coincidirá con la dirección
y dispersión\emph{ global} \textbf{C }computada en la muestra completa.
\item Aún cuando la estimación global de $\mathbf{C}$ sea localmente adecuada,
no resulta inmediatamente obvio que la suavización $\mathbf{H=C}$
inducida por la muestra sea óptima en términos de representación de
la densidad para regiones de alta densidad y outliers a la vez. Los
estimadores por densidad nuclear así construidos suelen suavizar de
más\footnote{\emph{oversmooth}} en regiones de alta densidad, y de
menos\footnote{\emph{undersmooth}} alrededor de los \emph{outliers}
en la muestra.
\item Al ubicar una ``montañita'' de densidad en \emph{cada} dato de la
muestra, el cómputo del estimador hasta aquí expuesto se vuelve prohibitamente
costoso para $N$ relativamente grande.
\end{enumerate}
Distintos autores han intentado solucionar estas dificultades con
éxito mixto.

\subsubsection{Funciones de pérdida alternativas}

Minimizar el (A)MISE como criterio de bondad en la evaluación de $\hat{f}$
responde antes que nada a conveniencias para la manipulación algebraica\footnote{todos sabemos qué bien se portan los cuadrados}.
La diferencia \emph{absoluta}\textbf{\emph{ }}del error integrado
medio\footnote{$MIAE\left(\hat{f}_{\mathbf{H}},f\right)=E\int_{\R^{d}}\left|\hat{f}_{\mathbf{H}}\left(y\right)-f\left(y\right)\right|dy$,
y AMIAE análogo a AMISE\ref{fn:amise}} es una alternativa atractiva: a diferencia del error \emph{cuadrático},
el error absoluto es invariante con respecto a transformaciones monótonas
de los datos\cite{garcia-portuguesShortCourseNonparametric2022}.
A pesar de esta deseable propiedad, el tratamiento es arduo en $d=1$
y excruciante en dimensiones mayores.

Motivado por la aplicación concreta de estimación de densidad al problema
de clasificación, \cite{hallBandwidthChoiceNonparametric2005} toma
un camino más directo: minimiza el \emph{riesgo de Bayes} de $\hat{f}\left(x;h\right),$$h\in\R$
, que tiene una interpretación inmediata en el \ref{def:prob-clf}.
\begin{defn}
[riesgo de Bayes]\label{def:riesgo-bayes}. Sea $\mathcal{R}\left(\cdot|f_{1},\dots,f_{K}\right)$
un \nameref{def:regla-clf} y una región $\Gamma\subseteq\t{dom}\ X$.
El \emph{riesgo de Bayes} asociado a $\calR$ en $\Gamma$ es 
\begin{align*}
\text{err}\left(\calR\vert\ensuremath{\Gamma}\right) & =\\
 & \sum_{j=1}^{K}p_{j}\int_{\Gamma}Pr\left(x\text{\textbf{ no }sea clasificado por \ensuremath{\mathcal{R}} como \ensuremath{\in C_{j}}}\right)f_{j}\left(x\right)dx
\end{align*}
\end{defn}

Hall muestra que el el clasificador de \ref{def:clf-kde-univ} es
una regla óptima en el sentido del riesgo de Bayes - y por ende, para
clasificación. Luego, sería razonable argumentar que elegir $h$ como
Hall propone es superador a optimizar $h$ para el objetivo intermedio
de estimar las verdaderas densidades de clase $f_{i}$. En un análisis
concienzudo del caso $d=1,K=2,\Gamma\subset\R$, Hall halla que para
el caso más sencillo $K=2,d=1$, según los signos de las derivadas
$f_{1}^{'},f_{2}'$ en los puntos de cruce de $f_{1},f_{2}$ respectivas,
el orden de magnitud del $h$ óptimo varía drásticamente. El rango
de <<malos condicionamientos>> que llevan a estas situaciones, sin
embargo, se vuelve mucho más angosto en el problema multivariado ($d>1$)
o de múltiples clases ($K>2$). En estos contextos, el ancho de banda
óptimo es generalmente el mismo que es apropiado para estimación de
densidad, según (A)M{[}I|S{]}E.

\subsubsection{La elección de H en el caso bivariado}

Para ilustrar la creciente complejidad en la elección de los coeficientes
de \textbf{H}, consideremos el caso multivariado más sencillo, $d=2$,
siguiendo a \cite{wandComparisonSmoothingParameterizations1993} que
realizan un estudio exhaustivo de este problema, en relación a un
conjunto de densidades $f$ que se desea estimar vía KDE, con distintas
propiedades\footnote{e.g., con componentes con y sin correlación, sesgadas, kurtoticas,
bi-, tri- y tetra-modales.} que dificulten la tarea. 

Consideremos las familias de creciente complejidad para $\mathbf{H}$,
siempre positivas definidas:
\begin{itemize}
\item en términos generales,
\begin{itemize}
\item productos escalares de la identidad: $\mathcal{H}_{1}\coloneqq\left\{ h_{1}^{2}\mathbf{I};h_{1}>0\right\} $
\item matrices diagonales con distintas escalas en cada eje: $\mathcal{H}_{2}\coloneqq\left(\text{diag}\left(h_{1}^{2},h_{2}^{2}\right);h_{1},h_{2}>0\right)$
\item matrices completas: 
\[
\mathcal{H}_{3}\coloneqq\left\{ \left[\begin{array}{cc}
h_{1}^{2} & h_{12}\\
h_{12} & h_{2}^{2}
\end{array}\right];h_{1},h_{2}>0,\left|h_{12}\right|<h_{1}h_{2}\right\} 
\]
,
\end{itemize}
\item basadas en una ``esferización'' de los datos vía matriz de covarianza
muestral $\mathbf{C}=\left[\begin{array}{cc}
c_{11} & c_{12}\\
c_{12} & c_{22}
\end{array}\right]$ :
\begin{itemize}
\item ignorando la correlación $\mathcal{C}_{2}\coloneqq\left\{ h^{2}\mathbf{D};h^{2}>0\right\} $,
con $\mathbf{D}=\text{diag}\left(c_{11,}c_{22}\right)$,
\item completa $\mathcal{C}_{3}\coloneqq\left\{ h^{2}\mathbf{C};h^{2}>0\right\} $
e
\item \emph{híbridas}, con suavizado independiente en cada dirección 
\[
\mathcal{Y}\coloneqq\left\{ \left[\begin{array}{cc}
h_{1}^{2} & \rho_{12}h_{1}h_{2}\\
\rho_{12}h_{1}h_{2} & h_{2}^{2}
\end{array}\right];h_{1},h_{2}>0\right\} 
\]
 y coeficiente de correlación $\rho_{12}=c_{12}/\sqrt{c_{11}c_{22}}$
\end{itemize}
\end{itemize}
Nótese que $\mathcal{H}_{1}\subseteq\mathcal{H}_{2}\subseteq\mathcal{H}_{3},\ \ \mathcal{C}_{2}\subseteq\mathcal{H}_{2},\ \ \mathcal{C}_{3}\subseteq\mathcal{H}_{3},\ \ \mathcal{Y}\subseteq\mathcal{H}_{3}$.
Para cada distribución estudiada y familia de matrices \textbf{H},
se elige la matriz de ancho de banda optimizando el \emph{error cuadrático
integrado medio asintótico}\footnote{[AMISE]\label{fn:amise} El error cuadrático medio integrado (MISE,
por sus siglas en inglés) se define como
\[
MISE\left(\mathbf{H}\right)=MISE\left(\hat{f}_{\mathbf{H}},f\right)=E\int_{\R^{d}}\left(\hat{f}_{\mathbf{H}}\left(y\right)-f\left(y\right)\right)^{2}dy
\]

y su versión asintótica,
\[
AMISE\left(\H\right)=\lim_{N\rightarrow\infty}MISE\left(\H\right)
\]

Luego, fijada una densidad $f$ cuyo KDE se desea estudiar, restringiendo
\textbf{H} a una familia $\mathcal{A}$, se toma 
\[
\mathbf{H_{\mathcal{A}}^{*}}=\arg\inf_{\H\in\mathcal{A}}AMISE\left(\H\right)
\]
}\emph{, }y luego analizan la \emph{eficiencia relativa asintótica}\footnote{[ARE] por \emph{Asymptotic Relative Efficiency, }definido como 
\[
ARE\left(\mathcal{A}:\mathcal{B}\right)=AMISE\left(\mathbf{H_{\mathcal{A}}^{*}}\right)/AMISE\left(\mathbf{H_{\mathcal{B}}^{*}}\right)
\]
} de cada familia $\mathcal{A\in\text{\ensuremath{\left\{  \mathcal{H}_{1},\mathcal{H}_{2},\mathcal{C}_{2},\mathcal{C}_{3},\mathcal{Y}\right\} } }}$,
en comparación con la familia <<irrestricta>> $\mathcal{H}_{3}$,
que en virtud de darle tantos grados de libertad como es posible a
\textbf{H}, tendrá siempre el menor AMISE - a costa de ser la más
difícil de parametrizar.

Los autores notan una dificultad cualitativamente nueva en el caso
multivariado en comparación al univariado: definir la \emph{orientación}
de $\text{\textbf{H}}$. Aún en el relativamente sencillo contexto
bivariado, muestran cómo la estrategia ``ingenua'' de depender para
ello de la covarianza muestral conlleva considerables pérdidas de
eficiencia, aún para la familia $\mathcal{Y}$, la más cercana a $\mathcal{H}_{3}$,
cuando $f$ es multimodal o se aleja de la normalidad.\footnote{W\&J (1993) tiene un lindo ejemplo ``(F) Bimodal II'' de cómo la
covarianza estimada para una mezcla de dos gausianas con diferencias
en la locación sobre el eje x, y mayor dispersión en el eje y, termina
dando una estimación de la covarianza inútil para suavizado. Podría
reproducirlo con scipy+matplotlib para ilustrar. }En su recomendación final, los autores sugieren que en general ``hay
mucho para ganar incluyendo parámetros de orientación'' (es decir,
elementos no-diagonales) en la parametrización de $\mathbf{H}$.

\subsubsection{El caso $d-$\textmd{dimensional} }

Si el dominio bivariado ya presentaba suficientes dificultades para
que ningún método de de elección de \textbf{H }<<dominase>> a los
demás para cualquier densidad $f$, el caso $d-$dimensional general
no es excepción. Como secuela de \cite{wandComparisonSmoothingParameterizations1993},
los autores proponen un estimador ``plug-in'' del $\mathbf{H}$
óptimo - en el sentido de AMISE\footnote{ver \ref{fn:amise}} - que
se puede calcular para \textbf{H }<<completa>>, a través de ciertos
<<funcionales>> $\psi_{\text{\textbf{m}}}$ que dependen de $f$
y sus derivadas parciales de orden $d$\footnote{Sea $\mathbf{m}$ una $d-$tupla $\mathbf{m}=\text{\ensuremath{\left(m_{1},\dots,m_{d}\right)}}$
y $f^{\left(\mathbf{m}\right)}$ la derivada parcial de $f$ en $\mathbf{m}$,
entonces $\psi_{\mathbf{m}}=\int f^{\left(\mathbf{m}\right)}\left(x\right)f\left(x\right)dx$}. Cuando se busca una matriz \textbf{H} completa, aún para dimensiones
moderadas ($d\leq5)$ la cantidad de funcionales a estimar es enorme,
por lo que luego se limitan a matrices diagonales para su aplicación
concreta\cite{wandMultivariatePluginBandwidth1994}.

\cite{duongCrossvalidationBandwidthMatrices2005} sintetiza aportes
propios y ajenos alrededor de la estimación de \textbf{$\mathbf{H}$}
completa según tres métodos de validación cruzada <<deja-uno-afuera>>:
sesgada (BCV), insesgada (UCV), y (SCV)\footnote{en inglés: Biased, Unbiased \& Smoothed Cross Validation}.
Con cada método, busca minimizar cierto error cuadrático: MISE para
UCV; el asintótico AMISE en BCV y una combinación lineal de ambos
define SCV. El método con el que mejores resultados obtienen, SCV,
es también el más complejo en su implementación, pues requiere considerar
un ``suavizador piloto'' $\mathbf{G}\in\R^{d\times d}$ cuya elección
no es transparente.

Una parametrización completa de \textbf{H} en $d$ dimensiones requiere
la hercúlea tarea de elegir $\tbinom{d}{1}+\tbinom{d}{2}=\text{\ensuremath{\left(d^{2}+d\right)}/2}$
coeficientes\footnote{¡y yo me trabo eligiendo entre té ver y té negro!}.
El ya-mencionado trabajo de \cite{jenq-nenghwangNonparametricMultivariateDensity1994}
toma un camino alternativo: pre-transformar los datos para que tengan
media cero y matriz de covarianza unitaria\footnote{práctica también conocida como <<esferización>> o <<blanqueo>>.
En particular, si $\bar{X},\text{\textbf{C}}$ aon respectivamente
la media y covarianza muestral de $\X$, el conjunto $\left\{ \mathbf{Z}\right\} :=\left\{ Z_{i}=\mathbf{C}^{-1/2}\left(X_{i}-\bar{X}\right)\ \forall\ X\in\X\right\} $
es su equivalente blanqueado. Es fácil ver que $E\left[Z\right]=0,\ E\left[ZZ^{T}\right]=\mathbf{I}$.}, y luego intenta buscar $h$ para los datos transformados. La práctica
es equivalente a buscar $\H$ en la familia $\mathcal{C}_{3}\coloneqq\left\{ h^{2}\mathbf{C};h^{2}>0\right\} $
de \cite{wandComparisonSmoothingParameterizations1993}. Hwang encuentra
las dificultades listadas en \eqref{enu:dif-kde-esf}, y compara varios
algoritmos superadores en algún sentido al KDE con ancho de banda
fijo (FKDE).

\paragraph{KDE Adaptativo (AKDE)}

Similar a FKDE, pero con un factor de ancho local $\lambda_{n}$ para
cada núcleo
\[
\hat{f}_{AKDE}\left(z\right)=\frac{1}{Nh^{d}}\sum_{i=1}^{N}\lambda_{i}^{-d}K\left(\frac{1}{h\lambda_{i}}\left(Z-Z_{i}\right)\right)
\]

Aunque cada núcleo estará mejor escalado a su contexto local, el enfoque
sigue utilizando una misma orientación global \textbf{C }para todos
los núcleos. El cómputo de los factores $\lambda_{i}$ ha de resolverse
iterativamente, comenzando por el caso FKDE, $\lambda_{i}=1\forall i\in\left[N\right]$,
con lo cual el costo computacional será aún más alto que en el caso
base.

\paragraph{KDE de base funcional radial (RBF)}

Para minimizar la cantidad de núcleos a ajustar a los datos, divide
el proceso de estimación de densidad en dos partes: (i) agrupar los
datos en clusters según cierto algoritmo no-supervisado, y luego (ii)
ajustar a cada cluster un núcleo gaussiano, su altura y su ancho según
la posición y cantidad de sus observaciones. Por esto, también se
lo conoce com ``modelado de mezclas gaussianas''\footnote{GMM, o \emph{Gaussian Mixture Modelling }en inglés}.
Aunque el estimador final se puede expresar con tan pocos términos
como clusters haya, el procedimiento completo es considerablemente
más complejo que el de FKDE, dependiendo críticamente de la esferización
y remoción de \emph{outliers} para la detección de clusters. Dependiendo
de $N,d$ y la cantidad de clusters identificados, pueden aparecer
núcleos demasiado ``empinados'' o demasiado ``planos''. Así, una
de las principales ventajas de este método - la posibilidad de ajustar
una matriz de covarianza distinta a cada cluster de datos - implicará
una minuciosa inspección de los datos para saber qué escala y orientación
es razonable para cada base.

\paragraph{KDE por \textquotedblleft persecución de la proyección\textquotedblright{}
(PPDE)}

El espíritu de PPDE consiste en buscar iterativamente proyecciones
``interesantes'' de los datos en bajas dimensiones (típicamente
1-D), modificar la muestra original $\left\{ \mathbf{Z}\right\} ^{\left(0\right)}$
para remover la estructura encontrada en la proyección, e iterar el
proceso en los datos resultantes. Siguiendo a \cite{huberProjectionPursuit1985},
la distribución normal se considera la ``menos interesante'', y
será ``más interesante'' aquella proyección de los datos que más
se le aleje.

Para evitar confundir la dirección y escala de la muestra con proyecciones
verdaderamente interesante, el método de PPDE requiere también esferizar
los datos e ignorar \emph{outliers }juiciosamente\footnote{\cite[p. 29]{huberProjectionPursuit1985}}.
Un problema específico a PPDE, es que no puede lidiar satisfactoriamente
con estructuras ``escondidas'' en alta dimensión, <<detrás>> de
proyecciones en baja dimensión\footnote{e.g., las proyecciones unidimensionales de una densidad en $\R^{2}$
con forma de dona no dan cuenta fehaciente de la estructura original.
Estimar estas \emph{variedades} escondida en el espacio ambiente euclídeo
será un punto central de nuestro trabajo más adelante.}


\subsection{La maldición de la dimensionalidad}

Hasta aquí, pareciera ser que el enfoque de estimación de densidad
por núcleos para el caso multivariado está irremediablemente condenado
al fracaso, o al menos a una agotadora complejidad. Sin embargo, antes
de claudicar, vale la pena entender algunas de las razones de tamaña
complejidad.

Una dificultad obvia es que aún considerando un único suavizador global
\textbf{H}, en $d$ dimensiones hacen falta estimar $\tbinom{d}{1}+\tbinom{d}{2}=\text{\ensuremath{\left(d^{2}+d\right)}/2}$
varianzas y covarianzas, respectivamente. El crecimiento cuadrático
en la cantidad de parámetros implicará que el tamaño muestral $N$
necesario para obtener estimaciones razonables crezca insosteniblemente.
El fenómeno, conocido como ``maldición de la dimensionalidad'',
se puede entender intuitivamente considerando el siguiente escenario:
\begin{rem}
[maldición de la dimensionalidad]\label{rem:curse-dim}Sea $B\left(c,r,d\right)$
la bola $d-$dimensional de radio $r$ centrada en $c\in\R^{d}$,
y $X$ v.a. uniformemente distribuida (por volumen), $X\sim Unif\left(B\left(0,r,d\right)\right)$.
Sea $\epsilon>0$; cuál es la probabilidad de que $X$ se encuentre
al ``interior'' de la bola, sustrayendo un ``cascarón'' externo
de espesor $\epsilon$, $\prob{X\in B\left(0,r-\epsilon,d\right)}$?

Como la distribución de $X$ es uniforme en volumen, y $B\left(0,r-\epsilon,d\right)\subset B\left(0,r,d\right)$,
basta con comparar los volúmenes de de ambas $d-$esferas para encontrar
la solución. El volumen$d-$dimensional de una bola es

\[
{\displaystyle Vol\left(B\left(\cdot,r,d\right)\right)=Vol_{B}\left(r,d\right)={\frac{\pi^{d/2}}{\Gamma\left(\tfrac{d}{2}+1\right)}}r^{d}}
\]

donde ${\displaystyle \Gamma\left(z\right)=\int_{0}^{\infty}t^{z-1}e^{-t}\,dt}$
es la función gamma. Luego,
\[
Pr\text{\ensuremath{\left(X\in B\left(0,r-\epsilon,d\right)\right)}}=\frac{Vol_{B}\left(r-\epsilon,d\right)}{Vol_{B}\left(r,d\right)}=\left(\frac{r-\epsilon}{r}\right)^{d}
\]

Como $\left(\frac{r-\epsilon}{r}\right)<1$, $\lim_{d\rightarrow\infty}Pr\text{\ensuremath{\left(X\in B\left(0,r-\epsilon,d\right)\right)}}\rightarrow0$.
Es decir, a medida que crece la dimensión del soporte de $X$, el
``interior'' de la bola esta (casi) vacío, y la distribución de
$X$ se concentra en el ``cascarón'' exterior. Aún para valores
moderados de $d,\epsilon$ el efecto es pronunciado. Por ejemplo,
en 20 dimensiones, un cascarón de 2\% de espesor ($\epsilon=0.02r$)
concentrará $1-\text{\ensuremath{\left(\tfrac{r-\epsilon}{r}\right)}}^{d}=1-0.98^{20}=0.6676\dots\approx\text{¡}2/3$
de la masa de probabilidad de $X$!

Este enorme ``vacío'' en el espacio de alta dimensión, se traduce
en una irrelevancia de las métricas ``ingenuas'' de distancia. Como
$x\in B\left(0,r,d\right)\iff\norm x\leq r$, y similarmente $x\notin B\left(0,r-\epsilon,d\right)\iff\norm x>\left(r-\epsilon\right)$,
podemos escribir 
\begin{align*}
Pr\text{\ensuremath{\left(X\notin B\left(0,r-\epsilon,d\right)\right)}} & =Pr\text{\ensuremath{\left(X\notin B\left(0,r-\epsilon,d\right),X\in B\left(0,r,d\right)\right)}}\\
1-\left(\frac{r-\epsilon}{r}\right)^{d} & =Pr\left(\left(r-\epsilon\right)<\norm X\leq r\right)
\end{align*}

De manera que $\lim_{d\rightarrow\infty}Pr\left(\left(r-\epsilon\right)\sqrt{d}<\norm X\leq r\sqrt{d}\right)\rightarrow1$.
Es decir, a medida que $d\rightarrow\infty$ y para $\epsilon$ arbitrariamente
pequeño, la distancia euclídea de cualquier observación al centro
de la esfera tiende a $r$. En altas dimensiones, la distancia euclídea
resulta inútil para diferenciar entre elementos muestrales.
\end{rem}


\subsection{La hipótesis de la variedad}

El resultado previo descansa sobre el hecho de que la distribución
de $X$ sobre su soporte $\text{sop}\left(X\right)=B\left(0,r,d\right)\subset\R^{d}$
es uniforme, e independiente en todas las dimensiones. En casi cualquier
contexto material, este supuesto no es sostenible. Por poner un ejemplo,
podemos representar todas las posibles imágenes en escala de grises
de 1 megapíxel como puntos $X$ pertenecientes al espacio $\R^{1024\times1024}$,
pero si tomamos una imagen la basta mayoría de ellas consistirían
en ``puro ruido blanco'' y no significarían nada para un observador.
Las imágenes que sí tiene sentido reconocer y clasificar (un gato,
una bicicleta, etc.) son un conjunto muchísimo más restringido - aún
teniendo en cuenta todo tipo de posiciones y contrastes posibles -,
y sus diferentes elementos (como la posición de los ojos y las orejas
del gato) guardan relaciones específicas entre sí. Es decir, están
\emph{correlacionados}\footnote{Un desarrollo contemporáneo sumamente interesante es el de \cite{linShellTheoryStatistical2021},
que se puede traducir como \emph{Teoría de los <<caparazones>>:
Un modelo estadístico de la realidad.} Los autores observan que en
teoría, debido a la \nameref{rem:curse-dim}, el aprendizaje estadístico
debería ser lisa y llanamente imposible en altas dimensiones, pero
en la práctica se ve que funciona. Propone un marco estadístico riguroso
destinado a concebir el aprendizaje automático en alta dimensión,
la <<teoría de los caparazones>> - aunque en inglés suena más bonito,
\emph{shell theory} . Fundado en la observación de que las relaciones
entre objetos que deseamos entender forman una jerarquía (gato siamés
$\subset$ gato $\subset$ animal), propone que las observaciones
en alta dimensión son resultado de un proceso de ``generadores jerárquicos''.
Desarrollando una noción de distancia adecuada, muestran que en dichos
procesos generativos, las instancias de cada proceso en la jerarquía
- casi siempre - están encapsuladas por un <<caparazón>> distintivo
que excluye a (casi) cualquier otra instancia, y permite identificar
clases rigurosamente.}

Si nos suponemos en esta situación, el camino más directo para aliviarla,
es \emph{reducir la dimensionalidad} del problema. Al fin y al cabo,
es el crecimiento en $d$ lo que nos embrolló en un principio. Dadas
$\left\{ \mathbf{X}\right\} =\left\{ X_{i}\vert X_{i}\in\Rdimx,\ i\in\text{\ensuremath{\left[N\right]}}\right\} $,
buscaremos una \emph{representación} $f:\Rdimx\rightarrow\R^{d_{y}}$,
que preserve fielmente los atributos más relevantes de $x\in\Rdimx$,
en la menor cantidad de dimensiones $d_{y}$. Encontrar compromisos
ideales entre la ``fidelidad'' y la dimensionalidad de estas representaciones,
dió lugar al campo de \emph{aprendizaje de representaciones, }del
cual\cite{bengioRepresentationLearningReview2014} hace un excelente
censo. El autor relaciona la tarea del área con la noción geométrica
de una \emph{variedad,} a través de la \emph{hipótesis de la variedad}.\footnote{El término no es del todo riguroso pero figura frecuentemente en la
literatura sobre aprendizaje automático. El mismo Bengio se explaya
sobre el origen del término \href{https://www.reddit.com/r/MachineLearning/comments/mzjshl/comment/gwq8szw/?utm_source=share&utm_medium=web2x&context=3}{en Reddit},
y \cite{rifaiManifoldTangentClassifier2011} distingue entre varias
formulaciones íntimamente relacionadas de la misma hipótesis.}. 
\begin{rem}
A nuestros fines, una variedad $\mathcal{\M}$ de dimensión $d_{\M}$,
es un espacio que \emph{localmente}, se asemeja a $\R^{d_{\M}}$.
En efecto, una variedad puede ser vista como un objeto compuesto de
parches $d_{\M}$-dimensionales pegados. Una variedad se llama \emph{cerrada}
si no tiene borde y es compacta.
\end{rem}

La \emph{hipótesis de la variedad} (``manifold hypothesis'') postula
que los datos \textbf{$x$} obtenidos del mundo real con alta dimensionalidad
$d_{x}$ habrían de concentrarse en una variedad $\M$ de -potencialmente
- mucha menor dimensionalidad $d_{\M}\ll d_{x}$, embebido en el espacio
original $\R^{d_{x}}$. Esta asunción parece particularmente adecuada
en tareas de aprendizaje para las cuales las configuraciones muestreadas
aleatoriamente no son como las que ocurren naturalmente: ya mencionamos
imágenes, pero esperamos lo mismo de cualquier tipo de observaciones
multivariadas obtenidas <<naturalmente>>: sonidos, texto, secuencias
genómicas y hasta las respuestas al eterno cuestionario del censo.
Siguiendo a \cite{bengioRepresentationLearningReview2014},
\begin{quotation}
Ni bien tenemos una \emph{representación,} uno piensa en una variedad
considerando las variaciones en el dominio original que están bien
capturadas o reflejadas (por correspondientes cambios) en la representación
aprendida. A \emph{grosso modo}, algunas direcciones estarán bien
preservadas (las direcciones \emph{localmente tangentes} a cada punto
en la variedad), mientras que otras se perderán - las ortogonales
a $\M$ . Desde esta perspectiva, la principal tarea del aprendizaje
no-supervisado, puede ser vista como el modelado de la estructura
de la variedad que soporta los datos observados. La representación
que se aprenda, puede asociarse a un sistema intrínseco de coordenadas
en la variedad embebida. El algoritmo arquetípico de modelado de variedades
es, oh sorpresa, también el algoritmo arquetípico de aprendizaje de
representaciones de baja dimensionalidad: Análisis de Componenetes
Principales (PCA).

PCA modela una \emph{variedad lineal. }Fue inicialmente diseñado con
el objetivo de encontrar la variedad lineal más cercana a una nube
de puntos. Las componentes principales, i.e., la representación $f_{\theta}\left(x\right)$
que devuelve PCA para un input $x$, ubica unívocamente su proyección
en esa variedad: se corresponde con coordenadas intrínsecas de la
variedad. Las variedades que soportan dominios complejos del mundo
real, sin embargo, se espera que sean fuertemente no-lineales.
\end{quotation}
%
Más que una propiamente dicha hipótesis falsificable al respecto de
la distribución de los datos, mencionamos la \emph{hipótesis de la
variedad} como un modelo mental útil para entender cómo estimar la
densidad generadora de los datos en altas dimensiones. Ya mencionamos
que a medida que $d_{x}$ crece, la distancia euclídea en $\R^{\dimx}$
se vuelve menos informativa. Trabajar dentro de $\M$, con dimensión
$d_{\M}$ puede aliviar la situación sobre todo cuando $d_{\M}\ll d_{x}$,
pero hay una ventaja más escondida en el hecho de que una variedad
es sólo localmente semejante al espacio euclídeo - es decir, \emph{lineal}
-, pero puede ``arrugarse'' en el espacio ambiente.

Imaginemos un conjunto de datos $\left\{ \mathbf{u}\right\} =\left\{ u_{i},i\in\left[N\right],u_{i}\in\mathcal{U}\subseteq\R^{2}\right\} $,
con forma de letra ``U'', justamente. $\mathcal{U}$ es una variedad
1-dimensional - un segmento curvo - embebida en el espacio cartesiano
- $\R^{2},$ una variedad 2-dimensional. En la variedad latente, los
dos puntos extremos del dibujo de la ``U'' están tan separados entre
sí como es posible; sin embargo, si medimos la distancia entre ambos
en el espacio ambiente - $\R^{2}$ - obtendremos que están mucho más
cerca entre sí. La razón de tal insensatez, es simplemente, que hemos
tomado una medida de distancia que no se ajusta bien al espacio latente.

\subsection{KDE en variedades\label{subsec:KDE-en-variedades}}

¡Excelente! Fieles a la hipótesis de la variedad, podemos sugerir
un camino alternativo a los complejos derroteros por los que nos llevó
de paseo KDE multivariado en alta dimensión: en lugar de calcular
un KDE en el espacio ambiente $\R^{d_{x}},$ hipotetizamos que $X\in\M\subseteq\R^{d_{x}},\dim\M=d_{\M}\ll d_{x}$,
y por lo tanto podemos restringir la definición de su densidad $f:\M\rightarrow\left(0,\infty\right)$
para obtener una mejor representación. Pero: ¿cómo se construye una
función de densidad \emph{en una variedad}? Algunas variedades particularmente
interesantes, como en la circunferencia $S^{1}$ y la esfera $S^{2}$,
fueron estudiadas temprano en el siglo XX \footnote{Motivado por el estudio de los pesos atómicos de elementos químicos,
\cite{vonmisesUberGanzzahligkeitAtomgewicht1918} (en alemán) introduce
la <<distribución von Mises>> en la circunferencia $S^{1}$, adaptando
la densidad normal estándar univariada. \cite{fisherDispersionSphere1997}
propone la <<distribución de Fisher>> en la esfera $S^{2}$para
desarrollar un test sobre la dirección de flujos de lava en Islandia.
A mediados del siglo XX el campo de la <<estadística direccional>>
- un antecesor directo de la estadística en variedades arbitrarias
- estaba bien desarrolada, y \cite{mardiaDistributionTheoryMisesFisher1975}
estudia en detalle la <<distribución de von Mises-Fisher>>, generalización
$d-$dimensional de los aportes antedichos.}, pero la estimación de densidad en variedades arbitrarias no parece
haber sido tratado sistemáticamente antes de \cite{pelletierKernelDensityEstimation2005},
quien convenientemente hizo exactamente eso: proponer un estimador
de densidad por núcleos en variedades Riemannianas. En lo que sigue,
intentamos ser fieles a lo que entendimos de la exposición de Bruno,
y suplimos algunos agujeros teóricos con la más amena tesis de licenciatura
de \cite{munozEstimacionNoParametrica2011}. 
\begin{defn}
[variedad Riemanniana](\cite[§3.3.2]{munozEstimacionNoParametrica2011})\emph{.
}Una variedad Riemanniana es una variedad diferenciable $\M$ dotada
de un métrica Riemanniana $g$, que denotaremos con $\left(\M,g\right)$\footnote{Para quienes no entienden casi absolutamente nada de geometría diferencial
como yo, la tesis de \cite{munozEstimacionNoParametrica2011} es un
excelente puente a los trabajos que se citan en las siguientes secciones,
en especial §3 de íbid., <<Preliminares Geométricos>>.}.
\end{defn}

\begin{defn}
[KDE en variedades]\label{def:manif-kde}(\cite[§2]{pelletierKernelDensityEstimation2005})
Sea $\left(\M,g\right)$ una variedad Riemanniana compacta sin frontera
de dimensión $d$. Asumiremos que $\left(\M,g\right)$ es completo,
es decir, $\left(\M,d_{g}\right)$ es un espacio métrico completo,
donde $d_{g}$ denota la distancia de Riemann.

Sea X un elemento aleatorio en $\M$ \footnote{Más precisamente, $X:\Omega\rightarrow\M$ es un mapa medible en un
espacio de probabilidad $\left(\Omega,\mathcal{A},P\right)$ que toma
valores en $\left(\M,\mathcal{B}\right),$donde $\mathcal{B}$ representa
el $\sigma$-campo de Borel de $\M$. Asumiremos que la medida imagen
de $P$ por $X$ es absolutamente continua con respecto a la medida
Riemanniana de volumen - que notaremos $v_{g}$ -, admitiendo una
densidad $f$ continua en c.t.p. sobre $\M$.} con densidad $f$ continua en casi todo punto. Sea $\text{\ensuremath{\left\{  \mathbf{X}\right\} } }$
un conjunto de $N$ elementos aleatorios i.i.d. a X. Sea $K:\R_{+}\rightarrow\R$
un mapa no-negativo tal que
\begin{enumerate}
\item $\int_{\R^{d}}K\text{\ensuremath{\left(\norm x\right)}d\ensuremath{\lambda\left(x\right)}}=1$
($K$ es una función de densidad)
\item $\int_{\R^{d}}xK\text{\ensuremath{\left(\norm x\right)}d\ensuremath{\lambda\left(x\right)}}=0$
($EX=0$, $K$ es simétrica)
\item $\int_{\R^{d}}\norm x^{2}K\text{\ensuremath{\left(\norm x\right)}d\ensuremath{\lambda\left(x\right)}}<\infty$
($VarX<\infty)$
\item $\text{sop}K=\text{\ensuremath{\left[0,1\right]}}$
\item $\sup K\text{\ensuremath{\left(x\right)}}=K\left(0\right)$
\end{enumerate}
donde $\lambda$ es la medida de Lebesgue en $R^{d}.$ Luego, el mapa
$\R^{d}\ni x\rightarrow K\left(\norm x\right)\in\R$ es un núcleo
isotrópico\footnote{iso-tropos: igual (iso) en toda dirección (tropos). El núcleo gaussiano
estándar es isotrópico.} en $\R^{d}$ con soporte en la bola unitaria.

Sean $p,q$ dos puntos de $\M$. Sea $\theta_{p}\left(q\right)$ la
\emph{función de densidad volumétrica} en $\M$\footnote{Besse 1978 (p. 154) lo define aproximadamente como
\[
\theta_{p}:q\rightarrow\theta_{p}\left(q\right)=\frac{\mu_{\exp_{p}^{*}g}}{\mu_{g_{p}}}\left(\exp_{p}^{-1}\left(q\right)\right)
\]

i.e., el cociente entre la medida canónica de la métrica Riemanniana
$\exp_{p}^{*}g$ en espacio tangente $T_{p}\left(M\right)$, y la
medida de Lebesgue en la estructura euclídea $g_{p}$en $T_{p}\left(M\right).$
La función de densidad volumétrica está ciertamente definida para
$q$ en un vecindario de $p$. En términos de coordenadas normales
geod´sicas en $p$, $\theta_{p}\left(q\right)$ es igual al determinante
de la métrica $g$ expresado dichas estas coordenadas en $\exp_{P}^{-1}\left(q\right)$.}. Definimos el estimador de densidad de $\hat{f}\text{\ensuremath{\left(p|N,K\right)}}$
como el mapa $\hat{f}:\M\rightarrow\R$ que a cada $p\in\M$ le asocia
el valor $\hat{f}\left(p\right)$ definido como
\[
\hat{f}\left(p\right)=N^{-1}\sum_{i=1}^{N}\frac{1}{h^{d}}\frac{1}{\theta_{X_{i}}\left(p\right)}K\left(\frac{d_{g}\left(p,X_{i}\right)}{h}\right)
\]
\end{defn}

\begin{rem}
(concordancia con espacios euclídeos) Sea $\M=\R^{d}$ con su típica
métrica euclídea. Luego, $\theta_{p}\left(q\right)=1\ \forall\ p,q\in\M$
y $f_{N,K}$ se puede escribir como $f_{N,K}=N^{-1}\sum_{i=1}^{N}h^{-d}K\left(\norm{p-X_{i}}/h\right)$.
La expresión de $f_{N,K}$ es consistente con la expresión de \ref{def:kde-multiv}
como producto de $d$ \nameref{def:kde-univ} de idéntico ancho de
banda $h$ . Sea $\M=\R^{d}$ y $d_{M}\left(p,q|\Sigma\right)$ la
\nameref{rem:mahalanobis-dist} con covarianza $\Sigma$. Como $\norm{\Sigma^{-1/2}\left(p-q\right)}=d_{M}\left(p,q|\Sigma\right)$
y $\Sigma$ es una transformación lineal, $\forall p,q\in\R^{d},\ \theta_{p}\text{\ensuremath{\left(q\right)}}=\det\text{\ensuremath{\Sigma^{1/2}=}}\left(\det\Sigma\right)^{1/2}$
y 
\begin{align*}
\widehat{f}(x|\Sigma) & =N^{-1}\sum_{i=1}^{N}\frac{1}{h^{d}}\frac{1}{\theta_{X_{i}}\left(p\right)}K\left(\frac{d_{g}\left(p,X_{i}\right)}{h}\right)\\
 & ={\displaystyle N^{-1}\sum_{i=1}^{N}\frac{1}{h^{d}}\frac{1}{\left(\det\Sigma\right)^{-1/2}}K\left(\frac{d_{M}\left(p,X|\Sigma\right)}{h}\right)}\\
 & ={\displaystyle N^{-1}\sum_{i=1}^{N}\frac{1}{h^{d}}\frac{1}{\left(\det\Sigma\right)^{-1/2}}K\left(\frac{\norm{\Sigma^{-1/2}\left(x-X_{i}\right)}}{h}\right)}\\
 & =N^{-1}\sum_{i=1}^{N}\frac{1}{h^{d}}K_{\Sigma}\left(\frac{\norm{x-X_{i}}}{h}\right)
\end{align*}

y el estimador en variedades es consistente con el caso general del
\nameref{def:kde-multiv}.
\end{rem}

Para asegurarse de que $f_{N,K}$ sea integrable sobre $\M$, habremos
de imponer una restricción más sobre el ancho de banda: \label{constr:inj-radius}
\[
h_{n}<h_{0}<\text{inj}_{g}\M
\]
, donde $\text{inj}_{g}\M$ es el \emph{radio de inyectividad} de
$\M$\footnote{\begin{defn}
\cite[p. 108]{chavelRiemannianGeometryModern2006}\cite[p. 23]{munozEstimacionNoParametrica2011}
Sea $(\M,g)$ una variedad Riemanniana de dimensión $d$. Llamamos
\emph{\small{}radio de inyectividad} a 
\[
\text{inj}_{g}\M=\inf_{p\in\M}\sup\left\{ s\in\R>0:B\left(p,s\right)\t{es\ una\ bola\ normal}\right\} 
\]
\end{defn}

Burdamente, diremos que $B$ es una \emph{\small{}bola normal} centrada
en \emph{p} si existe una bola $V$ en $T_{p}(\M)$ (un vecindario
de $p$) en el que las coordenadas de cada punto $q\in V$ se pueden
mapear biyectivamente a coordenadas en $\R^{d}$: por ejemplo, si
$\M_{1}=\R^{d}$ con la métrica canónica ($\norm{\cdot}$) entonces
$\t{inj}_{g}\M_{1}=\infty$, pues todo el espacio comparte un único
mapa de coordenadas global. Si le quitamos un punto, $\M_{2}=\M_{1}-\left\{ p\right\} $
entonces $\t{inj}_{g}\M_{2}=0$ (para un punto ``muy cercano a $p$'',
$q\in\M,q\approx p$, no habrá bola normal posible) . Si $\M=S^{1}\times\R$
(un cilindro vacío en $\R^{3}$)  con la métrica inducida de $\R^{3}$,
el radio de inyectividad es $\pi$. Lo ventajoso de que $h$ esté
por debajo del radio de inyectividad, será que al integrar sobre las
bolas que soportan el núcleo $K$ alrededor de cada $p\in\M$, la
densidad se podrá integrar - luego de una transformación - en $\R^{d},$
donde nuestras herramientas tradicionales han sido afinadas.}. Sin entrar en demasiados detalles, siempre y cuando $\M$ sea compacta
este radio de inyectividad será $>0$, y al menos para los resultados
asintóticos (cuando el tamaño muestral es lo suficientemente grande
como para que $h\rightarrow0$), siempre existe un $0<h<h_{0}$ posible.

Pelletier avanza algunas propiedades elementales de este estimador:
adapta el concepto de <<media>> para elementos aleatorios en $\R^{d}$
a el de <<media intrínseca>> en variedades Riemannianas compactas
sin frontera \cite[Prop. II]{pelletierKernelDensityEstimation2005}),
y prueba que es consistente para $f$ en el siguiente sentido
\begin{thm}
\cite[Teorema 5]{pelletierKernelDensityEstimation2005}\emph{ Sea
$f$ una densidad de probabilidad dos veces diferenciable en $\M$
con segunda derivada covariante acotada. Sea $\hat{f}$ su estimador
definido en \ref{def:manif-kde} con ancho de banda $h$ que satisface
la condición \ref{constr:inj-radius}. Luego, existe una constante
$C_{f}$ tal que }

\[
E_{f}\norm{\hat{f}-f}_{L^{2}\left(\M\right)}^{2}\leq C_{f}\left(\frac{1}{Nh^{d}}+h^{4}\right)
\]

\emph{En consecuencia, para $h\sim N^{\frac{-1}{d+4}}$, }
\[
E_{f}\norm{\hat{f}-f}_{L^{2}\left(\M\right)}^{2}\leq\text{O}\left(N^{\frac{-4}{d+4}}\right)
\]
\end{thm}

\textcolor{red}{Pelletier considera la convergencia en $L^{2}\left(\M\right),$
¿esto qué tipo de consistencia sería? ¿débil? ¿Qué diferencia hay
con la que estudian Henry\&Rodríguez2009?}

\cite{henryKernelDensityEstimation2009} continúan el estudio de este
estimador, probando
\begin{enumerate}
\item bajo ciertas condiciones de regularidad sobre conjuntos compactos
$\M_{0}\subseteq\M$ - la consistencia fuerte
\[
\sup_{p\in\M_{0}}\vert f_{n,K}\left(p\right)-f\left(p\right)\vert\xrightarrow{c.t.p.}0
\]
\item bajo condiciones extras sobre $f$ y la serie $h_{n}$, $f-f_{N,K}$
converge en distribución a ciert ley normal, con tasa $\sqrt{nh^{d}}$
\[
\sqrt{nh^{d}}\left(f\left(p\right)-f_{n,K}\left(p\right)\right)\xrightarrow{\mathcal{D}}\mathcal{N}\left(\mu,\Sigma\right)
\]
\end{enumerate}
%
\cite{loubesKernelbasedClassifierRiemannian2008} se apalancan sobre
los resultados previos proponiendo un clasificador binario ($M=2$)
basado en núcleos, para e.a. soportados sobre variedades compactas
y cerradas de Riemann. Recordemos que en \ref{def:soft-clf-kde} propusimos
un clasificador suave que asignase a cada clase, una probabilidad
de pertenencia 
\[
p\left(C\left(x\right)=j\right)=\frac{f^{(j)}\left(x\right)\cdot p\text{\ensuremath{\left(C_{j}\right)}}}{p\text{\ensuremath{\left(x\right)}}}
\]

de manera que podemos describir una reglas de clasificación dura,
como
\begin{align*}
\mathcal{R}\left(x\vert f_{1},\dots,f_{K}\right) & =\arg\max_{j\in\left[K\right]}p\left(C\left(x\right)=j\right)\\
 & =\arg\max_{j\in\left[K\right]}\frac{f_{j}\left(x\right)\cdot p\text{\ensuremath{\left(C_{j}\right)}}}{\sum_{j\in\left[K\right]}f_{j}\left(x\right)\cdot p\text{\ensuremath{\left(C_{j}\right)}}}\\
 & \arg\max_{j\in\left[K\right]}f_{j}\left(x\right)\cdot p\text{\ensuremath{\left(C_{j}\right)}}
\end{align*}

y su estimador muestral\label{def:manif-clf-kde}
\begin{align*}
\mathcal{\hat{R}}\left(x\vert\hat{f}_{1},\dots,\hat{f}_{K}\right) & =\arg\max_{j\in\left[K\right]}\hat{f}_{j}\left(x\right)\cdot\hat{p}\left(C_{j}\right)\\
 & =\arg\max_{j\in\left[K\right]}N_{j}^{-1}\sum_{i=1}^{N}\frac{1}{h^{d}}\frac{1}{\theta_{X_{i}}\left(p\right)}K\left(\frac{d_{g}\left(p,X_{i}\right)}{h}\right)\cdot\frac{N_{j}}{N}\\
 & =\arg\max_{j\in\left[K\right]}\sum_{i=1}^{N}\mathbf{1}\left\{ C\left(X_{i}\right)=j\right\} K_{h}\left(p,X_{i}\right)
\end{align*}

donde 
\[
K_{h}\left(p,X_{i}\right)=\frac{1}{h^{d}}\frac{1}{\theta_{X_{i}}\left(p\right)}K\left(\frac{d_{g}\left(p,X_{i}\right)}{h}\right)
\]

Este es, precisamente, el clasificador que Loubes y Pelletier proponen,
adaptado para $M$ clases. Considerando como función objetivo a minimizar
la misma probabilidad de error de clasificación que vimos con \cite{hallBandwidthChoiceNonparametric2005},
\[
L\left(\mathcal{R}\right)=Pr\left(\mathcal{R}\left(X\right)\ne C\left(X\right)\right)
\]

los autores muestran que el clasificador propuesto $\mathcal{\hat{R}}$
alcanza asintóticamente la misma pérdida que el clasificador óptimo
de bayes, $\mathcal{R}^{*}$
\[
\lim_{n\rightarrow\infty}Pr\left(\hat{L\left(\mathcal{R}_{n}\right)}=L\left(\mathcal{R^{*}}\right)\right)=1
\]
con 
\[
\text{\ensuremath{\calR}}^{*}\left(x\right)=\arg\max_{j\in\left[K\right]}Pr\left(C\left(X\right)=j\vert X=x\right)
\]

Siguiendo a \cite[§6 Consistencia]{devroyeProbabilisticTheoryPattern1996},
diremos que el clasificador es \emph{fuertemente consistente}, en
tanto alcanza el error de Bayes cuando $n\rightarrow\infty$. Los
autores dejan las consideraciones prácticas de su funcionamiento fuera
del trabajo.

\subsection{Variedades desconocidas}

Los resultados combinados de \nameref{subsec:KDE-en-variedades} nos
dejan bastante cerca de lo que venimos buscando - construir un clasificador
basado en densidades -, con una diferencia fatal: estos trabajos consideran
variedades \emph{conocidas}, mientras que nosotros trabajamos bajo
la \emph{hipótesis de la variedad}, pero en principio no conocemos
la variedad en sí. Crucialmente, desconocer la variedad $\M=\t{sop}X$
implica desconocer:
\begin{itemize}
\item su dimensión intrínseca $d_{\M}$,
\item la distancia geodésica $d_{g}$,
\item y la función de densidad de volumen $\theta_{p}$,
\end{itemize}
aún \emph{antes }de estimar la densidad $f:\M\rightarrow\R^{+}$,
que nos trajo hasta aquí. Por partes o juntas, tendremos que \emph{aprenderlas
de los datos} de alguna manera.

\subsubsection{La distancia geodésica $d_{g}$}

Dada la naturaleza localmente euclídea de las variedades, para puntos
``vecinos'' entre sí, la distancia en $\R^{d_{x}}$ (en el espacio
<<ambiente>> en que está <<embebida>> $\M$) será una aproximación
razonable a la distancia geodésica en la variedad. Para puntos alejados
entre sí, podemos aproximar $d_{g}$ como la suma de una secuencia
de ``pequeños saltos'' entre puntos vecinos en el grafo de la muestra.

Esta inocente observación es el núcleo de la innovación de Isomap\footnote{\textbf{Iso}metric feature \textbf{map}ping, en inglés},
algoritmo presentado en \cite{tenenbaumMappingManifoldPerceptual1997,tenenbaumGlobalGeometricFramework2000}
con el objetivo de ``aprender la geometría global subyacente de un
dataset, usando información métrica local fácilmente medible'', de
entre un conjunto amplio de variedades no-lineales. Su tarea central,
consiste en aproximar adecuadamente las distancias geodésicas en la
variedad $d_{g}\left(p,q\right)$ entre puntos alejados, conociendo
únicamente las distancias euclídeas en la muestra $\norm{p-q}$.

El algoritmo completo, consta de tres pasos principales \cite[Tabla 1]{tenenbaumGlobalGeometricFramework2000}:
\begin{enumerate}
\item \textbf{Constrúyase un grafo de vecinos muestrales} $\mathbf{NN}=\text{\ensuremath{\left(\text{\ensuremath{\X}},E\right)}}$
sobre el dataset completo, donde la arista $x\leftrightarrow y$ está
incluida si $\norm{x-y}_{d_{x}}<\epsilon$ (``$\epsilon-$Isomap''),
o si $y$ es uno de los $K$ vecinos más cercanos de $x$ (<<$K-$isomap>>)
. Tómese $\norm{x-y}$ como el valor de la arista $x\leftrightarrow y$.
\item \textbf{Compútense los caminos mínimos}, usando - según convenga -
el algoritmo de \href{https://es.wikipedia.org/wiki/Algoritmo_de_Floyd-Warshall}{Floyd-Warshall}
o \href{https://es.wikipedia.org/wiki/Algoritmo_de_Dijkstra}{Dijsktra}
en $G$. Los costos de los caminos mínimos $d_{\mathbf{NN}}\left(x,y\right)$
constituyen una aproximación de las distancias geodésicas $d_{g}\left(x,y\right)$.
\item \textbf{Constrúyase un }\textbf{\emph{embedding}}\textbf{ $\mathbf{d-}$dimensional.
}Utilizando \href{https://es.wikipedia.org/wiki/Escalamiento_multidimensional}{escalamiento multidimensional}\footnote{MDS, \textbf{M}ulti\textbf{d}imensional \textbf{S}caling},
un algoritmo de reducción de dimensionalidad), crear una representación
(<<embedding>>) en el espacio euclídeo $\R^{d}$ que minimice una
métrica de discrepancia denominada <<estrés>>, entre las distancias
$d_{\mathbf{NN}}$ antes computadas con las distancias en la representación
a construir $\norm{\cdot-\cdot}_{\R^{d}}$.
\end{enumerate}
Los resultados de este algoritmo - que han sido bastante espectaculares
para lo relativamente sencillo de su estructura, descansan en una
prueba de la convergencia asintótica, a medida que $N$ crece, de
que las distancias en el grafo $d_{G}$ proveen aproximaciones incrementalmente
mejores a las distancias geodésicas intrínsecas $d_{\M}$, volviéndose
arbitrariamente precisas en el límite de $N\rightarrow\infty$. La
tasa a la que esta convergencia sucede, depende de ciertos parámetros
de la variedad (su dimensión $d_{\M}$, la función de volumen $\theta_{p}$),
de cómo esta yace en el espacio ambiente (radio de curvatura $r_{0}$
y separación de ramas$s_{0}$) y de la densidad $f$ de la que estamos
sampleando.

Allende los costos computacionales, hay dos parámetros a fijar en
este algoritmo. El primero es el parámetro de ``vecindad'' $\epsilon$
ó $K$ de (1), cuyo valor óptimo no es trivial determinar. Consideremos
$\epsilon-$Isomap: valores demasiado pequeños de $\epsilon$ podrían
dejar muchos vértices de $G$ - muchas observaciones muestrales -
desconectadas de la componente gigante - la componente conexa de mayor
tamaño - de $G;$ valores demasiado grandes de $\epsilon$ podrían
<<cortocircuitar>> la representación - incluir en $G$ aristas $e\in E$
que cruzan el espacio ambiente $\R^{d_{x}}$ completamente por fuera
de $\M$. Consideraciones análogas complican la elección de cantidad
de vecinos en $K-$Isomap.

El otro parámetro de interés es la dimensión $d$ del embedding euclídeo
<<óptimo>>. Inspeccionando el gráfico de ``estrés'' de MDS como
función de la dimensión $d$ escogida, se pueden buscar punto(s) de
inflexión (``codos'') en que seguir aumentando $d$ no aliviana
significativamente la tensión del algoritmo, y son por tanto candidatos
naturales a la dimensión intrínseca de la variedad $d_{\M}$. Al menos
en ejemplos sintéticos, al método del codo lo heurístico no le quita
lo certero. La representación $d-$dimensional que produce MDS no
es la variedad $\M$ que buscamos, pero sí es razonable que con suficientes
datos, $d\approx d_{\M}$.

\subsubsection{La dimensión intrínseca $d_{\protect\M}$}

La literatura que intenta estimar directamente $d_{\M}$, compensa
su escasez con creatividad. \cite{brandChartingManifold2002} se propone
no sólo estimar $d_{\M}$, sino además ofrecer un algoritmo para proveer
un <<atlas>> \footnote{El par $\left(U,\varphi\right)$ compuesto por un conjunto abierto
$U$ medible en $\M$, y un homeomorfismo $\varphi:U\rightarrow A\subset\R^{d_{\M}}$
también abierto se denomina <<carta>> (<<chart>>), o <<entorno
coordenado>>. Un conjunto de cartas <<compatibles>> entre sí cuya
unión sea la variedad $\M$ es un <<atlas>>, exactamente como llamamos
en cartografía a un conjunto de mapas - euclídeos en $\R^{2}$- cuya
unión representa la superficie terrestre $S^{2}.$ El trabajo de \cite{brandChartingManifold2002}
es sumamente interesante, aunque queda algo por fuera del ya extenso
paseo bibliográfico. del trabajo de \cite[§3.1 "Variedades Diferenciables"]{munozEstimacionNoParametrica2011}
provee los preliminares necesarios para entenderlo.} de $\M$, una representación harto útil. 

Llamemos $n\text{\ensuremath{\left(r\right)}}$ a la <<función de
conteo>> que indica cuántos puntos de $\X$ se encuentran dentro
de una bola en $\R^{d_{\M}}$centrada en un punto $p\in\M$. $n\left(r\right)$
debería crecer a tasa $r^{d_{\M}}$, pero únicamente en la escala
en la que la variedad es efectivamente localmente lineal. Si hay ruido
en la medición en $\R^{d_{x}},$ en la mínima escala los puntos se
encontrarán en toda dirección y $n\left(r\right)$ crecerá a tasa
$r^{d_{x}}$; en escalas mayores a la localmente lineal, la tasa de
crecimiento de $n\left(r\right)$ también será mayor, pues la variedad
ya no es perpendicular a la superficie de la bola, y la curvatura
hace que $r$ no deba crecer tan rápido para incorporar nuevos puntos.
Un cuidadoso análisis de la tasa de crecimiento de $n\left(r\right)$
permitiría identificar la dimensión más probable de la variedad. Aunque
teóricamente llamativo, el resultado es costoso de computar y no tan
obvio de interpretar para datasets <<naturales>> o sintéticos pero
de pequeño $N$.

\cite{vincentManifoldParzenWindows2002} presenta una estrategia más
directa. El vecindario local de un punto $p\in\M$ debería encontrar
a sus vecinos en un subespacio lineal de dimensión $d_{\M}$, y espacio
ambiente vacío en las demás dimensiones. De computar PCA\footnote{Por <<consideraciones prácticas>>, los autores no implementan PCA
sino la descomposición en valores singulares, SVD, y toman los $d_{\M}$
mayores valores singulares en lugar de los respectivos autovalores.
Por qué han de hacerlo así no me queda del todo claro.} para el vecindario de $p$, esperaríamos encontrar $d_{\M}$ direcciones
principales con autovalores ordenados $\lambda_{1},\dots,\lambda_{d_{\M}}$
significativos, y $\lambda_{i}\approx0,\ i>d_{\M}$. A partir de esta
observación, proponen esencialmente un estimador \nameref{def:kde-multiv}
con una $\mathbf{H}_{i}$ elegida específicamente para cada $X_{i}$
en función de su vecindario <<suave>> o <<duro>>\footnote{Respectivamente, ponderando la contribución a la matriz de covarianza
de cada vecino según un núcleo gaussiano (vecindario <<suave>>),
o calculándola tradicionalmente sobre los K vecinos más cercanos (<<duro>>) }, añadiendo $\sigma^{2}\mathbf{I}$ a las las $d_{\M}$ direcciones
principales (a escala). El regularizador $\sigma^{2}\mathbf{I}$ provee
dos ventajas: evita tener que guardar las $d_{x}$ componenetes principales
para cada punto, y nos asegura que $\H_{i}$ siempre esté bien condicionada,
aún cuando el vecindario tiene sólo $K<d_{x}$ vecinos. Cuando $d_{\M}$
no se conoce de antemano, una heurística como el <<método del codo>>
antedicho o <<tantos autovalores como sean necesarios para explicar
$X\%$ de la varianza>> debería funcionar razonablemente bien.

\subsubsection{La densidad de volumen $\theta_{p}\left(q\right)$ - TBD}

\subsection{Distancias basadas en densidad}

\subsubsection{De Isomap al presente}

Al núcleo de \cite{tenenbaumGlobalGeometricFramework2000,vincentManifoldParzenWindows2002}
y otros, está la idea de considerar el grafo de vecinos más cercanos
\textbf{NN }de $\X$ como aproximación a la estructura de $\M$, y
asumir que en los vecindarios de cada punto la distancia euclídea
aún es representativa. Cuando $\M$ está ralamente muestreado, o tiene
una curvatura considerable, aún este supuesto relativamente benigno
puede resultar fatal.

\cite{vincentDensitySensitiveMetrics2003}\footnote{\cite{bijralSemisupervisedLearningDensity2012} lo cita, pero no me
resultó posible encontrar el PDF del trabajo original. Atención a
la fecha: 2003, hace dos décadas, probablemente por el trabajo de
\cite{tenenbaumGlobalGeometricFramework2000} fresquito en la memoria.} ya sugiere una alternativa heurística en el contexto de clustering:
construir un grafo con aristas pesadas sobre $\X$ con pesos iguales
\emph{al cuadrado} de la distancia (euclídea) entre sus extremos y
tomar como distancia entre vértices el costo de camino mínimo correspondiente.
Esencialmente, lo mismo que Isomap pero con costo $\norm{x-y}^{2}$en
el primer paso. El cuadrado castiga más severamente los saltos entre
puntos alejados, y favorecerá caminos mínimos que pasen por regiones
de alta densidad. El trabajo de \cite{bijralSemisupervisedLearningDensity2012}
ya habla explícitamente de <<distancias basadas en densidad>>\footnote{DBDs, density-based distances.},
definidas a partir de transformaciones $g$ monótonamente decrecientes
en la densidad $f$. Más aún, en el caso de la familia $g\left(f|r\right)=f^{-r}$
que resulta de pesar el grafo según $\norm{x-y}^{q}\t{donde\ }q=rd+1$,
considera su estimación empírica en el grafo completo de vértices
$\X$. La dimensión intrínseca $d$ de la variedad a estimar casi
nunca es conocida de antemano, pero esto no es obstáculo para aplicar
esta familia: podemos elegir - por validación cruzada, por ejemplo
- $q$ directamente, y atrapar en dicho parámetro $r,d$ a la vez.
\begin{rem}
\footnote{\cite[§3]{bijralSemisupervisedLearningDensity2012}}Cuando
la densidad es efectivamente uniforme en la variedad, $f$ es constante
en $\M$ y $g$ también, así que medir la distancia entre puntos según
$\norm{x-y}$es óptimo. Lamentablemente, las densidades que buscamos
estimar nunca son uniformes.
\end{rem}

\begin{rem}
Para Isomap, el parámetro de vecindad es clave, en tanto <<esculpe>>
la estructura local del grafo completo. Al usar una distancia basada
en densidad, tal restricción ya no es necesaria. Se puede elegir un
$k,\epsilon$ pequeño por consideraciones computacionales, pero en
principio las distancias basadas en densidad sólo se benefician al
agrandar los vencindarios a considerar.
\end{rem}

La década de los 2010 fue fructífera para las distancias basadas en
densidad, y hacia fines de ella se dan múltiples resultados sólidos
casi en paralelo. \cite{chuExactComputationManifold2019} prueba una
relación sorprendente entre la distancia de vecinos más cercanos y
la de <<aristas cuadradas>>
\begin{defn}
\cite[Definición 1.1]{chuExactComputationManifold2019} Dada una
función de costo continua $c:\R^{d}\rightarrow\R$ definimos el costo
<<basado en densidad>> de un sendero $\gamma$ relativo a $c$ como
$c\left(\gamma\right)=\int_{0}^{1}c\left(\gamma\left(t\right)\right)\norm{\gamma'\left(t\right)}dt$,
donde el sendero $\gamma$ es un mapa continuo $\gamma:\left[0,1\right]\rightarrow\R^{d}$.
Sea $\t{senderos}\left(p,q\right)$ el conjunto de senderos $C^{1}$
de a tramos\footnote{i.e., con primera derivada continua. Es el conjunto de senderos sobre
los que es factible computar $c\left(\gamma\right)$}. Definimos la DBD entre dos puntos $p,q\in\R^{d}$ como 
\[
d_{c}(p,q)=\inf_{\gamma\in\t{senderos}\left(p,q\right)}c\left(\gamma\right)
\]
\end{defn}

\begin{defn}
\cite[Definición 1.2]{chuExactComputationManifold2019}Sea $Q\subseteq\R^{d}$
un conjunto finito. Definiremos la métrica de vecino más cercano,
$\mathbf{r}_{Q}\left(a\right)=4\min_{q\in Q}\norm{a-q}$ y la distancia
asociada como 
\[
d_{\mathbf{N}}\left(a,b\right)\coloneqq d_{\mathbf{r}_{Q}}\forall\ a,b\in\R^{d}
\]
.
\end{defn}

\begin{defn}
\cite[Definición 1.3]{chuExactComputationManifold2019}Para un conjunto
de puntos $Q\in\R^{d},$la distancia de aristas cuadradas $\forall\ p,q\in Q$
es
\[
d_{\mathbf{2}}\left(a,b\right)=\inf_{\left(q_{0},\dots,q_{k}\right)}\sum_{i=1}^{k}\norm{q_{i}-q_{i-1}}^{2}
\]

donde el ínfimo es sobre secuencia de $k$ puntos con $q_{0}=a$ y
$q_{k}=b$.
\end{defn}

\begin{thm}
\cite[Definición 1.3]{chuExactComputationManifold2019} La métria
de vecino más cercano y la métrica de aristas cuadradas son equivalentes
para cualquier conjunto finito de puntos $Q$ en dimensión arbitraria\footnote{El resultado es aún más fuerte: establece que $d_{\mathbf{N}}\equiv d_{\mathbf{2}}$
para todo $Q$ sea una colección finita de conjuntos compactos conectados
por senderos. Es decir, si reemplazamos los puntos por <<regiones
compactas>> del espacio - que no tenga costo atravesar -, la equivalencia
aguanta.}.
\end{thm}

Contar con una aproximación en un grafo finito para computar una distancia
sobre senderos arbitrarios en el espacio ambiente es un resultado
muy poderoso. Sin embargo, ya \cite[§2]{bijralSemisupervisedLearningDensity2012}
mencionaba que la construcción de un estimador $\hat{f}$ basado en
la métrica de vecino más cercano es insesgado para la \emph{mediana},
pero no es consistente, pues su varianza permanece constante aún cuando
$N\rightarrow\infty$.

\subsubsection{Distancia de Fermat}

El estudio de las distancias correspondientes a las funciones de costo
$g=f^{-r}$, equivalentes continuos a la distancia de camino mínimo
en el grafo pesado por $\norm{x-y}^{q}$, es estudiado por \cite{mckenziePowerWeightedShortest2019,littleBalancingGeometryDensity2021,groismanNonhomogeneousEuclideanFirstpassage2019},
con diferencias de notación y aplicación, pero no sustantivas. \cite[Def. 2.1]{groismanNonhomogeneousEuclideanFirstpassage2019}
considera la generalización $\mathbf{d_{\alpha}}$ de $\mathbf{d_{2}}$,
que llaman <<distancia de Fermat muestral>>\footnote{\cite{mckenziePowerWeightedShortest2019,littleBalancingGeometryDensity2021}
consideran una versión <<normalizada>> de la distancia de Fermat,
$\left(d_{\mathbf{\alpha}}\left(a,b\right)\right)^{1/\alpha}$. Donde
éstos últimos autores consideran una $f$ definida en la unión de
variedades disjuntas y usan la distancia resultante para <<clustering
espectral>>, los primeros consideran una única variedad compacta
y usan la distancia en clustering por $K-$medoides. Para evitar estirar
este de por sí extenso censo del arte, la exposición posterior corresponde
únicamente a \cite{groismanNonhomogeneousEuclideanFirstpassage2019} }
\begin{equation}
d_{Q\mathbf{,\alpha}}\left(a,b\right)=\inf_{\left(q_{0},\dots,q_{K}\right)}\sum_{i=1}^{K}\norm{q_{i}-q_{i-1}}^{\alpha},\ \alpha\geq1\label{eq:sample-fermat-dist}
\end{equation}
 Los autores definen esta distancia muestral para conjuntos arbitrarios
$Q$, pero en general consideraremos $Q=\X$, la muestra $d_{x}$-dimensional
de interés. Nótese que $d_{Q\mathbf{,\alpha}}$ satisface la desigualdad
triangular, y define una métrica sobre $Q$. Cuando no se preste a
confusión, omitiremos la dependencia en $Q,\alpha$. A continuación,
se define la versión \emph{macroscópica} de la distancia de Fermat
muestral.
\begin{defn}
[distancia de Fermat]\cite[Definicion 2.2]{groismanNonhomogeneousEuclideanFirstpassage2019}
Sea $\M$ una variedad de Riemann, $f:\M\rightarrow\R_{+}$ una función
continua y positiva en $\M$, $\beta\geq0$ y $s,t\in\M$. Definimos
la \emph{distancia de Fermat macroscópica}\footnote{O \emph{distancia de Fermat}, a secas.}\emph{
$\mathcal{D}_{f,\beta}\left(s,t\right)$ }como
\[
\mathcal{T}_{f,\beta}\left(\gamma\right)=\int_{\gamma}f^{-\beta},\quad\mathcal{D}_{f,\beta}\left(s,t\right)=\inf_{\gamma\in\Gamma}\mathcal{T}_{f,\beta}\left(\gamma\right)
\]

donde el ínfimo esta tomado sobre el conjunto de todos los caminos
continuos y rectificables contenidos en $\bar{\M}$ (la clausura de
$\M$) que comienzan en $s$ y terminan en $t$, y la integral es
respecto de la longitud de arco dada por la distancia euclídea. Se
omitirán las dependencias de $f,\beta$ cuando no haya confusión posible. 
\end{defn}

Uniendo las dos definiciones previas, el teorema central del trabajo
es el siguiente:
\begin{thm}
\emph{\cite[Teorema 2.7]{groismanNonhomogeneousEuclideanFirstpassage2019}
Sea $\M$ una variedad $d$-dimensional, isométrica y $C^{1}$ embebida
en $\R^{D}$}\footnote{Es decir, existe un conjunto abierto y conexo $S\subset\R^{d}$ y
$\phi:\bar{S}\rightarrow\R^{D}$ una transformación isométrica tal
que $\phi\left(\bar{S}\right)=\M$. En aplicaciones reales se espera
que $d\ll D$, pero no es necesario.}\emph{. Sea $Q_{n}=\left\{ q_{1},\dots,q_{n}\right\} $ puntos independientes
con densidad común $f$. Luego, para $\alpha>1$ y $x,y\in\M$ se
tiene
\[
\lim_{n\rightarrow\infty}n^{\beta}D_{Q_{n},\alpha}\text{\ensuremath{\left(x,y\right)}=\ensuremath{\mu\mathcal{D}_{f,\beta}}\ensuremath{\left(x,y\right)\ }casi seguramente}.
\]
Aquí, $\beta=\left(\alpha-1\right)/d$ y $\mu$ es una constante que
depende únicamente de $\alpha$ y la dimensión de la variedad $d$.}\footnote{Debería unificar la notación de \cite{groismanNonhomogeneousEuclideanFirstpassage2019,chuExactComputationManifold2019,bijralSemisupervisedLearningDensity2012},
creo que la de Chu es la más amena, pero estaría bueno revisarlas.}
\end{thm}

En otras palabras, correctamente escalada, la distancia muestral de
Fermat converge a la distancia ``poblacional'' de Fermat, y $D_{Q_{n},\alpha}$
es un estimador consistente de $\mathcal{D}_{f,\beta}$. Los autores
prueban el caso en que $f$ corresponde a un proceso puntual de Poisson
homogéneo en $\M$, y conjeturan que es cierto para $f$ arbitraria%
\begin{comment}
Es así? O en realidad la prueba es más fuerte?
\end{comment}
.

\section{Propuesta}

Hemos repasado en detalle la historia y motivación por detrás de un
método eficiente y sumamente estudiado para responder al \nameref{def:prob-clf}
en dominios de alta dimensionalidad: la estimación de densidad por
núcleos (KDE), hasta llegar a definirla en variedades de Riemann.
Notamos que de los tres parámetros a elegir - el núcleo, el ancho
de banda y la distancia - tanto el ancho de banda como la distancia
son problemáticos en alta dimensiones. Para KDE, la elección del ancho
de banda el tratamiento encontrado en la literatura es extenso y exhaustivco;
no así para la elección de la distancia. Nos proponemos elucidar si
es posible mejorar la performance de \nameref{def:manif-kde} usando
una noción de distancia basada en densidades de desarrollo reciente,
la distancia muestral de Fermat. Más específicamente, construiremos
\begin{itemize}
\item un \nameref{def:manif-clf-kde} en variedades según \cite{loubesKernelbasedClassifierRiemannian2008}
\item con matriz de suavización \textbf{$\H_{i}$} individualmente orientada
en cada elemento muestral según \cite{vincentManifoldParzenWindows2002}
\item y distancia varietal aprendida según \cite{groismanNonhomogeneousEuclideanFirstpassage2019}
por validación cruzada de $\alpha$
\end{itemize}
%
Evaluaremos al clasificador resultante en un conjunto de datasets
sintéticos y naturales que representen un espectro amplio de casos
de alta dimensionalidad, a través de un estudio de ablación, para
entender cuál es la ventaja marginal de utilizar una distancia aprendida
por sobre el clasificador equivalente con distancia euclídea.

Los métodos de estimación por núcleos, aunque simples en su concepción,
tienen altos requerimientos computacionales, y el aprendizaje de distancias
basadas en grafos, más aún. Por ello, en el estudio ablativo comparado,
incluiremos como referencia de precisión:
\begin{itemize}
\item un clasificador KNN con distancia euclídea - la versión más sencilla
posible de un clasificador KDE, y
\item un clasificador por GBT - gradient boosting trees -, uno de los métodos
más ``plug \& play'' disponibles hoy en día.
\end{itemize}
Incluiremos algunos comentarios sobre el costo computacional de cada
método, comparando la expectativa teórica con los resultados de nuestras
- sencillas y caseras - implementaciones.

Finalmente, nos proponemos dar algunas garantías teóricas sobre el
comportamiento asintótico de la distancia muestral de Fermat como
estimador de la distancia (macroscópica / poblacional) homónima.

\section{Otros papers}

Hay varios papers con ideas muy piolas sobre como aprender una variedad,
y como usar la info (las cartas generadas) para clasificar. Se aleja
de nuestro interes principal, pero tal vez ameriten mención?

\subsubsection{Manifold Tangent Classifier (+TangentProp)}

Incluye un buen detalle de 3 versiones interreleacionadas de la hipotesis
de la variedad.

Usa una NN para encontrar en cada punto, direcciones tangentes en
las que la funcion de activacion no cambia significativamente. Luego,
usa tangentprop (una forma de gradient backpropagatiojn con restricciones
sobre las derivadas primeras) para incluir esa info en la optimizacion
y mejorar los resultados de clasificacion.

\subsubsection{The Curse of Highly Variable Functions for Local Kernel Machines}

Muestra cómo todos los métodos basados en núcleos (KNN,m KDE, hasta
isomap) comparten la necesidad de un tamaño muestral enorme cuando
la función objetivo a aprender tiene muchas variaciones, por depender
de entornos locales a cada observacion para mapear la variedad. Aún
funciones de baja ``complejidad de Kolmogorov'' (paridad, seno)
son muy difíciles de aprender con kernels, y sin info global.

\subsubsection{Learning Eigenfunctions Links Spectral Embedding and Kernel PCA }

Une un monton monton de metodos de estimacion de densidad / embeddings
demtro de un marco unificado de funciones basadas en nucleos. En particular,
Isomap (y landmark-Isomap) se pueden ampliar a puntos out-of-sample
computando la aproximacion a la distancia geodésica en el grafo de
kNN, a traves de los puntos de entrenamiento, basicamente como estamos
por proponer nosotros para extender distancia de fermat a out-of-sample.
Duro pero interesante.

\subsubsection{Chu2018 - Exploration of a Graph-based Density-Sensitive Metric}

\emph{We consider a simple graph-based metric on points in Euclidean
space known as the edge-squared metric. This metric is defined by
squaring the Euclidean distance between points, and taking the shortest
paths on the resulting graph. This metric has been studied before
in wireless networks and machine learning, and has the density- sensitive
property: distances between two points in the same cluster are short,
even if their Euclidean distance is long. This property is desirable
in machine learning.}

\subsubsection{Biijral2012 - Semi-supervised Learning with Density Based Distances}

Denoting the probability density function in R d by f (x), we can
define a path length measure through R d that assigns short lengths
to paths through highly den- sity regions and longer lengths to paths
through low density regions. We can express such a path length measure
as

\[
J_{f}\left(x_{1}\stackrel{\gamma}{\leadsto}x_{2}\right)=\int_{0}^{1}g(f(\gamma(t)))\left\Vert \gamma^{\prime}(t)\right\Vert _{p}dt,
\]

where $\gamma:[0,1]\rightarrow\R$ d is a continuous path from $\gamma(0)=x_{1}$
$\gamma(1)=x_{2}$ and $g:\R^{+}\rightarrow\R$ is monotonically decreasing
(e.g. $g(u)=1/u$). Using Equation 1 as a density-based measure of
path length, we can now define the density based distance (DBD) between
any two points $x_{1},x_{2}\in\R$ d as the density-based length of
a shortest path between the two points 
\[
D_{f}(x_{1},x_{2})=\inf_{\gamma}J_{f}(x_{1}\stackrel{\gamma}{\leadsto}x_{2})
\]

Alternatively, a simple heuristic was suggested by Vincent and Bengio
(2003) in the context of clustering, and is based on constructing
a weighted graph over the data set, with weights equal to the squared
dis- tances between the endpoints and calculating shortest paths on
this graph.

N.delA.: El paper de Vincent y Bengio que mencionan no está disponible
en internet, sólo aparece citado en otros trabajos: ``\emph{Vincent,
P., \& Bengio, Y. (2003). Density sensitive metrics and kernels. Proceedings
of the Snowbird Workshop.''}, pero todo indica que la formulación
es como la de Groisman2019, con \textbf{$\beta=2$}

Más adelante, considera funciones $g=f^{-r}$ y pareciera llegar a
una formulación idéntica a la de Groisman2019.

\section{Notas sueltas}
\begin{itemize}
\item soft clf chen
\item (¿Es lo mismo $\norm{\cdot}$ que la geodésica en R\textasciicircum d\_x?
Creo que sí)
\item mencion a t-SNE? como esta basada en distancia euclidea, no parece
que vaya a ayudar mucho
\item RKHS - reproducing kernel hilbert spaces -: alguito para entender
a que cuernos ser refieren?
\item biblio: No subirla, pero esconder script ligeramente disimulado que
la baje por uno?
\end{itemize}

\section{Análisis experimentale}

\section{Cuentita}

\section{Conclusiones}

\bibliographystyle{apalike}
\phantomsection\addcontentsline{toc}{section}{\refname}\bibliography{/Users/gonzalo/Git/fkdc/bib/references-old}

\end{document}
