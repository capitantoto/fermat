Distancia de Fermat en Clasificadores de Densidad por Núcleos

La mayoría de los algoritmos de clasificación asumen que las observaciones yacen sobre un espacio euclídeo: son escasos los métodos que también son válidos cuando el dominio de las variables aleatorias es una variedad arbitraria. En este ámbito, Loubes & Pelletier [1] proponen un clasificador basado en estimación de densidad por núcleos ("KDC") útil en variedades de Riemann conocidas.

Más aún, no siempre es conocida la variedad en que yacen los datos: una imagen de 1 megapíxel tiene 1.000.000 de píxeles, pero típicamente representa un objeto (un dígito, una letra, un animal) que - hipotetizamos - podríamos describir con (muchas) menos dimensiones. En estos contextos, se pueden aprender _distancias basadas en densidad_ (DBDs) que permiten estimar la variedad intrínseca de las observaciones a partir de la misma muestra, como la Distancia (muestral) de Fermat,  investigada por Groisman et al. [2]

En este trabajo, nos proponemos (1) programar el clasificador KDC, (2) extenderlo para utilizar la distancia muestral de Fermat ("F-KDC"), y (3) analizar comparativamente su _performance_ en distintas tareas de clasificación.

[1] J.-M. Loubes y B. Pelletier, «A Kernel-Based Classifier on a Riemannian Manifold», Statistics &
Decisions, vol. 26, n.º 1, pp. 35-51, mar. 2008, doi: 10.1524/stnd.2008.0911.
[2] P. Groisman, M. Jonckheere, y F. Sapienza, «Nonhomogeneous Euclidean First-Passage Percola-
tion and Distance Learning», n.º arXiv:1810.09398. arXiv, diciembre de 2019.