#LyX 2.3 created this file. For more info see http://www.lyx.org/
\lyxformat 544
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass article
\begin_preamble
\usepackage{babel}
\end_preamble
\use_default_options false
\begin_modules
theorems-ams
\end_modules
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding utf8
\fontencoding T1
\font_roman "default" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\use_microtype false
\use_dash_ligatures true
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref false
\papersize default
\use_geometry false
\use_package amsmath 2
\use_package amssymb 2
\use_package cancel 0
\use_package esint 1
\use_package mathdots 0
\use_package mathtools 0
\use_package mhchem 0
\use_package stackrel 0
\use_package stmaryrd 0
\use_package undertilde 0
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 0
\use_minted 0
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\is_math_indent 0
\math_numbering_side default
\quotes_style english
\dynamic_quotes 0
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Standard
\noindent
\begin_inset FormulaMacro
\newcommand{\R}{\mathbb{R}}
\end_inset


\begin_inset FormulaMacro
\newcommand{\dimx}{d_{x}}
\end_inset


\begin_inset FormulaMacro
\newcommand{\Rdimx}{\mathbb{R}^{\dimx}}
\end_inset


\begin_inset FormulaMacro
\newcommand{\Rd}{\Rdimx}
\end_inset


\begin_inset FormulaMacro
\newcommand{\M}{\mathcal{M}}
\end_inset


\begin_inset FormulaMacro
\newcommand{\dimm}{d_{\M}}
\end_inset


\begin_inset FormulaMacro
\newcommand{\var}{\mathcal{\M}}
\end_inset


\begin_inset FormulaMacro
\newcommand{\itR}{\mathcal{\R}}
\end_inset


\begin_inset FormulaMacro
\newcommand{\calR}{\mathcal{R}}
\end_inset


\begin_inset FormulaMacro
\newcommand{\Lj}{\mathcal{L}_{j}}
\end_inset


\begin_inset FormulaMacro
\newcommand{\H}{\mathbf{H}}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\norm}[1]{\left\Vert #1\right\Vert }
\end_inset


\begin_inset FormulaMacro
\newcommand{\t}[1]{\text{#1}}
\end_inset


\end_layout

\begin_layout Title
Distancia de Fermat en Clasificadores de Densidad Nuclear 
\end_layout

\begin_layout Author
Lic.
 Gonzalo Barrera Borla
\end_layout

\begin_layout Date
Buenos Aires, 02/03/23
\end_layout

\begin_layout Standard

\end_layout

\begin_layout Standard
\align center
\begin_inset Graphics
	filename logofac.jpg
	lyxscale 20
	scale 30

\end_inset

 
\end_layout

\begin_layout Standard
\align center
\begin_inset VSpace medskip
\end_inset

 UNIVERSIDAD DE BUENOS AIRES 
\end_layout

\begin_layout Standard
\align center
Facultad de Ciencias Exactas y Naturales 
\end_layout

\begin_layout Standard
\align center
Instituto del Cálculo 
\end_layout

\begin_layout Standard
\align center
\begin_inset VSpace 1cm
\end_inset

 
\end_layout

\begin_layout Standard
\align center
Tesis presentada para optar al título de Magíster en Estadística Matemática
 de la Universidad de Buenos Aires 
\end_layout

\begin_layout Standard
\align center
\begin_inset VSpace 1cm
\end_inset

 
\end_layout

\begin_layout Standard
\align center
Director: Dr.
 Pablo Groisman 
\end_layout

\begin_layout Standard
\begin_inset Newpage pagebreak
\end_inset

 
\end_layout

\begin_layout Abstract
TODO 
\end_layout

\begin_layout Standard
\begin_inset Newpage pagebreak
\end_inset


\end_layout

\begin_layout Standard
\begin_inset CommandInset toc
LatexCommand tableofcontents

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Newpage pagebreak
\end_inset


\end_layout

\begin_layout Section
Introducción
\end_layout

\begin_layout Subsection
El problema de clasificacion
\end_layout

\begin_layout Standard
Consideremos el problema de clasificación: 
\end_layout

\begin_layout Definition
\begin_inset CommandInset label
LatexCommand label
name "def:prob-clf"

\end_inset

(Problema de clasificación).
 Sea 
\begin_inset Formula $\boldsymbol{x}=\left(x_{i}\right)_{i=1}^{N}$
\end_inset

 una muestra de 
\begin_inset Formula $N$
\end_inset

 observaciones, repartidas en 
\begin_inset Formula $M$
\end_inset

 clases 
\begin_inset Formula $C_{1},\dots,C_{M}$
\end_inset

 mutuamente excluyentes y conjntamente exhaustivas (es decir, 
\begin_inset Formula $\forall\ i\in\left[N\right]\equiv\left\{ 1,\dots,N\right\} ,x_{i}\in C_{j}\iff x_{i}\notin C_{k},k\in\text{\ensuremath{\left[M\right]}},k\neq j$
\end_inset

).
 Asumamos además que la muestra está compuesta de observaciones independientes
 entre sí, y en particular, cada clase tiene su propia ley: si 
\begin_inset Formula $\Vert C_{j}\Vert=N_{j}$
\end_inset

 y 
\begin_inset Formula $x_{i}^{\left(j\right)}$
\end_inset

representa la i-ésima observación de la clase 
\begin_inset Formula $j$
\end_inset

, resulta que 
\begin_inset Formula $X_{i}^{(j)}\sim\mathcal{L}_{j}\left(X\right)\ \forall\ j\in\text{\ensuremath{\left[M\right]}},i\in\left[N_{j}\right]$
\end_inset

.
\end_layout

\begin_layout Definition
Dada una nueva observación 
\begin_inset Formula $x_{0}$
\end_inset

 cuya clase es desconocida, 
\end_layout

\begin_deeper
\begin_layout Enumerate
(clasificación dura) ¿a qué clase deberíamos asignarla? 
\end_layout

\begin_layout Enumerate
(clasificación suave) ¿qué probabilidad tiene de pertenecer a cada clase
 
\begin_inset Formula $C_{j},j\in\left[M\right]$
\end_inset

 ? 
\end_layout

\end_deeper
\begin_layout Standard
Todo método o algoritmo que pretenda responder el problema de clasificación,
 prescribe un modo u otro de combinar toda la información muestral disponible,
 ponderando las 
\begin_inset Formula $N$
\end_inset

 observaciones de manera relativa a su cercanía o similitud con 
\begin_inset Formula $x_{0}$
\end_inset

.
 Por caso, 
\begin_inset Formula $k-$
\end_inset

vecinos más cercanos (
\begin_inset Formula $k-$
\end_inset

NN) asignará la nueva observación 
\begin_inset Formula $x_{0}$
\end_inset

 a la clase modal entre las 
\begin_inset Formula $k$
\end_inset

 observaciones de entrenamiento más cercanas
\emph on
 
\emph default
(es decir, que minimizan la distancia euclídea 
\begin_inset Formula $\left\Vert x_{0}-\cdot\right\Vert )$
\end_inset

.
 
\begin_inset Formula $k-$
\end_inset

NN no hace ninguna mención explícita de las leyes de clase 
\begin_inset Formula $\mathcal{L}_{j}$
\end_inset

, lo cual lo mantiene sencillo a costa de ignorar la estructura del problema.
\end_layout

\begin_layout Subsection
Estimación de densidad
\end_layout

\begin_layout Standard
Una familia bastante genérica de métodos para resolver el problema de calsificac
ión, consisten aproximadamente de los siguientes pasos: 
\end_layout

\begin_layout Enumerate
Hacer algunos supuestos sobre la forma de las leyes 
\begin_inset Formula $\Lj$
\end_inset

 
\end_layout

\begin_layout Enumerate
Hallar estimadores 
\begin_inset Formula $\hat{\Lj}$
\end_inset

 de cada ley 
\begin_inset Formula $\Lj$
\end_inset

 usando las muestras de cada clase, 
\begin_inset Formula $\boldsymbol{x}^{\left(j\right)}=\left(x_{i}^{\left(j\right)}\right)_{i=1}^{N_{j}}$
\end_inset

 y algún procedimiento estándar (e.g.: máxima verosimilitud) 
\end_layout

\begin_layout Enumerate
Definir una regla de decisión 
\begin_inset Formula $\mathcal{R}\left(\cdot\vert\hat{\Lj},j\in\left[M\right]\right):\Rdimx\rightarrow\left[M\right]$
\end_inset

 que dados los estimadores de (2), asigne la observación 
\begin_inset Formula $x_{0}$
\end_inset

 a la clase 
\begin_inset Formula $\mathcal{R}\left(x_{0}\right)$
\end_inset

.
 
\end_layout

\begin_layout Standard
Esta familia de clasificadores, se distinguen por una explícita 
\emph on
estimación de densidades
\emph default
 que más tarde se utilizarán para la tarea de clasificación en sí.
 Por ejemplo, al considerar el problema de clasificación binaria, el análisis
 de discriminante lineal (LDA) de Fisher
\begin_inset Foot
status open

\begin_layout Plain Layout
https://en.wikipedia.org/wiki/Linear_discriminant_analysis
\end_layout

\end_inset

 queda encuadrado en esta familia de la siguiene manera:
\end_layout

\begin_layout Standard
En (1), asumimos que las las leyes 
\begin_inset Formula $\Lj$
\end_inset

 
\end_layout

\begin_layout Labeling
\labelwidthstring 00.00.0000
(a) son todas distribuciones normales con media 
\begin_inset Formula $\mu_{j}$
\end_inset

 y 
\end_layout

\begin_layout Labeling
\labelwidthstring 00.00.0000
(b) homocedásticas: 
\begin_inset Formula $\Sigma_{j}=\Sigma\ \forall\ j\in\text{\ensuremath{\left[M\right]})}$
\end_inset

.
 
\end_layout

\begin_layout Standard
En (2), estimamos 
\begin_inset Formula $\hat{\mu_{j}},\hat{\Sigma}$
\end_inset

 por máxima verosimilitud,
\end_layout

\begin_layout Standard
\begin_inset Formula $\hat{\mu_{j}}=N_{j}^{-1}\sum_{i=1}^{N_{j}}x_{i}^{(j)}$
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula $\hat{\Sigma}=N^{-1}\sum_{j=1}^{M}\sum_{{i=1}}^{N_{j}}(x_{i}^{(j)}-\hat{\mu_{j}})(x_{i}^{(j)}-\hat{\mu_{j}}).$
\end_inset


\end_layout

\begin_layout Standard
Y la regla de (3) es la indicadora 
\begin_inset Formula $1\left(\cdot\right)$
\end_inset

 del discriminante lineal
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
\mathcal{R}\left(x\right) & =1\left(w\cdot x>c\right)\\
w= & \Sigma^{-1}({\mu}_{1}-{\mu}_{0})\\
c= & {\displaystyle w\cdot{\frac{1}{2}}(\mu_{1}+\mu_{0})}
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
con los parámetros 
\begin_inset Formula $\mu_{j},\Sigma$
\end_inset

 reemplazados por las estimaciones de (2).
\end_layout

\begin_layout Standard
Inevitablemente, existe un 
\emph on
trade-off
\emph default
 entre lo restrictivo de los supuestos de (1), y la generalidad del clasificador
 resultante.
 En el caso de LDA, los supuestos (leyes normales y homocedasticidad) son
 inverosímiles en casi cualquier escenario real, pero el clasificador resultante
 es muy sencillo de computar.
 En general, este será el caso para todos los métodos
\emph on
 paramétricos
\emph default
 de estimación de densidad, en que de todas las posibles funciones de densidad
\begin_inset Note Comment
status open

\begin_layout Plain Layout
que cardinalidad tienen? versus la familia normal?
\end_layout

\end_inset

, quedan acotadas a aquellas que se pueden expresar de forma cerrada con
 una expresión predefinida (en este caso, la densidad normal), y 
\begin_inset Formula $Q$
\end_inset

 parámetros (aquí, 
\begin_inset Formula $\mu$
\end_inset

 y 
\begin_inset Formula $\Sigma$
\end_inset

).
\end_layout

\begin_layout Standard
Alternativamente, existen métodos en que los supuestos de (1) se obvian
 del todo, o al menos son lo suficientemente generales como para representar
 todas salvo las más patológicas leyes (e.g.: asumir que la media y dispersión
 son finitas).
 A estos se los conoce, naturalmente, como métodos 
\emph on
no paramétricos
\emph default
 de estimación de densidad.
\end_layout

\begin_layout Subsection*
Estimación de densidad por núcleos
\end_layout

\begin_layout Standard
La estimación de densidad por núcleos (o KDE, por sus siglas en inglés),
 es uno de los métodos mejor estudiados dentro del amplio universo no-paramétric
o
\begin_inset Foot
status open

\begin_layout Plain Layout
Algo sobre NNs, otros metodos nopa
\end_layout

\end_inset

.
 Introducidos hacia 1960 (Rosenblatt 1958, Parzen 1962) para variables aleatoria
s unidimensionales, han sido ampliamente desarrollados y adaptados a espacios
 mucho más generales.
 El objetivo es encontrar un estimador 
\emph on
suave
\emph default
 de la densidad poblacional 
\begin_inset Formula $f$
\end_inset

 de una v.a.
 
\begin_inset Formula $X$
\end_inset

 a partir de una muestra discreta, usando una función no-negativa 
\begin_inset Formula $K$
\end_inset

 llamada 
\emph on
núcleo
\emph default
 (
\begin_inset Quotes eld
\end_inset

kernel
\begin_inset Quotes erd
\end_inset

) y un parámetro de suavización 
\begin_inset Formula $h$
\end_inset

, el 
\emph on
ancho de banda
\emph default
 (
\begin_inset Quotes eld
\end_inset

bandwith
\begin_inset Quotes erd
\end_inset

).
\end_layout

\begin_layout Definition
(función núcleo) Una función 
\begin_inset Formula $\phi$
\end_inset

 es un 
\emph on
núcleo
\emph default
 (
\begin_inset Quotes eld
\end_inset

kernel
\begin_inset Quotes erd
\end_inset

), si
\end_layout

\begin_deeper
\begin_layout Itemize
toma únicamente valores reales no-negativos: 
\begin_inset Formula $\phi\left(x\right)\geq0\forall x$
\end_inset

,
\end_layout

\begin_layout Itemize
está normalizada: 
\begin_inset Formula $\int_{-\infty}^{+\infty}\phi\left(u\right)du=1$
\end_inset

 y
\end_layout

\begin_layout Itemize
es simétrica: 
\begin_inset Formula $K\left(u\right)=\phi\left(-u\right)\forall u$
\end_inset


\end_layout

\end_deeper
\begin_layout Remark
Si 
\begin_inset Formula $K$
\end_inset

 es un núcleo, entonces 
\begin_inset Formula $K_{\lambda}\text{\left(u\right)}=\lambda K\left(\lambda u\right)$
\end_inset

 también lo es, lo cual permite construir un núcleo adecuadamente escalado
 a los datos.
\end_layout

\begin_layout Definition
\begin_inset CommandInset label
LatexCommand label
name "def:kde-univ"

\end_inset

(KDE univariado) Sea 
\begin_inset Formula $\left(x_{1},\dots,x_{N}\right)$
\end_inset

 una muestra de elementos i.i.d.
 tomada de cierta distribución univariada con densidad desconocida 
\begin_inset Formula $f$
\end_inset

, cuya forma deseamos conocer.
 Su estimador de densidad por núcleos (su 
\begin_inset Quotes eld
\end_inset

KDE
\begin_inset Quotes erd
\end_inset

) es
\end_layout

\begin_layout Definition
\begin_inset Formula ${\displaystyle {\widehat{f}}_{h}(x)={\frac{1}{n}}\sum_{i=1}^{n}\phi_{h}(x-x_{i})={\frac{1}{nh}}\sum_{i=1}^{n}\phi{\Big(}{\frac{x-x_{i}}{h}}{\Big)}}$
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Standard
Dejando por un momento de lado qué par 
\begin_inset Formula $\left(K,h\right)$
\end_inset

 usar, podemos derivar un clasificador 
\begin_inset Quotes eld
\end_inset

duro
\begin_inset Quotes erd
\end_inset

 de manera bastante directa para la versión univariada del problema 
\begin_inset CommandInset ref
LatexCommand ref
reference "def:prob-clf"
plural "false"
caps "false"
noprefix "false"

\end_inset

:
\end_layout

\begin_layout Definition
\begin_inset CommandInset label
LatexCommand label
name "def:clf-kde-univ"

\end_inset

(clasificador KDE univariado).
 Sea 
\begin_inset Formula $C:\Rdimx\rightarrow\left[M\right]$
\end_inset

 la 
\begin_inset Quotes eld
\end_inset

función de clase
\begin_inset Quotes erd
\end_inset

, tal que 
\begin_inset Formula $\forall x\in\Rdimx,\ C\left(x\right)=j\iff x\in C_{j}$
\end_inset

.
 Sean además 
\begin_inset Formula $\hat{f}_{h}^{(1)},\dots,\hat{f}_{h}^{(M)}$
\end_inset

 los estimadores de densidad obtenidos según 
\begin_inset CommandInset ref
LatexCommand ref
reference "def:kde-univ"

\end_inset

.
 El 
\begin_inset Quotes eld
\end_inset

clasificador por estimación de densidad nuclear
\begin_inset Quotes erd
\end_inset

 correspondiente será:
\begin_inset Formula 
\begin{align*}
\hat{C}\left(x\right) & =\mathrm{\arg\max_{j\in\left[M\right]}\ }\hat{f}_{h}^{(j)}\left(x\right)
\end{align*}

\end_inset


\end_layout

\begin_layout Definition
asignando cada observación a la clase en la que maximiza la densidad estimada.
\begin_inset Note Comment
status open

\begin_layout Plain Layout
Debería estar ya incluyendo para hard clf las proporciones muestrales 
\begin_inset Formula $p_{j}=N_{j}/N$
\end_inset

 como probabilidades de clase 
\emph on
a priori
\emph default
?
\end_layout

\end_inset


\end_layout

\begin_layout Standard
Cuando las clases de las cuales se compone la población se encuentran muy
 
\begin_inset Quotes eld
\end_inset

separadas
\begin_inset Quotes erd
\end_inset

 entre sí (es decir, 
\begin_inset Formula $\exists k\in\left[M\right]:f_{h}^{(k)}\text{\left(x_{0}\right)\ensuremath{\gg}0\ },\ f_{h}^{(j)}\simeq0\ \forall\ j\in\left[M\right]/k$
\end_inset

), la clasificación 
\begin_inset Quotes eld
\end_inset

dura
\begin_inset Quotes erd
\end_inset

 de 
\begin_inset CommandInset ref
LatexCommand ref
reference "def:clf-kde-univ"

\end_inset

 será suficiente.
 Ahora bien, ¿cómo hacemos para cuantificar la incertidumbre asociada a
 la clasificación, cuando existe más de una clase con densidad estimada
 no despreciable? Como las 
\begin_inset Formula $\hat{f}_{h}^{(j)}$
\end_inset

 estimadas identifican distribuciones, es razonable decir que 
\begin_inset Formula $p\left(C\text{\left(x\right)}=j\right)\propto f_{h}^{(j)}\left(x\right)$
\end_inset

.
 Usando la regla de Bayes y un 
\emph on
a priori
\emph default
 sobre las probabilidades de clase basado en las proporciones muestrales
 
\begin_inset Formula $\hat{p}\left(C_{j}\right)=N_{j}/N$
\end_inset

, podemos conseguir una regla 
\emph on
suave
\emph default
 de clasificación:
\end_layout

\begin_layout Definition
\begin_inset CommandInset label
LatexCommand label
name "def:soft-clf-kde"

\end_inset

(clasificador KDE univariado suave) Sea el problema 
\begin_inset CommandInset ref
LatexCommand ref
reference "def:prob-clf"
plural "false"
caps "false"
noprefix "false"

\end_inset

 y los estimadores de densidad de 
\begin_inset CommandInset ref
LatexCommand ref
reference "def:kde-univ"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
 Por la regla de bayes, 
\begin_inset Formula 
\[
p\left(C\left(x\right)=j\right)=\frac{f^{(j)}\left(x\right)\cdot p\text{\left(C_{j}\right)}}{p\text{\left(x\right)}}
\]

\end_inset


\end_layout

\begin_layout Definition
Reemplazando el a priori 
\begin_inset Formula $p\text{\left(C_{j}\right)}$
\end_inset

 por su estimación muestral, las densidades 
\begin_inset Formula $f^{(j)}$
\end_inset

 por sus estimadores y usando la ley de la probabilidad total para expandir
 
\begin_inset Formula $p\left(x\right)$
\end_inset

, obtenemos:
\begin_inset Formula 
\[
\hat{p}\left(C\left(x\right)=j\right)=\frac{\hat{f}_{h}^{(j)}\left(x\right)\cdot N_{j}}{\sum_{i\in\text{\left[M\right]}}\hat{f}_{h}^{(i)}\left(x\right)\cdot N_{i}}
\]

\end_inset


\end_layout

\begin_layout Subsection
La noción de distancia en KDE
\end_layout

\begin_layout Standard
El peso de cada 
\begin_inset Formula $x_{i}$
\end_inset

 en 
\begin_inset Formula $\hat{f}\left(x_{0}\right)$
\end_inset

 es 
\begin_inset Formula $\phi\left(x_{0}-x_{i}\right)$
\end_inset

, y como 
\begin_inset Formula $\phi$
\end_inset

 es simétrica respecto al 0, sólo importa la 
\emph on
distancia
\emph default
 entre el nuevo punto y cada muestra, 
\begin_inset Formula $x_{0},x_{i};$
\end_inset

 no así la 
\emph on
dirección
\emph default
.
 En una dimensión al menos, el núcleo 
\begin_inset Formula $\phi$
\end_inset

 pondera - escalando por 
\begin_inset Formula $h$
\end_inset

 - la distancia (euclídea) entre el punto a clasificar y cada datum:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\phi_{h}\left(x_{0}-x_{i}\right)=\phi_{h}\left(\left|x_{0}-x_{i}\right|\right)=\frac{1}{h}\phi\left(\frac{\left|x_{0}-x_{i}\right|}{h}\right)
\]

\end_inset


\end_layout

\begin_layout Standard
En mayore dimensiones, la situación es más compleja, pero análoga
\end_layout

\begin_layout Definition
\begin_inset CommandInset label
LatexCommand label
name "def:kde-multiv"

\end_inset

(KDE multivariado, Hwang 1994) Sea 
\begin_inset Formula $\left\{ \mathbf{x}\right\} =$
\end_inset


\begin_inset Formula $\left\{ x_{1},\dots,x_{N}\right\} $
\end_inset

 una muestra de elementos i.i.d.
 tomada de cierta distribución 
\begin_inset Formula $d-$
\end_inset

dimensional con densidad desconocida 
\begin_inset Formula $f$
\end_inset

, cuya forma deseamos conocer.
 Su estimador de densidad por núcleos (su 
\begin_inset Quotes eld
\end_inset

KDE
\begin_inset Quotes erd
\end_inset

) será
\end_layout

\begin_layout Definition
\begin_inset Formula ${\displaystyle {\widehat{f}_{h}}(x)={\frac{1}{Nh^{d}}}\sum_{i=1}^{N}\phi\left(\frac{1}{h}\left(x-x_{i}\right)\right)}$
\end_inset


\end_layout

\begin_layout Definition
donde el núcleo 
\begin_inset Formula $\phi$
\end_inset

 debe satisfacer
\end_layout

\begin_layout Definition
\begin_inset Formula $\phi\left(x\right)\geq,0,$
\end_inset

 y 
\begin_inset Formula $\int_{\Rd}$
\end_inset


\begin_inset Formula $\left(x\right)dx=1$
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Standard
Un núcleo muy popular es el gaussiano 
\begin_inset Formula $\phi\left(x\right)=\left(2\pi\right)^{-d/2}\exp\left(-\frac{\norm x^{2}}{2}\right)$
\end_inset

,
\end_layout

\begin_layout Standard
un núcleo simétrico con su valor decayendo suavemente a medida que se aleja
 del centro.
\end_layout

\begin_layout Standard
Usualmente los datos no se encontrarán distribuidos uniformemente en todas
 las direcciones, y será deseable pre-escalarlos para evitar diferencias
 extremas en su dispersión y locación.
 Un enfoque atractivo, es primero 
\begin_inset Quotes eld
\end_inset

esferar
\begin_inset Quotes erd
\end_inset

 o 
\begin_inset Quotes eld
\end_inset

blanquear
\begin_inset Quotes erd
\end_inset

 los datos mediante una transformación afín que devuelva data con media
 cero y matriz de covarianza unitaria; y luego aplicar 
\begin_inset CommandInset ref
LatexCommand ref
reference "def:kde-multiv"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
 Más específicamente, dada una muesta 
\begin_inset Formula $\left\{ x\right\} ,$
\end_inset

podemos definir su versión 
\begin_inset Quotes eld
\end_inset

esférica
\begin_inset Quotes erd
\end_inset

 como
\begin_inset Formula 
\[
z=\mathbf{S}^{-1/2}\left(x-Ex\right)
\]

\end_inset


\end_layout

\begin_layout Standard
donde la esperanza 
\begin_inset Formula $E$
\end_inset

 es evaluada a través de la media muestral, y 
\begin_inset Formula $\mathbf{S}\in\R^{d\times d}$
\end_inset

es la matriz de covarianza de los datos:
\begin_inset Formula 
\begin{align*}
\mathbf{S} & =E\left[\left(x-Ex\right)\left(x-Ex\right)^{T}\right]=\mathbf{UDU}^{T}\\
\mathbf{S}^{-1/2} & =\mathbf{UD}^{-1/2}\mathbf{U}^{T}
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
Donde 
\begin_inset Formula $\mathbf{U}$
\end_inset

 es una matriz ortonormal y 
\begin_inset Formula $\mathbf{D}$
\end_inset

 es una matriz diagonal.
 De preocuparse por la influencia de 
\emph on
outliers
\emph default
 en los datos, existen métodos robustos para su derivación (Huber 1981).
\end_layout

\begin_layout Standard
Se puede mostrar fácilment que luego del 
\begin_inset Quotes eld
\end_inset

blanqueo
\begin_inset Quotes erd
\end_inset

, 
\begin_inset Formula $Ez=0$
\end_inset

 y 
\begin_inset Formula $E\left[zz^{T}\right]=\mathbf{I_{d}}$
\end_inset

.
 El estimador resultante para los datos esféricos realiza un estimación
 de densidad más sofisticada
\begin_inset CommandInset label
LatexCommand label
name "def:kde-multiv-blanco"

\end_inset


\end_layout

\begin_layout Standard

\color red
Acá sigo a Hwang, que hace una presentación algo distinta a Wikipedia/Wand&Jones
, primero blanquea y luego usa núcleo con 
\begin_inset Formula $\H=\mathbf{I_{d}}$
\end_inset

; W&J no recomienda blanquear porque luego usa 
\begin_inset Formula $\mathbf{H}$
\end_inset

 arbitraria.
 Debería revisar bien la equivalencia, qué conviene más.
\color inherit
:
\begin_inset Formula 
\begin{align*}
\hat{f}\left(z\right) & =\frac{1}{Nh^{d}}\sum_{i=1}^{N}\phi\left(\frac{1}{h}\left(z-z_{i}\right)\right)\\
\hat{f}\left(x\right) & =\frac{\left(\det\text{\ensuremath{\mathbf{S}}}\right)^{-1/2}}{Nh^{d}}\sum_{i=1}^{N}\phi\left(\frac{1}{h}\mathbf{S}^{-1/2}\left(x-x_{i}\right)\right)
\end{align*}

\end_inset


\end_layout

\begin_layout Remark
\begin_inset CommandInset label
LatexCommand label
name "rem:mahalanobis-dist"

\end_inset

Dada una distribución de probabilidad 
\begin_inset Formula $Q$
\end_inset

 en 
\begin_inset Formula $\R^{d},$
\end_inset

 con media 
\begin_inset Formula $\mu\in\R^{d}$
\end_inset

 y matriz de covarianza positiva definida 
\begin_inset Formula $\mathbf{S}\in\R^{d\times d}$
\end_inset

, la 
\emph on
distancia de Mahalanobis
\begin_inset Foot
status open

\begin_layout Plain Layout
https://en.wikipedia.org/wiki/Mahalanobis_distance
\end_layout

\end_inset


\emph default
 de un punto 
\begin_inset Formula $x$
\end_inset

 a 
\begin_inset Formula $Q$
\end_inset

 es
\begin_inset Formula 
\[
d_{M}\text{\left(x,Q\right)=\ensuremath{\sqrt{\left(x-\mu\right)^{T}\mathbf{S}^{-1}\left(x-\mu\right)}}}
\]

\end_inset


\end_layout

\begin_layout Remark
Dados dos puntos 
\begin_inset Formula $x,y$
\end_inset

 en 
\begin_inset Formula $\R^{n}$
\end_inset

, la distancia de Mahalanobis 
\emph on
entre si
\emph default
 con respecto a 
\begin_inset Formula $Q$
\end_inset

 es
\begin_inset Formula 
\[
d_{M}\text{\left(x,Q\right)}=d_{M}\left(x,\mu;Q\right)
\]

\end_inset


\end_layout

\begin_layout Remark
Como 
\begin_inset Formula $\mathbf{S}$
\end_inset

 es definida positiva, también lo es 
\begin_inset Formula $S^{-1}$
\end_inset

, con lo que las raíces cuadradas están bien definidas.
 Por el teorema espectral, 
\begin_inset Formula $S^{-1}$
\end_inset

 se puede descomponer en 
\begin_inset Formula $S^{-1}=\left(S^{-1/2}\right)^{T}S^{^{-1/2}}$
\end_inset

 para alguna matriz real 
\begin_inset Formula $d\times d$
\end_inset

, lo cual sugiere una definición equivalente
\end_layout

\begin_layout Remark
\begin_inset Formula $d_{M}\left(x,y;Q\right)=\norm{S^{^{-1/2}}\left(x-y\right)}$
\end_inset


\end_layout

\begin_layout Remark
donde 
\begin_inset Formula $\norm{\cdot}$
\end_inset

 es la norma euclídea.
 Es decir, la distancia de Mahalanobis es la distancia euclídea luego de
 una transformación de blanqueo.
\end_layout

\begin_layout Remark
Reemplazando 
\begin_inset Formula $W=S^{-1/2},\mu=x_{1},\dots,x_{N}$
\end_inset

, podemos redefinir el estimador de 
\begin_inset CommandInset ref
LatexCommand ref
reference "def:kde-multiv-blanco"
plural "false"
caps "false"
noprefix "false"

\end_inset

 para 
\begin_inset Formula $f\left(x\right)$
\end_inset

 como un estimador de núcleos basado en la distancia de Mahalanobis de 
\begin_inset Formula $x$
\end_inset

 a cada observación de 
\begin_inset Formula $\left\{ \mathbf{x}\right\} $
\end_inset

.
\end_layout

\begin_layout Remark
La relativa sencillez para el cómputo del método hasta aquí descrito lo
 hace un perenne favorito entre los estimadores de densidad no paramétricos.
 Quedará a criterio del investigador considerar si sus bondades vuelven
 tolerables las limitaciones impuestas o no.
 A saber,
\end_layout

\begin_layout Enumerate
Salvo en casos excepcionalmente bien portados, la dirección y dispersión
 
\emph on
local
\emph default
 de la muestra alrededor de un cierto punto 
\begin_inset Formula $x_{i}$
\end_inset

 típicamente no coincidirá con la dirección 
\series bold

\begin_inset Formula $\mathbf{U}$
\end_inset


\series default
 y dispersión 
\series bold
\emph on

\begin_inset Formula $\mathbf{D}$
\end_inset


\series default
 global
\emph default
 que se obtienen de computar 
\begin_inset Formula $\mathbf{S}=\mathbf{UDU}^{T}$
\end_inset

 en la muestra completa.
\end_layout

\begin_layout Enumerate
Aún cuando la estimación global de 
\begin_inset Formula $\mathbf{S}$
\end_inset

 sea localmente adecuada, no resulta inmediatamente obvio que la suavización
 
\begin_inset Formula $\mathbf{H=S}$
\end_inset

 inducida por la muestra sea óptima en términos de representación de la
 densidad, super- y sub-suavizando
\begin_inset Foot
status open

\begin_layout Plain Layout
oversmoothing y undersmoothing
\end_layout

\end_inset

 regiones de alta densidad y 
\emph on
outliers
\emph default
, respectivamente.
\end_layout

\begin_layout Enumerate
Al ubicar una 
\begin_inset Quotes eld
\end_inset

montañita
\begin_inset Quotes erd
\end_inset

 de densidad en 
\emph on
cada
\emph default
 dato de la muestra, el cómputo del estimador hasta aquí expuesto se vuelve
 prohibitamente costoso para 
\begin_inset Formula $N$
\end_inset

 relativamente grande.
\end_layout

\begin_layout Standard
Wand & Jones (1993) realiza un estudio exhaustivo de las consecuencias de
 distintas parametrizaciones de 
\begin_inset Formula $\mathbf{H}$
\end_inset

 para el caso multivariado más sencillo, 
\begin_inset Formula $d=2$
\end_inset

, considerando familias de creciente complejidad para 
\begin_inset Formula $\mathbf{H}$
\end_inset

, siempre positivas definidas:
\end_layout

\begin_layout Itemize
en términos generales,
\end_layout

\begin_deeper
\begin_layout Itemize
productos escalares de la identidad: 
\begin_inset Formula $\mathcal{H}_{1}\coloneqq\left\{ h_{1}^{2}\mathbf{I};h_{1}>0\right\} $
\end_inset


\end_layout

\begin_layout Itemize
matrices diagonales con distintas escalas en cada eje: 
\begin_inset Formula $\mathcal{H}_{2}\coloneqq\left(\text{diag}\left(h_{1}^{2},h_{2}^{2}\right);h_{1},h_{2}>0\right)$
\end_inset


\end_layout

\begin_layout Itemize
matrices completas: 
\begin_inset Formula 
\[
\mathcal{H}_{3}\coloneqq\left\{ \left[\begin{array}{cc}
h_{1}^{2} & h_{12}\\
h_{12} & h_{2}^{2}
\end{array}\right];h_{1},h_{2}>0,\left|h_{12}\right|<h_{1}h_{2}\right\} 
\]

\end_inset

,
\end_layout

\end_deeper
\begin_layout Itemize
basadas en una 
\begin_inset Quotes eld
\end_inset

esferización
\begin_inset Quotes erd
\end_inset

 de los datos vía matriz de covarianza 
\begin_inset Formula $\mathbf{C}=\left[\begin{array}{cc}
c_{11} & c_{12}\\
c_{12} & c_{22}
\end{array}\right]$
\end_inset

 de la densidad objetivo 
\begin_inset Formula $f$
\end_inset

:
\end_layout

\begin_deeper
\begin_layout Itemize
ignorando la correlación 
\begin_inset Formula $\mathcal{C}_{2}\coloneqq\left\{ h^{2}\mathbf{D};h^{2}>0\right\} $
\end_inset

, con 
\begin_inset Formula $\mathbf{D}=\text{diag}\left(c_{11,}c_{22}\right)$
\end_inset

,
\end_layout

\begin_layout Itemize
completa 
\begin_inset Formula $\mathcal{C}_{3}\coloneqq\left\{ h^{2}\mathbf{C};h^{2}>0\right\} $
\end_inset

 e
\end_layout

\begin_layout Itemize

\emph on
híbridas
\emph default
, con suavizado independiente en cada dirección 
\begin_inset Formula 
\[
\mathcal{Y}\coloneqq\left\{ \left[\begin{array}{cc}
h_{1}^{2} & \rho_{12}h_{1}h_{2}\\
\rho_{12}h_{1}h_{2} & h_{2}^{2}
\end{array}\right];h_{1},h_{2}>0\right\} 
\]

\end_inset

 y coeficiente de correlación 
\begin_inset Formula $\rho_{12}=c_{12}/\sqrt{c_{11}c_{22}}$
\end_inset


\end_layout

\end_deeper
\begin_layout Standard
Nótese que 
\begin_inset Formula $\mathcal{H}_{1}\subseteq\mathcal{H}_{2}\subseteq\mathcal{H}_{3},\ \ \mathcal{C}_{2}\subseteq\mathcal{H}_{2},\ \ \mathcal{C}_{3}\subseteq\mathcal{H}_{3},\ \ \mathcal{Y}\subseteq\mathcal{H}_{3}$
\end_inset

.
 Wand & Jones se 
\begin_inset Quotes eld
\end_inset

desembarazan
\begin_inset Quotes erd
\end_inset

 del problema de 
\emph on
selección
\emph default
 de anchos de banda, eligiendo enfocarse en la 
\emph on
eficiencia relativa asintótica
\begin_inset Foot
status open

\begin_layout Plain Layout
burdamente, la relación entre los tamaños muestrales necesarios para conseguir
 el mismo error asintótico restringiendo 
\begin_inset Formula $\mathbf{H}$
\end_inset

 a cierto par de las clases aquí descritas 
\end_layout

\end_inset


\emph default
 de cada clase con respecto a la más general 
\begin_inset Formula $\mathcal{H}_{3}$
\end_inset

.
 A tal fin, toman como medida del error global incurrido por cierto estimador
 
\begin_inset Formula $\hat{f}_{\mathbf{H}}$
\end_inset

 el error cuadrático medio integrado (MISE, por sus siglas en inglés)
\begin_inset Formula 
\[
MISE\left(\mathbf{H}\right)=MISE\left(\hat{f}_{\mathbf{H}},f\right)=E\int_{\R^{d}}\left(\hat{f}_{\mathbf{H}}\left(y\right)-f\left(y\right)\right)^{2}dy
\]

\end_inset


\end_layout

\begin_layout Standard
y su aproximación asintótica.
\end_layout

\begin_layout Standard
Los autores notan una dificultad cualitativamente nueva en el caso multivariado
 en comparación al univariado: definir la 
\emph on
orientación
\emph default
 de 
\begin_inset Formula $\text{\textbf{H}}$
\end_inset

.
 Aún en el relativamente sencillo contexto bivariado, muestran cómo la estrategi
a 
\begin_inset Quotes eld
\end_inset

ingenua
\begin_inset Quotes erd
\end_inset

 de depender para ello de la covarianza muestral conlleva enormes pérdidas
 de eficiencia, aún para la familia 
\begin_inset Formula $\mathcal{Y}$
\end_inset

, sobre todo para densidades multimodales y otras que se alejan de la normalidad.
\begin_inset Note Comment
status collapsed

\begin_layout Plain Layout
W&J (1993) tiene un lindo ejemplo 
\begin_inset Quotes eld
\end_inset

(F) Bimodal II
\begin_inset Quotes erd
\end_inset

 de cómo la covarianza estimada para una mezcla de dos gausianas con diferencias
 en la locación sobre el eje x, y mayor dispersión en el eje y, termina
 dando una estimación de la covarianza inútil para suavizado.
 Podría reproducirlo con scipy+matplotlib para ilustrar.
\end_layout

\end_inset

 En su recomendación final, los autores sugieren que en general 
\begin_inset Quotes eld
\end_inset

hay mucho para ganar incluyendo parámetros de orientación
\begin_inset Quotes erd
\end_inset

 (es decir, elementos no-diagonales) en la parametrización de 
\begin_inset Formula $\mathbf{H}$
\end_inset

.
\end_layout

\begin_layout Standard
Autores posteriores han tomado el desafío y considerado métodos para la
 elección de un suavizador 
\begin_inset Formula $\mathbf{H}\in\mathcal{H}_{3}$
\end_inset

 para el caso general 
\begin_inset Formula $d-$
\end_inset

dimensional.
 Los mismos autores en un trabajo posterior (WandJones94) proponen un estimador
 
\begin_inset Quotes eld
\end_inset

plug-in
\begin_inset Quotes erd
\end_inset

 del 
\begin_inset Formula $\mathbf{H}$
\end_inset

 óptimo que se puede aplicar a 
\begin_inset Formula $\mathcal{H}_{3}$
\end_inset

, pero luego se limitan a la familia diagonal 
\begin_inset Formula $\mathcal{H}_{2}$
\end_inset

 para su aplicación concreta.
 Duong2005 sintetiza sus aportes propios y otros precedentes alrededor de
 la estimación de 
\series bold

\begin_inset Formula $\mathbf{H}$
\end_inset


\series default
 completa según tres métodos de validación cruzada
\begin_inset Foot
status open

\begin_layout Plain Layout
\begin_inset Quotes eld
\end_inset

cross-validation
\begin_inset Quotes erd
\end_inset

, o CV, por sus siglas en inglés.
\end_layout

\end_inset

: CV sesgada (BCV), CV insesgada (UCV), y CV 
\begin_inset Quotes eld
\end_inset

suavizada
\begin_inset Quotes erd
\end_inset

 (SCV).
 Todos los métodos propuestos buscan minimizar un error cuadrático (UCV
 usa MISE; BCV el asintótico AMISE y SCV una combinación lineal de ambos),
 en el contexto de validación cruzada 
\begin_inset Quotes eld
\end_inset

dejar-uno-afuera
\begin_inset Quotes erd
\end_inset

.
 El método con el que mejores resultados obtienen, SCV, es también el más
 complejo en su implementación, pues requiere considerar un 
\begin_inset Quotes eld
\end_inset

suavizador piloto
\begin_inset Quotes erd
\end_inset

 
\begin_inset Formula $\mathbf{G}\in\R^{d\times d}$
\end_inset

 cuya elección no es transparente.
\end_layout

\begin_layout Standard
Hall2005, por su parte, motivado por la aplicación concreta de estimación
 de densidad al problema de clasificación, toma un camino distinto para
 la optimización: en lugar de elegir 
\begin_inset Formula $\mathbf{H}$
\end_inset

 minimizando (A)MISE, se propone elegir 
\begin_inset Formula $\mathbf{H}$
\end_inset

 de manera que minimice una función relacionada directamente con la tarea
 propuesta: el riesgo de Bayes.
 Sea 
\begin_inset Formula $\mathcal{R}$
\end_inset

 una regla de decisión como se planteó en 
\begin_inset CommandInset ref
LatexCommand ref
reference "def:prob-clf"
plural "false"
caps "false"
noprefix "false"

\end_inset

, diremos que el 
\emph on
riesgo de Bayes
\emph default
 en una región 
\begin_inset Formula $\Gamma$
\end_inset

 es 
\begin_inset Formula 
\begin{align*}
\text{err}_{\mathcal{R}}\text{\left(f_{1},\dots,f_{K}\vert\gamma\right)}\\
= & \sum_{j=1}^{K}p_{j}\int_{\Gamma}Pr\left(x\text{\textbf{ no }sea clasificado por \ensuremath{\mathcal{R}} como \ensuremath{\in C_{j}}}\right)f_{j}\left(x\right)dx
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
En este sentido, el clasificador de 
\begin_inset CommandInset ref
LatexCommand ref
reference "def:clf-kde-univ"
plural "false"
caps "false"
noprefix "false"

\end_inset

 es óptimo, y por ende es razonable argumentar que elegir 
\begin_inset Formula $\mathbf{H}$
\end_inset

 como Hall propone es superador a optimizar 
\begin_inset Formula $\mathbf{H}$
\end_inset

 para el 
\begin_inset Quotes eld
\end_inset

resultado intermedio
\begin_inset Quotes erd
\end_inset

 de estimar las densidades de cada clase.
 En efecto, para el caso más sencillo 
\begin_inset Formula $K=2,d=1$
\end_inset

 y las densidades se 
\begin_inset Quotes eld
\end_inset

cruzan
\begin_inset Quotes erd
\end_inset

 en un solo punto con un mismo signo, los anchos de banda encontrados por
 minimización del riesgo de Bayes son un orden de magnitud distintos de
 los ya cubiertos.
 Sin embargo, para 
\begin_inset Formula $d>1,K\geq2$
\end_inset

 , resulta ser el caso que el ancho de banda óptimo según el error de Bayes
 es el mismo que via (A)MISE.
\end_layout

\begin_layout Standard
Hwang (1994) comienza estudiando explícitamente cómo elegir 
\begin_inset Formula $h$
\end_inset

 para datos esferizados (la familia 
\begin_inset Formula $\mathcal{C}_{3}$
\end_inset

 en Wand&Jones93), luego nota las dificultades (2) y (3) previamente mencionadas
, y compara varios algoritmos superadores en algún sentido al KDE con ancho
 de banda fijo (FKDE):
\end_layout

\begin_layout Itemize
KDE adaptativo (AKDE), similar a FKDE esferizado pero con un factor de ancho
 local 
\begin_inset Formula $\lambda_{n}$
\end_inset

 para cada núcleo
\begin_inset Formula 
\[
\hat{f}_{AKDE}\left(z\right)=\frac{1}{Nh^{d}}\sum_{i=1}^{N}\lambda_{i}^{-d}\phi\left(\frac{1}{h\lambda_{i}}\left(z-z_{i}\right)\right)
\]

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
El cómputo de los factores 
\begin_inset Formula $\lambda_{i}$
\end_inset

 ha de resolverse iterativamente, comenzando por el caso FKDE, 
\begin_inset Formula $\lambda_{i}=1\forall i\in\left[N\right]$
\end_inset

, con lo cual el costo computacional será aún más alto que en el caso base.
\end_layout

\begin_layout Itemize
Aunque cada núcleo estará mejor escalado a su contexto local, el enfoque
 sigue utilizando una misma orientación global para todos los núcleos.
\end_layout

\end_deeper
\begin_layout Itemize
KDE de base funcional radial (RBF): para minimizar la cantidad de núcleos
 a ajustar a los datos, divide el proceso de estimación de densidad en dos
 partes: (i) agrupar los datos en clusters según cierto algoritmo no-supervisado
, y luego (ii) ajustar un núcleo gaussiano, su altura y su ancho a cada
 cluster de (i).
 Por esto, también se lo conoce com 
\begin_inset Quotes eld
\end_inset

modelado de mezclas gaussianas
\begin_inset Quotes erd
\end_inset

.
\end_layout

\begin_deeper
\begin_layout Itemize
Aunque el estimador final se puede expresar con muy pocos términos, el procedimi
ento completo es considerablemente más complejo que el de FKDE, dependiendo
 críticamente de la esferización y remoción de 
\emph on
outliers
\emph default
 para la detección de clusters.
\end_layout

\begin_layout Itemize
Dependiendo del tamaño muestral, la dimensionalidad de los datos y la cantidad
 de bases utilizadas, ciertos clusters pueden resultar en núcleos demasiado
 
\begin_inset Quotes eld
\end_inset

empinados
\begin_inset Quotes erd
\end_inset

 o demasiado 
\begin_inset Quotes eld
\end_inset

planos
\begin_inset Quotes erd
\end_inset

.
 Así, una de las principales ventajas de este método - la posibilidad de
 ajustar una matriz de covarianza distinta a cada cluster de datos - implicará
 una minuciosa inspección de los datos para saber qué escala y orientación
 es razonable para cada base.
\end_layout

\end_deeper
\begin_layout Itemize
KDE por 
\begin_inset Quotes eld
\end_inset

persecución de la proyección
\begin_inset Quotes erd
\end_inset

 (PPDE): El espíritu de este método, está basado en buscar iterativamente
 proyecciones 
\begin_inset Quotes eld
\end_inset

interesantes
\begin_inset Quotes erd
\end_inset

 de los datos en bajas dimensiones (típicamente 1-D), modificar la muestra
 original 
\begin_inset Formula $\left\{ \mathbf{x}\right\} ^{\left(0\right)}$
\end_inset

 para remover la estructura encontrada en la proyección, y repetir el proceso
 en los datos resultantes.
 Siguiendo a Huber 85, la distribución normal se considera la 
\begin_inset Quotes eld
\end_inset

menos interesante
\begin_inset Quotes erd
\end_inset

, y será 
\begin_inset Quotes eld
\end_inset

más interesante
\begin_inset Quotes erd
\end_inset

 aquella proyección de los datos que más se le aleje.
\end_layout

\begin_deeper
\begin_layout Itemize
Para evitar confundir la dirección y escala de la muestra con proyecciones
 verdaderamente interesante, el método de PPDE requiere también esferizar
 los datos e ignorar 
\emph on
outliers 
\emph default
juiciosamente (p.
 29 Huber85).
\end_layout

\begin_layout Itemize
Un problema específico a PPDE, es que no puede lidiar satisfactoriamente
 con estructuras 
\begin_inset Quotes eld
\end_inset

escondidas
\begin_inset Quotes erd
\end_inset

 detrás de otras.
 E.g., las proyecciones de una densidad 2-D con forma de dona a 1-D no dan
 cuenta fehaciente de la estructura original.
\end_layout

\end_deeper
\begin_layout Standard
En resumen:
\end_layout

\begin_layout Itemize
Wand, Jones y Duong, entre tantos otros, pretenden buscar selectores basados
 en MISE para 
\begin_inset Formula $\mathbf{H}$
\end_inset

 completa, pero terminan encontrando dificultades que los restringen, en
 la práctica, a matrices diagonales, o los enriedan en la selección de parámetro
s auxiliares con complejidad propia.
\end_layout

\begin_layout Itemize
Hall, motivado por el problema que nos compete, intenta un selector basado
 en el riesgo de Bayes, pero el método resulta tan complejo en su propio
 derecho, que aún al tratar muestras multivariadas, lo hace con un suavizador
 escalar 
\begin_inset Formula $h,\ \mathbf{H}\in\mathcal{H}_{1}$
\end_inset

, y nota que los resultados no difieren significativamente de los obtenidos
 minimizando el error cuadrático integrado.
\end_layout

\begin_layout Itemize
Hwang explora todo tipo de anchos de banda escalares (fijos, adaptativos,
 clusterizados), y por último 
\begin_inset Quotes eld
\end_inset

abandona
\begin_inset Quotes erd
\end_inset

 esta línea y considera un método que no requiere definir explícitamente
 un suavizador 
\series bold
H
\series default
: PPDE.
\end_layout

\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Subsection
La maldición de la dimensionalidad
\end_layout

\begin_layout Standard
Hasta aquí, pareciera ser que el enfoque de estimación de densidad por núcleos
 para el caso multivariado está irremediablemente condenado al fracaso,
 o al menos a una agotadora complejidad.
 Sin embargo, antes de claudicar, vale la pena entender algunas de las razones
 de tamaña complejidad.
\end_layout

\begin_layout Standard
Una dificultad obvia es que aún considerando un único suavizador global
 
\series bold
H
\series default
, en 
\begin_inset Formula $d$
\end_inset

 dimensiones hacen falta estimar 
\begin_inset Formula $\tbinom{d}{1}+\tbinom{d}{2}=\text{\left(d^{2}+d\right)/2}$
\end_inset

 varianzas y covarianzas, respectivamente.
 El crecimiento cuadrático en la cantidad de parámetros implicará que el
 tamaño muestral 
\begin_inset Formula $N$
\end_inset

 necesario para obtener estimaciones razonables crezca insosteniblemente.
 El fenómeno, conocido como 
\begin_inset Quotes eld
\end_inset

maldición de la dimensionalidad
\begin_inset Quotes erd
\end_inset

, se puede entender intuitivamente considerando el siguiente escenario:
\end_layout

\begin_layout Remark
\begin_inset CommandInset label
LatexCommand label
name "rem:curse-dim"

\end_inset

Sea 
\begin_inset Formula $B\left(x,r,d\right)$
\end_inset

 la bola 
\begin_inset Formula $d-$
\end_inset

dimensional de radio 
\begin_inset Formula $r$
\end_inset

 centrada en 
\begin_inset Formula $x\in\R^{d}$
\end_inset

, y consideremos una v.a.
 uniformemente distribuida dentro de ella (por volumen), 
\begin_inset Formula $X\sim Unif\left(B\left(0,r,d\right)\right)$
\end_inset

.
 Sea 
\begin_inset Formula $\epsilon>0$
\end_inset

; cuál es la probabilidad de que 
\begin_inset Formula $X$
\end_inset

 se encuentre al 
\begin_inset Quotes eld
\end_inset

interior
\begin_inset Quotes erd
\end_inset

 de la bola (sustrayendo un 
\begin_inset Quotes eld
\end_inset

cascarón
\begin_inset Quotes erd
\end_inset

 externo de espesor 
\begin_inset Formula $\epsilon$
\end_inset

) 
\begin_inset Formula $Pr\text{\left(X\in B\left(0,r-\epsilon,d\right)\right)}$
\end_inset

?
\end_layout

\begin_layout Remark
Como la distribución de 
\begin_inset Formula $X$
\end_inset

 es uniforme en volumen, y 
\begin_inset Formula $B\left(x,r-\epsilon,d\right)\subset B\left(x,r,d\right)$
\end_inset

, basta con comparar los volúmenes de de ambas 
\begin_inset Formula $d-$
\end_inset

esferas para encontrar la solución.
 El volumen
\begin_inset Formula $d-$
\end_inset

dimensional de una bola es
\end_layout

\begin_layout Remark
\begin_inset Formula 
\[
{\displaystyle Vol\left(B\left(x,r,d\right)\right)=Vol_{B}\left(r,d\right)={\frac{\pi^{d/2}}{\Gamma\left(\tfrac{d}{2}+1\right)}}r^{d}}
\]

\end_inset


\end_layout

\begin_layout Remark
donde 
\begin_inset Formula ${\displaystyle \Gamma\left(z\right)=\int_{0}^{\infty}t^{z-1}e^{-t}\,dt}$
\end_inset

 es la función gamma.
 Luego,
\begin_inset Formula 
\[
Pr\text{\ensuremath{\left(X\in B\left(0,r-\epsilon,d\right)\right)}}=\frac{Vol\left(B\left(0,r-\epsilon,d\right)\right)}{Vol\left(B\left(0,r,d\right)\right)}=\left(\frac{r-\epsilon}{r}\right)^{d}
\]

\end_inset


\end_layout

\begin_layout Remark
Como 
\begin_inset Formula $\left(\frac{r-\epsilon}{r}\right)<1$
\end_inset

, 
\begin_inset Formula $\lim_{d\rightarrow\infty}Pr\text{\ensuremath{\left(X\in B\left(0,r-\epsilon,d\right)\right)}}\rightarrow0$
\end_inset

.
 Es decir, a medida que crece la dimensión del soporte de 
\begin_inset Formula $X$
\end_inset

, el 
\begin_inset Quotes eld
\end_inset

interior
\begin_inset Quotes erd
\end_inset

 de la bola esta (casi) vacío, y la distribución de 
\begin_inset Formula $X$
\end_inset

 se concentra en el 
\begin_inset Quotes eld
\end_inset

cascarón
\begin_inset Quotes erd
\end_inset

 exterior.
 Aún para valores moderados de 
\begin_inset Formula $d,\epsilon$
\end_inset

 el efecto es pronunciado.
 Por ejemplo, en 20 dimensiones, un cascarón de 2% de espesor (
\begin_inset Formula $\epsilon=0.02r$
\end_inset

) concentrará 
\begin_inset Formula $1-\text{\left(\tfrac{r-\epsilon}{r}\right)}^{d}=1-0.98^{20}=0.6676\dots\approx\text{¡}2/3$
\end_inset

 de la masa de probabilidad de 
\begin_inset Formula $X$
\end_inset

! 
\end_layout

\begin_layout Remark
Este enorme 
\begin_inset Quotes eld
\end_inset

vacío
\begin_inset Quotes erd
\end_inset

 en el espacio de alta dimensión, se traduce en una irrelevancia de las
 métricas 
\begin_inset Quotes eld
\end_inset

ingenuas
\begin_inset Quotes erd
\end_inset

 de distancia.
 Como 
\begin_inset Formula $x\in B\left(0,r,d\right)\iff\norm x\leq r\sqrt{d}$
\end_inset

, y similarmente 
\begin_inset Formula $x\notin B\left(0,r-\epsilon,d\right)\iff\norm x>\left(r-\epsilon\right)\sqrt{d}$
\end_inset

, podemos escribir 
\begin_inset Formula 
\begin{align*}
Pr\text{\ensuremath{\left(X\notin B\left(0,r-\epsilon,d\right)\right)}} & =Pr\text{\ensuremath{\left(X\notin B\left(0,r-\epsilon,d\right),X\in B\left(0,r,d\right)\right)}}\\
1-\left(\frac{r-\epsilon}{r}\right)^{d} & =Pr\left(\left(r-\epsilon\right)\sqrt{d}<\norm X\leq r\sqrt{d}\right)
\end{align*}

\end_inset


\end_layout

\begin_layout Remark
De manera que 
\begin_inset Formula $\lim_{d\rightarrow\infty}Pr\left(\left(r-\epsilon\right)\sqrt{d}<\norm X\leq r\sqrt{d}\right)\rightarrow1$
\end_inset

.
 Es decir, a medida que 
\begin_inset Formula $d\rightarrow\infty$
\end_inset

 y para 
\begin_inset Formula $\epsilon$
\end_inset

 arbitrariamente pequeño, la distancia euclídea de (casi) toda la distribución
 al centro de la esfera tiende a ser aproximadamente 
\begin_inset Formula $r\sqrt{d}$
\end_inset

, lo cual hace que esta distancia euclídea sea inútil para diferenciar entre
 elementos de la muestra.
\end_layout

\begin_layout Subsection
Reducción de dimensionalidad y la hipótesis de la variedad
\end_layout

\begin_layout Standard
A pesar de lo sorprendente del resultado, vale notar que descansa sobre
 el hecho de que la distribución de 
\begin_inset Formula $X$
\end_inset

 sobre su soporte 
\begin_inset Formula $\text{supp}\left(X\right)=B\left(0,r,d\right)\subset\R^{d}$
\end_inset

 es uniforme, e independiente en todas las dimensiones.
 En casi cualquier contexto material, este supuesto no es sostenible.
 Por poner un ejemplo, podemos representar todas las posibles imágenes en
 escala de grises de 1 megapixel como puntos 
\begin_inset Formula $X$
\end_inset

 pertenecientes al espacio 
\begin_inset Formula $\R^{1024\times1024}$
\end_inset

, pero la basta mayoría de ellas consistirían en 
\begin_inset Quotes eld
\end_inset

puro ruido blanco
\begin_inset Quotes erd
\end_inset

 y no significarían nada para un observador.
 Las imágenes que sí tiene sentido reconocer y clasificar (un gato, una
 bicicleta, etc.) son un conjunto muchísimo más restringido - aún teniendo
 en cuenta todo tipo de posiciones y contrastes posibles -, y sus diferentes
 elementos (como la posición de los ojos y las orejas del gato) guardan
 relaciones específicas entre sí.
 Es decir, están 
\emph on
correlacionados
\emph default
.
\end_layout

\begin_layout Standard
Si nos suponemos en esta situación, el camino más directo para aliviarla,
 es 
\emph on
reducir la dimensionalidad
\emph default
 del problema.
 Al fin y al cabo, es el crecimiento en 
\begin_inset Formula $d$
\end_inset

 lo que nos embrolló en un principio.
 Dadas 
\begin_inset Formula $\left\{ \mathbf{x}\right\} =\left\{ x_{i}\vert x_{i}\in\Rdimx,\ i\in\text{\left[N\right]}\right\} $
\end_inset

, buscaremos una 
\emph on
representación
\emph default
 
\begin_inset Formula $f:\Rdimx\rightarrow\R^{d_{y}}$
\end_inset

, que preserve fielmente los atributos más relevantes de 
\begin_inset Formula $x\in\Rdimx$
\end_inset

, en la menor cantidad de dimensiones 
\begin_inset Formula $d_{y}$
\end_inset

.
 Encontrar compromisos ideales entre la 
\begin_inset Quotes eld
\end_inset

fidelidad
\begin_inset Quotes erd
\end_inset

 y la dimensionalidad de estas representaciones, dió lugar al campo de 
\emph on
aprendizaje de representaciones, 
\emph default
del cual Bengio2012
\begin_inset Foot
status collapsed

\begin_layout Plain Layout
https://www.reddit.com/r/MachineLearning/comments/mzjshl/comment/gwq8szw/?utm_sour
ce=share&utm_medium=web2x&context=3 Bengio himself sobre el origen del término.
\end_layout

\end_inset

 hace un excelente censo.
 El autor relaciona la tarea del área con la noción geométrica de una 
\emph on
variedad.
\end_layout

\begin_layout Remark
A nuestros fines, una variedad 
\begin_inset Formula $\mathcal{\M}$
\end_inset

 es un espacio de dimensión 
\begin_inset Formula $d_{\M}$
\end_inset

 que 
\emph on
localmente
\emph default
, se asemeja a 
\begin_inset Formula $\R^{d_{\M}}$
\end_inset

.
 En efecto, una variedad puede ser vista como un objeto compuesto de parches
 
\begin_inset Formula $d_{\M}$
\end_inset

-dimensionales pegados.
 Una variedad se llama 
\emph on
cerrada
\emph default
 si no tiene borde y es compacta.
\end_layout

\begin_layout Standard
La 
\emph on
hipótesis de la variedad
\emph default
 (
\begin_inset Quotes eld
\end_inset

manifold hypothesis
\begin_inset Quotes erd
\end_inset

) postula que los datos 
\series bold

\begin_inset Formula $x$
\end_inset


\series default
 obtenidos del mundo real con alta dimensionalidad 
\begin_inset Formula $d_{x}$
\end_inset

 habrían de concentrarse en una variedad 
\begin_inset Formula $\M$
\end_inset

 de -potencialmente - mucha menor dimensionalidad 
\begin_inset Formula $d_{\M}\ll d_{x}$
\end_inset

, embebido en el espacio original 
\begin_inset Formula $\R^{d_{x}}$
\end_inset

.
 
\end_layout

\begin_layout Quotation
Esta asunción parece particularmente adecuada en tareas de aprendizaje para
 las cuales las configuraciones muestreadas aleatoriamente no son como las
 que ocurren naturalmente: ya mencionamos imágenes, pero esperamos lo mismo
 de sonidos, texto, secuencias genómicas y hasta aún las respuestas a algunos
 cuestionarios inverosímilmente exhaustivos de los departamentos estatales
 de estadística.
 
\end_layout

\begin_layout Quotation
Ni bien tenemos una 
\emph on
representación,
\emph default
 uno piensa en una variedad considerando las variaciones en el dominio original
 que están bien capturadas o reflejadas (por correspondientes cambios) en
 la representación aprendida.
 A 
\emph on
grosso modo
\emph default
, algunas direcciones estarán bien preservadas (las direcciones 
\emph on
localmente tangentes
\emph default
 a cada punto en la variedad), mientras que otras se perderán - las ortogonales
 a 
\begin_inset Formula $\M$
\end_inset

 .
 Desde esta perspectiva, la principal tarea del aprendizaje no-supervisado,
 puede ser vista como el modelado de la estructura de la variedad que soporta
 los datos observados.
 La representación que se aprenda, puede asociarse a un sistema intrínseco
 de coordenadas en la variedad embebida.
 El algoritmo arquetípico de modelado de variedades es, oh sorpresa, también
 el algoritmo arquetípico de aprendizaje de representaciones de baja dimensional
idad: Análisis de Componenetes Principales (PCA).
\end_layout

\begin_layout Quotation
PCA modela una 
\emph on
variedad lineal.
 
\emph default
Fue inicialmente diseñado con el objetivo de encontrar la variedad lineal
 más cercana a una nube de puntos.
 Las componentes principales, i.e., la representación 
\begin_inset Formula $f_{\theta}\left(x\right)$
\end_inset

 que devuelve PCA para un input 
\begin_inset Formula $x$
\end_inset

, ubica unívocamente su proyección en esa variedad: se corresponde con coordenad
as intrínsecas de la variedad.
 Las variedad que soportan dominios complejos del mundo real, sin embargo,
 se esperan que sean fuertemente no-lineales.
\end_layout

\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Standard
Más que una propiamente dicha hipótesis falsificable al respecto de la distribuc
ión de los datos, mencionamos la 
\emph on
hipótesis de la variedad
\emph default
 en tanto resulta modelo mental útil para entender cómo estimar la densidad
 generadora de los datos en altas dimensiones.
 Ya mencionamos que a medida que 
\begin_inset Formula $d_{x}$
\end_inset

 crece, la distancia euclídea en 
\begin_inset Formula $\R^{\dimx}$
\end_inset

 se vuelve menos informativa.
 Trabajar dentro de 
\begin_inset Formula $\M$
\end_inset

, con dimensión 
\begin_inset Formula $d_{\M}$
\end_inset

 puede aliviar la situación sobre todo cuando 
\begin_inset Formula $d_{\M}\ll d_{x}$
\end_inset

, pero hay una ventaja más escondida en el hecho de que una variedad es
 sólo localmente semejante al espacio euclídeo - es decir, 
\emph on
lineal
\emph default
 -, pero puede 
\begin_inset Quotes eld
\end_inset

arrugarse
\begin_inset Quotes erd
\end_inset

 en el espacio ambiente.
\end_layout

\begin_layout Standard
Imaginemos un conjunto de datos 
\begin_inset Formula $\left\{ \mathbf{u}\right\} =\left\{ u_{i},i\in\left[N\right],u_{i}\in\mathcal{U}\subseteq\R^{2}\right\} $
\end_inset

, con forma de letra 
\begin_inset Quotes eld
\end_inset

U
\begin_inset Quotes erd
\end_inset

, justamente.
 
\begin_inset Formula $\mathcal{U}$
\end_inset

 es una variedad 1-dimensional - una curva - embebida en el espacio cartesiano
 - 
\begin_inset Formula $\R^{2},$
\end_inset

 una variedad 2-dimensional., Llamemos 
\begin_inset Formula $u_{\alpha},u_{\omega}$
\end_inset

 a los dos puntos que se encuentran más cerca de los extremos superiores
 del dibujo de la 
\begin_inset Quotes eld
\end_inset

U
\begin_inset Quotes erd
\end_inset

.
 En la variedad latente, estos dos puntos están tan separados entre sí como
 es posible; sin embargo, si medimos la distancia entre ambos en el espacio
 ambiente - 
\begin_inset Formula $\R^{2}$
\end_inset

 - obtendremos que están mucho más cerca entre sí que, por ejemplo, el punto
 medio donde la 
\begin_inset Quotes eld
\end_inset

U
\begin_inset Quotes erd
\end_inset

 corta su eje de simetría axial.
 La razón de tal insensatez, es simplemente, que hemos tomado una medida
 de distancia que no se ajusta bien al espacio latente.
\end_layout

\begin_layout Subsection
KDE en variedades
\end_layout

\begin_layout Standard
¡Excelente! Fieles a la hipótesis de la variedad, podemos sugerir un camino
 alternativo a los complejos derroteros por los que nos llevó de paseo KDE
 multivariado en alta dimensión: en lugar de calcular un KDE en el espacio
 ambiente 
\begin_inset Formula $\R^{d_{x}},$
\end_inset

 hipotetizamos que 
\begin_inset Formula $X\in\M\subseteq\R^{d_{x}},\dim\M=d_{\M}\ll d_{x}$
\end_inset

, y por lo tanto podemos restringir la definición de su densidad 
\begin_inset Formula $f:\M\rightarrow\left(0,\infty\right)$
\end_inset

 para obtener una mejor representación.
 Pero: ¿cómo se construye una función de densidad 
\emph on
en una variedad
\emph default
? Algunas variedades particularmente interesantes, como el círculo 
\begin_inset Formula $S^{1}$
\end_inset

 y la esfera 
\begin_inset Formula $S^{2}$
\end_inset

, fueron estudiadas temprano en el siglo XX (Rao, Fisher, citar bien), pero
 la estimación de densidad en variedades arbitrarias no parece haber sido
 tratado antes que en Pelletier2005, quien - convenientemente - hizo exactamente
 eso, 
\begin_inset Quotes eld
\end_inset

Kernel density estimation on Riemannian Manifolds
\begin_inset Quotes erd
\end_inset

.
 En lo que sigue, (intentamos) ser fieles a lo que entendimos de la exposición
 de Bruno.
 
\end_layout

\begin_layout Definition
\begin_inset CommandInset label
LatexCommand label
name "def:manif-kde"

\end_inset

(estimación de densidad por núcleos en variedades, Pelletier2005, seccion
 2; Muñoz2011 en su tesis de lic.
 con directores Henry y Rodriguez)
\end_layout

\begin_layout Definition
Sea 
\begin_inset Formula $\left(\M,g\right)$
\end_inset

 una variedad Riemanniana compacta sin frontera de dimensión 
\begin_inset Formula $d$
\end_inset

.
 Asumiremos que 
\begin_inset Formula $\left(\M,g\right)$
\end_inset

 es completo, es decir, 
\begin_inset Formula $\left(\M,d_{g}\right)$
\end_inset

 es un espacio métrico completo, donde 
\begin_inset Formula $d_{g}$
\end_inset

 denota la distancia de Riemann.
\end_layout

\begin_layout Definition
Sea X un elemento aleatorio en 
\begin_inset Formula $\M$
\end_inset


\begin_inset Foot
status collapsed

\begin_layout Plain Layout
i.e., un mapa medible en un espacio de probabilidad 
\begin_inset Formula $\left(\Omega,\mathcal{A},P\right)$
\end_inset

 que toma valores en 
\begin_inset Formula $\left(\M,\mathcal{B}\right),$
\end_inset

donde 
\begin_inset Formula $\mathcal{B}$
\end_inset

 representa el 
\begin_inset Formula $\sigma$
\end_inset

-campo de Borel de 
\begin_inset Formula $\M$
\end_inset

.
 Asumiremos que la medida imagen de 
\begin_inset Formula $P$
\end_inset

 por 
\begin_inset Formula $X$
\end_inset

 es absolutamente continua con respecto a la medida Riemanniana de volumen
 - que notaremos 
\begin_inset Formula $v_{g}$
\end_inset

 -, admitiendo una densidad 
\begin_inset Formula $f$
\end_inset

 continua en c.t.p.
 sobre 
\begin_inset Formula $\M$
\end_inset

.
\end_layout

\end_inset

 con densidad 
\begin_inset Formula $f$
\end_inset

 continua en casi todo punto.
 Sea 
\begin_inset Formula $\text{\left\{  \mathbf{X}\right\}  }$
\end_inset

 un conjunto de 
\begin_inset Formula $N$
\end_inset


\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\xout off
\uuline off
\uwave off
\noun off
\color none
 elementos aleatorios i.i.d.

\family default
\series default
\shape default
\size default
\emph default
\bar default
\strikeout default
\xout default
\uuline default
\uwave default
\noun default
\color inherit
 a X.
 Sea 
\begin_inset Formula $K:\R_{+}\rightarrow\R$
\end_inset

 un mapa no-negativo tal que
\end_layout

\begin_deeper
\begin_layout Enumerate
\begin_inset Formula $\int_{\R^{d}}K\text{\left(\norm x\right)d\lambda\left(x\right)}=1$
\end_inset

 (
\begin_inset Formula $K$
\end_inset

 es una función de densidad)
\end_layout

\begin_layout Enumerate
\begin_inset Formula $\int_{\R^{d}}xK\text{\left(\norm x\right)d\lambda\left(x\right)}=0$
\end_inset

 (
\begin_inset Formula $EX=0$
\end_inset

, 
\begin_inset Formula $K$
\end_inset

 es simétrica),
\end_layout

\begin_layout Enumerate
\begin_inset Formula $\int_{\R^{d}}\norm x^{2}K\text{\left(\norm x\right)d\lambda\left(x\right)}<\infty$
\end_inset

 (
\begin_inset Formula $VarX<\infty),$
\end_inset


\end_layout

\begin_layout Enumerate
\begin_inset Formula $\text{sop}K=\text{\left[0,1\right]}$
\end_inset

,
\end_layout

\begin_layout Enumerate
\begin_inset Formula $\sup K\text{\left(x\right)}=K\left(0\right)$
\end_inset

,
\end_layout

\end_deeper
\begin_layout Definition
donde 
\begin_inset Formula $\lambda$
\end_inset

 es la medida de Lebesgue en 
\begin_inset Formula $R^{d}.$
\end_inset

 Luego, el mapa 
\begin_inset Formula $\R^{d}\ni x\rightarrow K\left(\norm x\right)\in\R$
\end_inset

 es un núcleo isotrópico en 
\begin_inset Formula $\R^{d}$
\end_inset

 con soporte en la bola unitaria.
\end_layout

\begin_layout Definition
Sean 
\begin_inset Formula $p,q$
\end_inset

 dos puntos de 
\begin_inset Formula $\M$
\end_inset

.
 Sea 
\begin_inset Formula $\theta_{p}\left(q\right)$
\end_inset

 la 
\emph on
función de densidad volumétrica
\emph default
 en 
\begin_inset Formula $\M$
\end_inset


\begin_inset Foot
status open

\begin_layout Plain Layout
Besse 1978 (p.
 154) lo define aproximadamente como
\begin_inset Formula 
\[
\theta_{p}:q\rightarrow\theta_{p}\left(q\right)=\frac{\mu_{\exp_{p}^{*}g}}{\mu_{g_{p}}}\left(\exp_{p}^{-1}\left(q\right)\right)
\]

\end_inset


\end_layout

\begin_layout Plain Layout
i.e., el cociente entre la medida canónica de la métrica Riemanniana 
\begin_inset Formula $\exp_{p}^{*}g$
\end_inset

 en espacio tangente 
\begin_inset Formula $T_{p}\left(M\right)$
\end_inset

, y la medida de Lebesgue en la estructura euclídea 
\begin_inset Formula $g_{p}$
\end_inset

en 
\begin_inset Formula $T_{p}\left(M\right).$
\end_inset

 La función de densidad volumétrica está ciertamente definida para 
\begin_inset Formula $q$
\end_inset

 en un vecindario de 
\begin_inset Formula $p$
\end_inset

.
 En términos de coordenadas normales geod´sicas en 
\begin_inset Formula $p$
\end_inset

, 
\begin_inset Formula $\theta_{p}\left(q\right)$
\end_inset

 es igual al determinante de la métrica 
\begin_inset Formula $g$
\end_inset

 expresado dichas estas coordenadas en 
\begin_inset Formula $\exp_{P}^{-1}\left(q\right)$
\end_inset

.
\end_layout

\end_inset

.
 Definimos el estimador de densidad de 
\begin_inset Formula $f$
\end_inset

 como el mapa 
\begin_inset Formula $f_{N,K}:\M\rightarrow\R$
\end_inset

 que a cada 
\begin_inset Formula $p\in\M$
\end_inset

 le asocia el valor 
\begin_inset Formula $f_{N,K}\left(p\right)$
\end_inset

 definido como
\begin_inset Formula 
\[
f_{N,K}\left(p\right)=N^{-1}\sum_{i=1}^{N}\frac{1}{h^{d}}\frac{1}{\theta_{X_{i}}\left(p\right)}K\left(\frac{d_{g}\left(p,X_{i}\right)}{h}\right)
\]

\end_inset


\end_layout

\begin_layout Remark
(concordancia con espacios euclídeos) Sea 
\begin_inset Formula $\M=\R^{d}$
\end_inset

 con su típica métrica euclídea.
 Luego, 
\begin_inset Formula $\theta_{p}\left(q\right)=1\ \forall\ p,q\in\M$
\end_inset

 y 
\begin_inset Formula $f_{N,K}$
\end_inset

 se puede escribir como 
\begin_inset Formula $f_{N,K}=N^{-1}\sum_{i=1}^{N}r^{-d}K\left(\norm{p-X_{i}}/r\right)$
\end_inset

.
 La expresión de 
\begin_inset Formula $f_{N,K}$
\end_inset

 es consistente con la expresión de KDEs en el caso euclídeo.
\end_layout

\begin_layout Standard
Para asegurarse de que 
\begin_inset Formula $f_{N,K}$
\end_inset

 sea integrable sobre 
\begin_inset Formula $\M$
\end_inset

, habremos de imponer una restricción más sobre el ancho de banda: 
\begin_inset CommandInset label
LatexCommand label
name "constr:inj-radius"

\end_inset


\begin_inset Formula 
\[
h_{n}<h_{0}<\text{inj}_{g}\M
\]

\end_inset

, donde 
\begin_inset Formula $\text{inj}_{g}\M$
\end_inset

 es el 
\emph on
radio de inyectividad
\emph default
 de 
\begin_inset Formula $\M$
\end_inset

 (Chavel1993, p.
 108; Muñoz2011, p.
 23)
\begin_inset Foot
status collapsed

\begin_layout Definition
(Muñoz2011, p.
 23, definición 3.3.16) Sea 
\begin_inset Formula $(\M,g)$
\end_inset

 una variedad Riemanniana de dimensión 
\begin_inset Formula $d$
\end_inset

.
 Llamamos 
\size small
\emph on
radio de inyectividad
\size default
\emph default
 a 
\begin_inset Formula 
\[
\text{inj}_{g}\M=\inf_{p\in\M}\sup\left\{ s\in\R>0:B\left(p,s\right)\t{es\ una\ bola\ normal}\right\} 
\]

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Plain Layout
Burdamente, diremos que 
\begin_inset Formula $B$
\end_inset

 es una 
\size small
\emph on
bola normal
\size default
\emph default
 centrada en 
\emph on
p
\emph default
 si existe una bola 
\begin_inset Formula $V$
\end_inset

 en 
\begin_inset Formula $T_{p}(\M)$
\end_inset

 (un vecindario de 
\begin_inset Formula $p$
\end_inset

) en el que las coordenadas de cada punto 
\begin_inset Formula $q\in V$
\end_inset

 se pueden mapear biyectivamente a coordenadas en 
\begin_inset Formula $\R^{d}$
\end_inset

: por ejemplo, si 
\begin_inset Formula $\M_{1}=\R^{d}$
\end_inset

 con la métrica canónica (
\begin_inset Formula $\norm{\cdot}$
\end_inset

) entonces 
\begin_inset Formula $\t{inj}_{g}\M_{1}=\infty$
\end_inset

, pues todo el espacio comparte un único mapa de coordenadas global.
 Si le quitamos un punto, 
\begin_inset Formula $\M_{2}=\M_{1}-\left\{ p\right\} $
\end_inset

 entonces 
\begin_inset Formula $\t{inj}_{g}\M_{2}=0$
\end_inset

 (para un punto 
\begin_inset Quotes eld
\end_inset

muy cercano a 
\begin_inset Formula $p$
\end_inset


\begin_inset Quotes erd
\end_inset

, 
\begin_inset Formula $q\in\M,q\approx p$
\end_inset

, no habrá bola normal posible) .
 Si 
\begin_inset Formula $\M=S^{1}\times\R$
\end_inset

 (un cilindro vacío en 
\begin_inset Formula $\R^{3}$
\end_inset

)  con la métrica inducida de 
\begin_inset Formula $\R^{3}$
\end_inset

, el radio de inyectividad es 
\begin_inset Formula $\pi$
\end_inset

.
 REPASAR.
\end_layout

\end_inset

.
 Sin entrar en demasiados detalles, siempre y cuando 
\begin_inset Formula $\M$
\end_inset

 sea compacta este radio de inyectividad será 
\begin_inset Formula $>0$
\end_inset

, y al menos para los resultados asintóticos (cuando el tamaño muestral
 es lo suficientemente grande como para que 
\begin_inset Formula $h\rightarrow0$
\end_inset

), 
\begin_inset Formula $0<h<h_{0}$
\end_inset

.
\end_layout

\begin_layout Standard
Pelletier2005 avanza algunas propiedades elementales de este estimador:
 adapta el concepto de 
\begin_inset Quotes eld
\end_inset

media
\begin_inset Quotes erd
\end_inset

 para elementos aleatorios en 
\begin_inset Formula $\R^{d}$
\end_inset

 a e.a.
 en variedades Riemannianas compactas sin frontera 
\begin_inset Formula $\M$
\end_inset

 (Proposición II, 
\begin_inset Quotes eld
\end_inset

media intrínseca
\begin_inset Quotes erd
\end_inset

), y prueba que 
\begin_inset Formula $f_{N,K}$
\end_inset

 es un estimador consistente 
\begin_inset Foot
status open

\begin_layout Plain Layout
Pelletier considera la convergencia en 
\begin_inset Formula $L^{2}\left(\M\right),$
\end_inset

 ¿esto sería consistencia débil? ¿Qué diferencia hay con la que estudian
 Henry&Rodríguez2009?
\end_layout

\end_inset

 de 
\begin_inset Formula $f$
\end_inset

 (Teorema 5), en el siguiente sentido
\end_layout

\begin_layout Theorem
(Teorema 5, Pelletier 2005) 
\emph on
Sea 
\begin_inset Formula $f$
\end_inset

 una densidad de probabilidad dos veces diferenciable en 
\begin_inset Formula $\M$
\end_inset

 con segunda derivada covariante acotada.
 Sea 
\begin_inset Formula $f_{N,K}$
\end_inset

 su estimador definido en 
\begin_inset CommandInset ref
LatexCommand ref
reference "def:manif-kde"
plural "false"
caps "false"
noprefix "false"

\end_inset

 con ancho de banda 
\begin_inset Formula $h$
\end_inset

 que satisface la condición 
\begin_inset CommandInset ref
LatexCommand ref
reference "constr:inj-radius"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
 Luego, existe una constante 
\begin_inset Formula $C_{f}$
\end_inset

 tal que 
\end_layout

\begin_layout Theorem
\begin_inset Formula 
\[
E_{f}\norm{f_{N,K}-f}_{L^{2}\left(\M\right)}^{2}\leq C_{f}\left(\frac{1}{Nh^{d}}+h^{4}\right)
\]

\end_inset


\end_layout

\begin_layout Theorem

\emph on
En consecuencia, para 
\begin_inset Formula $h\sim N^{\frac{-1}{d+4}}$
\end_inset

, 
\emph default

\begin_inset Formula 
\[
E_{f}\norm{f_{N,K}-f}_{L^{2}\left(\M\right)}^{2}\leq\text{O}\left(N^{\frac{-4}{d+4}}\right)
\]

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Standard
Henry & Rodríguez2009 continúan el estudio de este estimador, probando
\end_layout

\begin_layout Enumerate
bajo ciertas condiciones de regularidad sobre conjuntos compactos 
\begin_inset Formula $\M_{0}\subseteq\M$
\end_inset

 - la consistencia fuerte
\begin_inset Formula 
\[
\sup_{p\in\M_{0}}\vert f_{n,K}\left(p\right)-f\left(p\right)\vert\xrightarrow{c.t.p.}0
\]

\end_inset


\end_layout

\begin_layout Enumerate
bajo condiciones extras sobre 
\begin_inset Formula $f$
\end_inset

 y la serie 
\begin_inset Formula $h_{n}$
\end_inset

, 
\begin_inset Formula $f-f_{N,K}$
\end_inset

 converge en distribución a ciert ley normal, con tasa 
\begin_inset Formula $\sqrt{nh^{d}}$
\end_inset


\begin_inset Formula 
\[
\sqrt{nh^{d}}\left(f\left(p\right)-f_{n,K}\left(p\right)\right)\xrightarrow{\mathcal{D}}\mathcal{N}\left(\mu,\Sigma\right)
\]

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Standard
Loubes y Pelletier (2010) extienden el trabajo de Pelletier 2005, proponiendo
 un clasificador binario basado en núcleos, para e.a.
 soportados sobre variedades compactas y cerradas de Riemann.
 Recordemos que en 
\begin_inset CommandInset ref
LatexCommand ref
reference "def:soft-clf-kde"
plural "false"
caps "false"
noprefix "false"

\end_inset

 propusimos un clasificador suave que asignase a cada clase, una probabilidad
 de pertenencia 
\begin_inset Formula 
\[
p\left(C\left(x\right)=j\right)=\frac{f^{(j)}\left(x\right)\cdot p\text{\ensuremath{\left(C_{j}\right)}}}{p\text{\ensuremath{\left(x\right)}}}
\]

\end_inset


\end_layout

\begin_layout Standard
de manera que podemos describir una reglas de clasificación dura, como
\begin_inset Formula 
\begin{align*}
\mathcal{R}\left(x\vert f_{1},\dots,f_{K}\right) & =\arg\max_{j\in\left[K\right]}p\left(C\left(x\right)=j\right)\\
 & =\arg\max_{j\in\left[K\right]}\frac{f^{(j)}\left(x\right)\cdot p\text{\ensuremath{\left(C_{j}\right)}}}{\sum_{j\in\left[K\right]}f^{(j)}\left(x\right)\cdot p\text{\ensuremath{\left(C_{j}\right)}}}\\
 & \arg\max_{j\in\left[K\right]}f^{(j)}\left(x\right)\cdot p\text{\ensuremath{\left(C_{j}\right)}}
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
y su estimador muestral
\begin_inset CommandInset label
LatexCommand label
name "def:manif-clf-kde"

\end_inset


\begin_inset Formula 
\begin{align*}
\mathcal{\hat{R}}\left(x\vert\hat{f}_{1},\dots,\hat{f}_{K}\right) & =\arg\max_{j\in\left[K\right]}\hat{f}^{(j)}\left(x\right)\cdot\hat{p}\left(C_{j}\right)\\
 & =\arg\max_{j\in\left[K\right]}N_{j}^{-1}\sum_{i=1}^{N}\frac{1}{h^{d}}\frac{1}{\theta_{X_{i}}\left(p\right)}K\left(\frac{d_{g}\left(p,X_{i}\right)}{h}\right)\cdot\frac{N_{j}}{N}\\
 & =\arg\max_{j\in\left[K\right]}\sum_{i=1}^{N}\mathbf{1}\left\{ C\left(X_{i}\right)=j\right\} K_{h}\left(p,X_{i}\right)
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
donde 
\begin_inset Formula 
\[
K_{h}\left(p,X_{i}\right)=\frac{1}{h^{d}}\frac{1}{\theta_{X_{i}}\left(p\right)}K\left(\frac{d_{g}\left(p,X_{i}\right)}{h}\right)
\]

\end_inset


\end_layout

\begin_layout Standard
Este es, precisamente, el clasificador que Loubes y Pelletier proponen.
 Considerando como función objetivo a minimizar la misma probabilidad de
 error de clasificación que vimos con Hall2005, 
\begin_inset Formula 
\[
L\left(\mathcal{R}\right)=Pr\left(\mathcal{R}\left(X\right)=C\left(X\right)\right)
\]

\end_inset


\end_layout

\begin_layout Standard
los autores muestran que el clasificador propuesto 
\begin_inset Formula $\mathcal{\hat{R}}$
\end_inset

 alcanza asintóticamente la misma pérdida que el clasificador óptimo de
 bayes, 
\begin_inset Formula $\mathcal{R}^{*}$
\end_inset


\begin_inset Formula 
\[
\lim_{n\rightarrow\infty}Pr\left(\hat{L\left(\mathcal{R}_{n}\right)}=L\left(\mathcal{R^{*}}\right)\right)=1
\]

\end_inset

con 
\begin_inset Formula 
\[
\text{\calR}^{*}\left(x\right)=\arg\max_{j\in\left[K\right]}Pr\left(C\left(X\right)=j\vert X=x\right)
\]

\end_inset


\end_layout

\begin_layout Standard
Siguiendo a Drevoye1996, diremos que el clasificador es 
\emph on
fuertemente consistente
\emph default
, en tanto alcanza el error de Bayes cuando 
\begin_inset Formula $n\rightarrow\infty$
\end_inset

.
 Los autores dejan las consideraciones prácticas de su funcionamiento fuera
 del trabajo.
\end_layout

\begin_layout Standard
Los resultados combinados de Pelletier, Henry, Rodríguez y Loubes nos dejan
 bastante cerca de lo que venimos buscando - construir un clasificador basado
 en densidades -, con una diferencia fatal: estos trabajos consideran variedades
 
\emph on
conocidas
\emph default
, mientras que nosotros trabajamos bajo la 
\emph on
hipótesis de la variedad
\emph default
, pero en principio no conocemos la variedad en sí.
 Crucialmente, desconocer la variedad implica desconocer dos cosas: la distancia
 geodésica 
\begin_inset Formula $d_{g}$
\end_inset

 y la función de densidad de volumen 
\begin_inset Formula $\theta_{p}$
\end_inset

, que tendremos que 
\emph on
aprender de los datos
\emph default
 de alguna manera, junto con la densidad 
\begin_inset Formula $f$
\end_inset

.
\end_layout

\begin_layout Subsubsection
Isomap
\end_layout

\begin_layout Standard
Consideraremos brevemente uno de los algoritmos más famosos de aprendizaje
 de variedades no-lineales: Isomap
\begin_inset Foot
status open

\begin_layout Plain Layout

\series bold
Iso
\series default
metric feature 
\series bold
map
\series default
ping, en inglés
\end_layout

\end_inset

.
 Según Tenenbaum2000, el objetivo explícito de su algoritmo es 
\begin_inset Quotes eld
\end_inset

aprender la geometría global subyacente de un dataset, usando información
 métrica local fácilmente medible
\begin_inset Quotes erd
\end_inset

, para un conjunto amplio de variedades no-lineales.
 La tarea central, consiste en aproximar adecuadamente las distancias geodésicas
 en la variedad 
\begin_inset Formula $d_{g}\left(x,y\right)$
\end_inset

 entre puntos alejados, conociendo únicamente las distancias euclídeas en
 la muestra 
\begin_inset Formula $\norm{x-y}$
\end_inset

.
\end_layout

\begin_layout Standard
Dada la naturaleza localmente euclídea de las variedades, para puntos 
\begin_inset Quotes eld
\end_inset

vecinos
\begin_inset Quotes erd
\end_inset

 entre sí, la distancia en 
\begin_inset Formula $\R^{d_{x}}$
\end_inset

 (en el espacio de las 
\begin_inset Formula $X$
\end_inset

) será una aproximación razonable a la distancia geodésica en la variedad.
 Luego, para puntos alejados entre sí, podemos aproximar 
\begin_inset Formula $d_{g}$
\end_inset

 como la suma de una secuencia de 
\begin_inset Quotes eld
\end_inset

pequeños saltos
\begin_inset Quotes erd
\end_inset

 entre puntos vecinos en el grafo de la muestra.
\end_layout

\begin_layout Standard
El algoritmo completo, consta de tres pasos principales (Tabla 1, Tenenbaum2000)
:
\end_layout

\begin_layout Enumerate

\series bold
Constrúyase un grafo de vecinos muestrales
\series default
 
\begin_inset Formula $G=\text{\left(X,E\right)}$
\end_inset

 sobre el dataset completo, donde la arista 
\begin_inset Formula $x\leftrightarrow y$
\end_inset

 está incluida si 
\begin_inset Formula $\norm{x-y}_{d_{x}}<\epsilon$
\end_inset

 (
\begin_inset Quotes eld
\end_inset


\begin_inset Formula $\epsilon$
\end_inset

-Isomap
\begin_inset Quotes erd
\end_inset

), o si 
\begin_inset Formula $y$
\end_inset

 es uno de los 
\begin_inset Formula $K$
\end_inset

 vecinos más cercanos de 
\begin_inset Formula $x$
\end_inset

 .
 Tómese 
\begin_inset Formula $\norm{x-y}$
\end_inset

 como el valor de la arista 
\begin_inset Formula $x\leftrightarrow y$
\end_inset

.
\end_layout

\begin_layout Enumerate

\series bold
Compútense los caminos mínimos
\series default
, usando - según convenga - el algoritmo de Floyd-Warshall o Dijsktra en
 el grafo 
\begin_inset Formula $G$
\end_inset

.
 Los costos de los caminos mínimos 
\begin_inset Formula $d_{G}\left(x,y\right)$
\end_inset

 constituyen una aproximación de las distnacias geodésicas 
\begin_inset Formula $d_{\M}\left(x,y\right)$
\end_inset

 .
\end_layout

\begin_layout Enumerate

\series bold
Constrúyase un 
\emph on
embedding
\emph default
 
\begin_inset Formula $\mathbf{d-}$
\end_inset

dimensional.
 
\series default
Utilizando escalamiento multidimensional (MDS, MultiDimensional Scaling,
 un algoritmo de reducción de dimensionalidad), crear una representación
 (embedding) en el espacio euclídeo 
\begin_inset Formula $\R^{d}$
\end_inset

 que minimice una métrica de discrepancia entre las distancias computadas
 en (2), con las distancias en la representación a construir, llamada 
\begin_inset Quotes eld
\end_inset

stress
\begin_inset Quotes erd
\end_inset

 o 
\begin_inset Quotes eld
\end_inset

strain
\begin_inset Quotes erd
\end_inset

.
\end_layout

\begin_layout Standard
Los resultados de este algoritmo - que han sido bastante espectaculares
 para lo relativamente sencillo de su estructura, descansan en una prueba
 de la convergencia asintótica, a medida que 
\begin_inset Formula $N$
\end_inset

 crece, de que las distancias en el grafo 
\begin_inset Formula $d_{G}$
\end_inset

 proveen aproximaciones incrementalmente mejores a las distancias geodésicas
 intrínsecas 
\begin_inset Formula $d_{\M}$
\end_inset

, volviéndose arbitrariamente precisas en el límite de 
\begin_inset Formula $N\rightarrow\infty$
\end_inset

.
 La tasa a la que esta convergencia sucede, depende de ciertos parámetros
 de la variedad (su dimensión 
\begin_inset Formula $d_{\M}$
\end_inset

, la función de volumen 
\begin_inset Formula $\theta_{p}$
\end_inset

), de cómo esta yace en el espacio ambiente (radio de curvatura 
\begin_inset Formula $r_{0}$
\end_inset

 y separación de ramas
\begin_inset Formula $s_{0}$
\end_inset

) y de la densidad 
\begin_inset Formula $f:\M\rightarrow\R>0$
\end_inset

 de la que estamos sampleando.
\end_layout

\begin_layout Standard
Allende los costos computacionales, hay dos parámetros a fijar en este algoritmo
: el parámetro de 
\begin_inset Quotes eld
\end_inset

vecindad
\begin_inset Quotes erd
\end_inset

 
\begin_inset Formula $\epsilon$
\end_inset

 ó 
\begin_inset Formula $K$
\end_inset

 en (1), y la dimensión 
\begin_inset Formula $d$
\end_inset

 en (3).
 Inspeccionando el gráfico de 
\begin_inset Quotes eld
\end_inset

estrés
\begin_inset Quotes erd
\end_inset

 de MDS como función de la dimensión 
\begin_inset Formula $d$
\end_inset

 escogida, se pueden buscar un punto de inflexión (
\begin_inset Quotes eld
\end_inset

codo
\begin_inset Quotes erd
\end_inset

) en que seguir aumentando 
\begin_inset Formula $d$
\end_inset

 no aliviana significativamente la tensión del algoritmo, y son por tanto
 candidatos naturales a la dimensión intrínseca de la variedad 
\begin_inset Formula $d_{\M}$
\end_inset

.
\end_layout

\begin_layout Standard
Por su parte, el valor óptimo de 
\begin_inset Formula $\epsilon$
\end_inset

 ó 
\begin_inset Formula $K$
\end_inset

 no cuenta con una regla inmediata para su determinación.
 Consideremos 
\begin_inset Formula $\epsilon-$
\end_inset

Isomap: valores demasiado pequeños de 
\begin_inset Formula $\epsilon$
\end_inset

 podrían dejar muchos vértices de 
\begin_inset Formula $G$
\end_inset

 - muchas observaciones muestrales - desconectadas de la componente gigante
 - la componente conexa de mayor tamaño - de 
\begin_inset Formula $G;$
\end_inset

valores demasiado grandes de 
\begin_inset Formula $\epsilon$
\end_inset

 podrían llevar a incluir en 
\begin_inset Formula $G$
\end_inset

 aristas 
\begin_inset Formula $e\in E$
\end_inset

 que cruzan el espacio ambiente 
\begin_inset Formula $\R^{d_{x}}$
\end_inset

 completamente por fuera de 
\begin_inset Formula $\M$
\end_inset

, 
\begin_inset Quotes eld
\end_inset

cortocircuitando
\begin_inset Quotes erd
\end_inset

 la representación.
 Consideraciones análogas complican la elección de cantidad de vecinos en
 
\begin_inset Formula $K-$
\end_inset

Isomap.
\end_layout

\begin_layout Subsection
Distancia de Fermat
\end_layout

\begin_layout Standard
La idea central en Isomap, es 
\emph on
primero
\emph default
 aprender/computar una distancia (en el grafo de vecinos más cercanos) y
 
\emph on
luego
\emph default
 construir un 
\emph on
embedding 
\emph default
- una representación - en cierta dimensión dada, en lugar de usar una distancia
 dada, para aprender una representación de menor dimensión.
 Sin embargo, por cómo está definido el algoritmo, lo que Isomap aproxima
 es la distancia euclídea en la dimensión intrínseca de la variedad subyacente.
 A nuestros fines, será necesario también considerar la 
\emph on
densidad 
\begin_inset Formula $f$
\end_inset

 en la variedad 
\begin_inset Formula $\M$
\end_inset

.
\end_layout

\begin_layout Standard
Por ejemplo, si 
\begin_inset Formula $f$
\end_inset

 fuese una mezcla con iguales pesos de dos leyes gaussianas unidimensionales,
 
\begin_inset Formula $\mathcal{N}\left(0,1\right)$
\end_inset

 y 
\begin_inset Formula $\mathcal{N}\left(10,2\right)$
\end_inset

, quisiéramos que el punto 
\begin_inset Formula $x=5$
\end_inset

 - euclídeamente equidistante de ambas - estuviese más cerca de 
\begin_inset Formula $\mathcal{N}\left(10,2\right)$
\end_inset

 que de 
\begin_inset Formula $\mathcal{N}\left(0,1\right)$
\end_inset

 - exactamente como sucedería en 
\begin_inset Formula $\R^{d}$
\end_inset

 si consideramos la distancia de Mahalanobis 
\begin_inset CommandInset ref
LatexCommand ref
reference "rem:mahalanobis-dist"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
\end_layout

\begin_layout Standard
En casos reales, ni 
\begin_inset Formula $f$
\end_inset

 ni 
\begin_inset Formula $\M$
\end_inset

 se conocerán de antemano, así que pareciera conveniente tratar de aprender
 una distancia que considere ambas a la vez.
 Es exactamente en ese sentido que Groisman et al.
 (2019) proponen la 
\begin_inset Quotes eld
\end_inset

distancia de Fermat
\begin_inset Quotes erd
\end_inset

, una distancia basada en densidades aplicable a variedades que, de alguna
 manera, generaliza el trabajo de Tenenbaum2000.
\end_layout

\begin_layout Definition
(distancia de Fermat muestral, adaptado de Definición 2.1 en Groisman2019)
\end_layout

\begin_layout Definition
Sea 
\begin_inset Formula $Q$
\end_inset

 un conjunto no-vacío, localmente finito, contenido en 
\begin_inset Formula $\R^{d}$
\end_inset

.
 Para 
\begin_inset Formula $\alpha\geq1$
\end_inset


\begin_inset Note Comment
status collapsed

\begin_layout Plain Layout
hace falta el 
\begin_inset Quotes eld
\end_inset

>=1
\begin_inset Quotes erd
\end_inset

? o con >= 0 alcanzaria? Tiene algun valor la generalizacion?
\end_layout

\end_inset

 y 
\begin_inset Formula $s,t\in Q,$
\end_inset

 definimos la 
\emph on
distancia de Fermat muestral
\emph default
 como 
\begin_inset Formula 
\[
D_{Q,\alpha}\left(s,t\right)=\inf\left\{ \sum_{j=1}^{K-1}\norm{x_{i_{j}+1}-x_{i_{j}}}^{\alpha}:\left(q_{1},\dots,q_{K}\right)\text{es un camino de \ensuremath{s} a \ensuremath{t}},q_{i}\in Q\forall i\in\left[K\right],K\geq1\right\} 
\]

\end_inset


\end_layout

\begin_layout Standard
Los autores definen esta distancia muestral para conjuntos arbitrarios 
\begin_inset Formula $Q$
\end_inset

, pero de aquí en más consideraremos 
\begin_inset Formula $Q=\left\{ \mathbf{x}\right\} $
\end_inset

, la muestra 
\begin_inset Formula $d_{x}$
\end_inset

-dimensional de interés.
 Nótese que 
\begin_inset Formula $D_{Q,\alpha}$
\end_inset

 satisface la desigualdad triangular, y define una métrica sobre 
\begin_inset Formula $Q$
\end_inset

.
 Cuando no sea estrictamente necesario, omitiremos en la notación la dependencia
 en 
\begin_inset Formula $Q,\alpha$
\end_inset

.
\end_layout

\begin_layout Remark
La distancia de Fermat muestral es el costo del camino mínimo en el grafo
 
\emph on
completo 
\emph default
de 
\begin_inset Formula $Q$
\end_inset

, con las aristas pesadas por una potencia 
\begin_inset Formula $\alpha$
\end_inset

 de la distancia euclídea.
 Cuando 
\begin_inset Formula $Q=\left\{ \mathbf{x}\right\} ,\ K=N,\ \alpha=1$
\end_inset

, la distancia del paso (1) en Isomap es idéntica a la distancia muestral
 de Fermat.
\end_layout

\begin_layout Remark
A continuación, definen la versión macroscópica de la distancia de Fermat
 muestral,
\end_layout

\begin_layout Definition
(distancia de Fermat, definicion 2.2) Sea 
\begin_inset Formula $\M$
\end_inset

 una variedad de Riemann, 
\begin_inset Formula $f:\M\rightarrow\R_{+}$
\end_inset

 una función continua y positiva en 
\begin_inset Formula $\M$
\end_inset

, 
\begin_inset Formula $\beta\geq0$
\end_inset

 y 
\begin_inset Formula $s,t\in\M$
\end_inset

.
 Definimos la 
\emph on
distancia de Fermat 
\begin_inset Formula $\mathcal{D}_{f,\beta}\left(s,t\right)$
\end_inset

 
\emph default
como
\begin_inset Formula 
\[
\mathcal{T}_{f,\beta}\left(\gamma\right)=\int_{\gamma}f^{-\beta},\quad\mathcal{D}_{f,\beta}\left(s,t\right)=\inf_{\gamma\in\Gamma}\mathcal{T}_{f,\beta}\left(\gamma\right)
\]

\end_inset


\end_layout

\begin_layout Definition
donde el ínfimo esta tomado sobre el conjunto de todos los caminos continuos
 y rectificables contenidos en 
\begin_inset Formula $\bar{\M}$
\end_inset

 (la clausura de 
\begin_inset Formula $\M$
\end_inset

) que comienzan en 
\begin_inset Formula $s$
\end_inset

 y terminan en 
\begin_inset Formula $t$
\end_inset

, y la integral es respecto de la longitud de arco dada por la distancia
 euclídea.
\end_layout

\begin_layout Standard
Se omitirán las dependencias de 
\begin_inset Formula $f,\beta$
\end_inset

 cuando no haya confusión posible.
 
\end_layout

\begin_layout Standard
Uniendo las dos definiciones previas, el teorema central del trabajo es
 el siguiente:
\end_layout

\begin_layout Theorem

\emph on
(Teorema 2.7, Groisman2019) Sea 
\begin_inset Formula $\M$
\end_inset

 una variedad 
\begin_inset Formula $d$
\end_inset

-dimensional, isométrica y 
\begin_inset Formula $C^{1}$
\end_inset

 embebida en 
\begin_inset Formula $\R^{D}$
\end_inset


\begin_inset Foot
status collapsed

\begin_layout Plain Layout
Es decir, existe un conjunto abierto y conexo 
\begin_inset Formula $S\subset\R^{d}$
\end_inset

 y 
\begin_inset Formula $\phi:\bar{S}\rightarrow\R^{D}$
\end_inset

 una transformación isométrica tal que 
\backslash
phi
\begin_inset Formula $\phi\left(\bar{S}\right)=\M$
\end_inset

.
 En aplicaciones reales se espera que 
\begin_inset Formula $d\ll D$
\end_inset

, pero no es necesario.
\end_layout

\end_inset

.
 Sea 
\begin_inset Formula $Q_{n}=\left\{ q_{1},\dots,q_{n}\right\} $
\end_inset

 puntos independientes con densidad común 
\begin_inset Formula $f$
\end_inset

.
 Luego, para 
\begin_inset Formula $\alpha>1$
\end_inset

 y 
\begin_inset Formula $x,y\in\M$
\end_inset

 se tiene
\begin_inset Formula 
\[
\lim_{n\rightarrow\infty}n^{\beta}D_{Q_{n},\alpha}\text{\left(x,y\right)=\ensuremath{\mu\mathcal{D}_{f,\beta}}\left(x,y\right)\ casi seguramente}.
\]

\end_inset

Aquí, 
\begin_inset Formula $\beta=\left(\alpha-1\right)/d$
\end_inset

 y 
\begin_inset Formula $\mu$
\end_inset

 es una constante que depende únicamente de 
\begin_inset Formula $\alpha$
\end_inset

 y la dimensión de la variedad 
\begin_inset Formula $d$
\end_inset

.
\end_layout

\begin_layout Standard
En otras palabras, correctamente escalada, la distancia muestral de Fermat
 converge a la distancia 
\begin_inset Quotes eld
\end_inset

poblacional
\begin_inset Quotes erd
\end_inset

 de Fermat, y 
\begin_inset Formula $D_{Q_{n},\alpha}$
\end_inset

 es un estimador consistente de 
\begin_inset Formula $\mathcal{D}_{f,\beta}$
\end_inset

.
 Los autores prueban el caso en que 
\begin_inset Formula $f$
\end_inset

 corresponde a un proceso puntual de Poisson homogéneo en 
\begin_inset Formula $\M$
\end_inset

, y conjeturan que es cierto para 
\begin_inset Formula $f$
\end_inset

 arbitraria
\begin_inset Note Comment
status open

\begin_layout Plain Layout
Es así? O en realidad la prueba es más fuerte?
\end_layout

\end_inset

.
\end_layout

\begin_layout Section
Propuesta
\end_layout

\begin_layout Standard
En la Introducción hemos repasado en detalle un método eficiente y lo sumamente
 estudiado para responder al problema de clasificación en dominios de alta
 dimensionalidad: la estimación de densidad por núcleos (KDE), específicamente
 en variedades de Riemann.
 Notamos que de los tres parámetros a elegir - el núcleo, el ancho de banda
 y la distancia - tanto el ancho de banda como la distancia son problemáticos
 en HD, aunque para el ancho de banda el tratamiento encontrado en la literatura
 es mucho más extenso.
 Nos proponemos investigar si es posible mejorar la performance de los métodos
 descritos hasta ahora, con una noción de distancia aprendida de los datos,
 la distancia muestral de Fermat propuesta por Groisman2019.
 Más específicamente, aplicaremos
\end_layout

\begin_layout Itemize
un clasificador basado en estimaciones de densidad por núcleos (gaussianos)
 en variedades según Loubes2010
\end_layout

\begin_layout Itemize
con matriz de suavización 
\series bold
H
\series default
 individualmente orientada en cada elemento muestral según Vincent2003
\end_layout

\begin_layout Itemize
y distancia varietal aprendida según Groisman2019
\end_layout

\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Standard
Evaluaremos al clasificador resultante en un conjunto de datasets sintéticos
 y naturales que representen un espectro amplio de casos de alta dimensionalidad
, a través de un estudio de ablación, para entender cuál es la ventaja marginal
 de utilizar una distancia aprendida por sobre el clasificador equivalente
 con distancia euclídea.
\end_layout

\begin_layout Standard
Los métodos de estimación por núcleos, aunque simples en su concepción,
 tienen altos requerimientos computacionales, y el aprendizaje de distancias
 basadas en grafos, más aún.
 Por ello, en el estudio ablativo comparado, incluiremos como referencia
 de precisión:
\end_layout

\begin_layout Itemize
un clasificador KNN con distancia euclídea - la versión más sencilla posible
 de un clasificador KDE, y
\end_layout

\begin_layout Itemize
un clasificador por GBT - gradient boosting trees -, uno de los métodos
 más 
\begin_inset Quotes eld
\end_inset

plug & play
\begin_inset Quotes erd
\end_inset

 disponibles hoy en día.
\end_layout

\begin_layout Standard
Incluiremos algunos comentarios sobre el costo computacional de cada método,
 comparando la expectativa teórica con los resultados de nuestras - sencillas
 y caseras - implementaciones.
\end_layout

\begin_layout Standard
Finalmente, nos proponemos dar algunas garantías teóricas sobre el comportamient
o asintótico de la distancia muestral de Fermat como estimador de la distancia
 (macroscópica / poblacional) homónima.
\end_layout

\begin_layout Section
Otros papers
\end_layout

\begin_layout Standard
Hay varios papers con ideas muy piolas sobre como aprender una variedad,
 y como usar la info (las cartas generadas) para clasificar.
 Se aleja de nuestro interes principal, pero tal vez ameriten mención?
\end_layout

\begin_layout Subsubsection
Manifold Tangent Classifier (+TangentProp)
\end_layout

\begin_layout Standard
Incluye un buen detalle de 3 versiones interreleacionadas de la hipotesis
 de la variedad.
\end_layout

\begin_layout Standard
Usa una NN para encontrar en cada punto, direcciones tangentes en las que
 la funcion de activacion no cambia significativamente.
 Luego, usa tangentprop (una forma de gradient backpropagatiojn con restriccione
s sobre las derivadas primeras) para incluir esa info en la optimizacion
 y mejorar los resultados de clasificacion.
\end_layout

\begin_layout Subsubsection
Shell Theory
\end_layout

\begin_layout Standard
Por la maldicion de la dimensionalidad, debería ser directamente imposible
 ML en alta dimension, pero en la practica se ve que funciona.
 Propone una teoría general pero accesible de 
\begin_inset Quotes eld
\end_inset

generadores jerárquicos
\begin_inset Quotes erd
\end_inset

, 
\begin_inset Quotes eld
\end_inset

shell theory
\begin_inset Quotes erd
\end_inset

, que imita las clasificaciones jerárquicas semánticas que buscamos entender
 (gato siamés 
\begin_inset Formula $\subset$
\end_inset

 gato 
\begin_inset Formula $\subset$
\end_inset

 animal).
 
\end_layout

\begin_layout Subsubsection
Brand2003 - Charting a Manifold
\end_layout

\begin_layout Standard
Menciona una especie de hipotesis de la variedad antes que otros cuantos,
 aunque no la llama así.
\end_layout

\begin_layout Standard
Ofrece una idea empírica de cómo estimar la dimensión de la variedad mirando
 cómo crece la función de conteo de puntos 
\begin_inset Formula $c\left(r,y\right)=\sum_{i=1}^{N}\mathbf{1}\left\{ x_{i}\in B\left(y,r,d_{x}\right)\right\} $
\end_inset

 en relación al radio de la bola considerada.
\end_layout

\begin_layout Standard
Aplica el método propuesto al trefoil que consideramos recientemente.
\end_layout

\begin_layout Subsubsection
Vincent, Bengio 2003 - Manifold Parzen Windows
\end_layout

\begin_layout Standard
Esencialmente MVKDE con 
\begin_inset Formula $\mathbf{H}_{i}$
\end_inset

definida para 
\emph on
cada
\emph default
 obserbvación en un vecindario 
\begin_inset Quotes eld
\end_inset

suave
\begin_inset Quotes erd
\end_inset

 (usando kernels sobre la distancia a c/otra obs) o duro (KNN) alrededor
 de la obs.
 Usa algunos 
\begin_inset Quotes eld
\end_inset

trucos
\begin_inset Quotes erd
\end_inset

 para evitar 
\begin_inset Formula $\mathbf{H}_{i}$
\end_inset

 mal condicionadas.
\end_layout

\begin_layout Standard
Considera 
\begin_inset Quotes eld
\end_inset

negative conditional log likhelihood
\begin_inset Quotes erd
\end_inset

 como medida de bondad del clasificador, como alernativa continua al error
 de clasificación y otras.
\end_layout

\begin_layout Subsubsection
The Curse of Highly Variable Functions for Local Kernel Machines
\end_layout

\begin_layout Standard
Muestra cómo todos los métodos basados en núcleos (KNN,m KDE, hasta isomap)
 comparten la necesidad de un tamaño muestral enorme cuando la función objetivo
 a aprender tiene muchas variaciones, por depender de entornos locales a
 cada observacion para mapear la variedad.
 Aún funciones de baja 
\begin_inset Quotes eld
\end_inset

complejidad de Kolmogorov
\begin_inset Quotes erd
\end_inset

 (paridad, seno) son muy difíciles de aprender con kernels, y sin info global.
\end_layout

\begin_layout Subsubsection
Learning Eigenfunctions Links Spectral Embedding and Kernel PCA 
\end_layout

\begin_layout Standard
Une un monton monton de metodos de estimacion de densidad / embeddings demtro
 de un marco unificado de funciones basadas en nucleos.
 En particular, Isomap (y landmark-Isomap) se pueden ampliar a puntos out-of-sam
ple computando la aproximacion a la distancia geodésica en el grafo de kNN,
 a traves de los puntos de entrenamiento, basicamente como estamos por proponer
 nosotros para extender distancia de fermat a out-of-sample.
 Duro pero interesante.
\end_layout

\begin_layout Subsubsection
Chu2018 - Exploration of a Graph-based Density-Sensitive Metric
\end_layout

\begin_layout Standard

\emph on
We consider a simple graph-based metric on points in Euclidean space known
 as the edge-squared metric.
 This metric is defined by squaring the Euclidean distance between points,
 and taking the shortest paths on the resulting graph.
 This metric has been studied before in wireless networks and machine learning,
 and has the density- sensitive property: distances between two points in
 the same cluster are short, even if their Euclidean distance is long.
 This property is desirable in machine learning.
\end_layout

\begin_layout Subsubsection
Biijral2012 - Semi-supervised Learning with Density Based Distances
\end_layout

\begin_layout Standard
Denoting the probability density function in R d by f (x), we can define
 a path length measure through R d that assigns short lengths to paths through
 highly den- sity regions and longer lengths to paths through low density
 regions.
 We can express such a path length measure as
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
J_{f}\left(x_{1}\stackrel{\gamma}{\leadsto}x_{2}\right)=\int_{0}^{1}g(f(\gamma(t)))\left\Vert \gamma^{\prime}(t)\right\Vert _{p}dt,
\]

\end_inset


\end_layout

\begin_layout Standard
where 
\begin_inset Formula $γ:[0,1]→\R$
\end_inset

 d is a continuous path from 
\begin_inset Formula $γ(0)=x_{1}$
\end_inset

 
\begin_inset Formula $γ(1)=x_{2}$
\end_inset

 and 
\begin_inset Formula $g:\R^{+}\rightarrow\R$
\end_inset

 is monotonically decreasing (e.g.
 
\begin_inset Formula $g(u)=1/u$
\end_inset

).
 Using Equation 1 as a density-based measure of path length, we can now
 define the density based distance (DBD) between any two points 
\begin_inset Formula $x_{1},x_{2}\in\R$
\end_inset

 d as the density-based length of a shortest path between the two points
 
\begin_inset Formula 
\[
D_{f}(x_{1},x_{2})=\inf_{\gamma}J_{f}(x_{1}\stackrel{\gamma}{\leadsto}x_{2})
\]

\end_inset


\end_layout

\begin_layout Standard
Alternatively, a simple heuristic was suggested by Vincent and Bengio (2003)
 in the context of clustering, and is based on constructing a weighted graph
 over the data set, with weights equal to the squared dis- tances between
 the endpoints and calculating shortest paths on this graph.
\end_layout

\begin_layout Standard
N.delA.: El paper de Vincent y Bengio que mencionan no está disponible en
 internet, sólo aparece citado en otros trabajos: 
\begin_inset Quotes eld
\end_inset


\emph on
Vincent, P., & Bengio, Y.
 (2003).
 Density sensitive metrics and kernels.
 Proceedings of the Snowbird Workshop.
\begin_inset Quotes erd
\end_inset


\emph default
, pero todo indica que la formulación es como la de Groisman2019, con 
\series bold

\begin_inset Formula $\beta=2$
\end_inset


\end_layout

\begin_layout Standard
Más adelante, considera funciones 
\begin_inset Formula $g=f^{-r}$
\end_inset

 y pareciera llegar a una formulación idéntica a la de Groisman2019.
\end_layout

\begin_layout Section
Notas sueltas
\end_layout

\begin_layout Itemize
soft clf chen
\end_layout

\begin_layout Itemize
(¿Es lo mismo 
\begin_inset Formula $\norm{\cdot}$
\end_inset

 que la geodésica en R^d_x? Creo que sí)
\end_layout

\begin_layout Itemize
mencion a t-SNE? como esta basada en distancia euclidea, no parece que vaya
 a ayudar mucho
\end_layout

\begin_layout Itemize
RKHS - reproducing kernel hilbert spaces -: alguito para entender a que
 cuernos ser refieren?
\end_layout

\begin_layout Itemize
biblio: No subirla, pero esconder script ligeramente disimulado que la baje
 por uno?
\end_layout

\begin_layout Section
Análisis experimental
\end_layout

\begin_layout Section
Cuentita
\end_layout

\begin_layout Section
Conclusiones
\end_layout

\begin_layout Standard
\begin_inset CommandInset bibtex
LatexCommand bibtex
btprint "btPrintAll"
bibfiles "bib/references"
options "bibtotoc,plain"

\end_inset


\end_layout

\end_body
\end_document
