{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.spatial import distance_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from fermat import Fermat\n",
    "from sklearn.datasets import load_iris, load_digits\n",
    "import networkx as nx\n",
    "import igraph as ig\n",
    "import neo4j\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = load_iris(return_X_y=True, as_frame=True)\n",
    "N = len(X)  # # observaciones\n",
    "K = len(set(y))  # # clases\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    *load_iris(return_X_y=True, as_frame=True), test_size=0.2\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KernelDensity\n",
    "import numpy as np\n",
    "\n",
    "# rng = np.random.RandomState(42)\n",
    "runs = 200\n",
    "res = np.empty(runs)\n",
    "n, d = 1000, 3\n",
    "test_size = 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "int(1.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(runs):\n",
    "    kde = KernelDensity(kernel=\"gaussian\", bandwidth=0.5).fit(X[:test_size])\n",
    "    log_density = kde.score_samples(X)\n",
    "    res[i] = log_density[:test_size].mean() > log_density[test_size:].mean()\n",
    "res.mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_iris, _ = load_iris(return_X_y=True)\n",
    "n, d = X_iris.shape\n",
    "datasets = {\"iris\": X_iris, \"unif(0, 1)\": np.random.random_sample((n, d))}\n",
    "(n, d), X_iris[:5]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Qué probabilidad $p$ hay de que en la densidad estimatada sea, en promedio, más alta en el _train set_ que en un _test set_ aún no visto?\n",
    "Con un _test set_ pequeño en relación al de _train_, esperaría que la densidad esté bien estimada en todo el dominio de la X y entonces $p >= 1/2 + \\delta$, con un $\\delta$ \"pequeño\", ya que $E_{X_{train}}(\\hat{f_X}(X)) \\approx E_{X_{test}}(\\hat{f_X}(X))$. Coun un test set \"grande\", de tamaño cercano a $n$, la estimación de densidad,e la densidad esté sobreajustada a los puntos conocidos, y siempre dé \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(train_size, runs=100):\n",
    "    results = {name: np.empty(runs) for name in datasets}\n",
    "    for i in range(runs):\n",
    "        for name, X in datasets.items():\n",
    "            X_train, X_test = train_test_split(X, train_size=train_size)\n",
    "            kde = KernelDensity(kernel=\"gaussian\", bandwidth=0.5).fit(X_train)\n",
    "            results[name][i] = (\n",
    "                kde.score_samples(X_train).mean() > kde.score_samples(X_test).mean()\n",
    "            )\n",
    "    return pd.DataFrame(results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Ajustar bien la densidad es `facil` entrenando con \"casi todos\" los datos y `dificil` entrenando con \"casi ninguno\" de los datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "facil, dificil = 0.9, 0.1\n",
    "runs = 100\n",
    "favorable, adverso = run(facil, runs), run(dificil, runs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = pd.concat(\n",
    "    [favorable, adverso], keys=[\"favorable\", \"adverso\"], names=[\"modo\", \"run\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res.groupby(\"modo\").mean()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bingo! En el contexto `favorable` de entrenamiento, la densidad estimada está más ajustada a los datos de _train_ que _test_, pero no siempre. En particular, el sobreajuste se nota más en el dataset real, `iris`, que en el sintético y \"puramente aleatorio\" `unif(0, 1)`.\n",
    "En el contexto adverso, (casi) siempre la densidad estimada sobreajusta a train: en el dataset real \"siempre\" lo hace, mientras que en el sintético, alguna rara vez no se evidencia sobreajuste."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparación de del humilde `EuclideanKDE` con la implementación de `scikit-learn`, `KernelDensity`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_pdf = scipy.stats.norm.pdf\n",
    "\n",
    "\n",
    "class EuclideanKDE:\n",
    "    def __init__(self, kernel=norm_pdf, bandwidth=1):\n",
    "        self.kernel = kernel\n",
    "        self.bandwidth = bandwidth\n",
    "\n",
    "    def fit(self, X):\n",
    "        self.X_ = X\n",
    "        self.n_, self.dim_ = X.shape\n",
    "        return self\n",
    "\n",
    "    def density(self, X, log=True):\n",
    "        densities = (self.bandwidth**-self.dim_) * self.kernel(\n",
    "            distance_matrix(X, self.X_) / self.bandwidth\n",
    "        ).mean(axis=1)\n",
    "        return np.log(densities) if log else densities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = datasets[\"iris\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_kde = EuclideanKDE(kernel=norm_pdf, bandwidth=1).fit(X)\n",
    "sk_kde = KernelDensity(kernel=\"gaussian\", bandwidth=1).fit(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_score = my_kde.density(X)\n",
    "sk_score = sk_kde.score_samples(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_score[:5], sk_score[:5]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No se ven iguales pero pareciera haber una proporcionalidad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(my_score, sk_score)\n",
    "plt.axline([0, 0], slope=1, linestyle=\"dashed\", color=\"gray\")\n",
    "est = LinearRegression().fit(X=sk_score.reshape(-1, 1), y=my_score)\n",
    "plt.title(f\"my_score = {est.intercept_.round(3)} + {est.coef_[0].round(3)} * sk_score\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Efectivamente, la relación pareciera de identidad, salvo por una sospechosa constante $\\approx2.757$. Hmmmm. Al menos, parecieran funcionar igual. Revisando la documentación de [`KernelDensity.score_samples`](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KernelDensity.html?highlight=kerneldensity#sklearn.neighbors.KernelDensity.score_samples), leo:\n",
    "\n",
    "    Returns \n",
    "        density: ndarray of shape (n_samples,)\n",
    "\n",
    "        Log-likelihood of each sample in X. These are normalized to be probability densities, so values will be low for high-dimensional data.\n",
    "\n",
    "Y en el código fuente, se lee (editado):\n",
    "\n",
    "```python\n",
    "        N = self.tree_.data.shape[0]\n",
    "        log_density = self.tree_.kernel_density(X, h=self.bandwidth, kernel=self.kernel)\n",
    "        log_density -= np.log(N)\n",
    "        return log_density\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos si podemos revertir esa \"normalización\" de `log_density -= np.log(N)`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = sk_kde.tree_.data.shape[0]\n",
    "N, N == X.shape[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "atol_N = sk_kde.atol * N\n",
    "log_density = sk_kde.tree_.kernel_density(\n",
    "    X,\n",
    "    h=sk_kde.bandwidth,\n",
    "    kernel=sk_kde.kernel,\n",
    "    atol=atol_N,\n",
    "    rtol=sk_kde.rtol,\n",
    "    breadth_first=sk_kde.breadth_first,\n",
    "    return_log=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert np.allclose(sk_score + np.log(N), log_density)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_score[:5], log_density[:5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(my_score, log_density)\n",
    "plt.axline([0, 0], slope=1, linestyle=\"dashed\", color=\"gray\")\n",
    "est = LinearRegression().fit(X=log_density.reshape(-1, 1), y=my_score)\n",
    "plt.title(\n",
    "    f\"my_score = {est.intercept_.round(3)} + {est.coef_[0].round(3)} * log_density\"\n",
    ")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El problema, es que esa desnormalización, no es del factor esperado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.log(N), LinearRegression().fit(X=sk_score.reshape(-1, 1), y=my_score).intercept_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Qué más hay de distinto? Habrán normalizado los datos sin avisar? La `data` (nuestra `X`) del estimador de densidad se accede desde `KernelDensity.tree_.data`, que es una `MemoryView` del array original."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sk_data = np.asarray(sk_kde.tree_.data)\n",
    "assert np.all(sk_data == X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No, tampoco es eso. Bueno, ya se verá. Por el momento, son funcionalmente idénticas las implementaciones, salvo por esa constante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: review normalizing factors in BinaryTree.kernel_density\n",
    "factor = 0.5 * d * np.log(2 * np.pi)\n",
    "factor\n",
    "# -factor - d * np.log(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparación de Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "my_kde = EuclideanKDE(kernel=norm_pdf, bandwidth=1).fit(X_digits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "sk_kde = KernelDensity(kernel=\"gaussian\", bandwidth=1).fit(X_digits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`fit` es esencialmente \"gratis\", algunas pocas asignaciones en ambos casos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_kde = EuclideanKDE(kernel=norm_pdf, bandwidth=1).fit(X_digits)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "my_kde.density(X_digits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sk_kde = KernelDensity(kernel=\"gaussian\", bandwidth=1).fit(X_digits)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "sk_kde.score_samples(X_digits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_digits.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_kde = EuclideanKDE(kernel=norm_pdf, bandwidth=1).fit(X_iris)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "my_kde.density(X_iris)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sk_kde = KernelDensity(kernel=\"gaussian\", bandwidth=1).fit(X_iris)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "sk_kde.score_samples(X_iris)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_digits.shape, X_iris.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los resultados para `my_kde`, por ejemplo:\n",
    "```\n",
    "digits: 1.44 s ± 219 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
    "iris:  2.23 ms ± 313 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1440 / 2.23, 1797 / 150\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ajustar `digits` toma 645 veces más tiempo que `iris`, con 12 veces más observaciones (pero claro, también 60 dimensiones más).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_kde = EuclideanKDE(kernel=norm_pdf, bandwidth=1).fit(X_digits)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "my_kde.density(X_digits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "my_kde.density(X_digits[:180])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parece lineal en `n`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_kde = EuclideanKDE(kernel=norm_pdf, bandwidth=1).fit(X_digits[:, :8])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "my_kde.density(X_digits[:,:8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_kde = EuclideanKDE(kernel=32, bandwidth=1).fit(X_digits[:, :16])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "my_kde.density(X_digits[:,:16])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_kde = EuclideanKDE(kernel=norm_pdf, bandwidth=1).fit(X_digits[:, :32])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "my_kde.density(X_digits[:,:32])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_dims = np.linspace(8, 64, 8).astype(int)\n",
    "runs = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for n_dim in n_dims:\n",
    "    X = X_digits[:, :n_dim]\n",
    "    my_kde = EuclideanKDE().fit(X)\n",
    "    for i in range(runs):\n",
    "        t0 = time.time()\n",
    "        my_kde.density(X)\n",
    "        t = time.time() - t0\n",
    "        results.append({\"n_dim\": n_dim, \"run\": i, \"time\": t})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x=\"n_dim\", y=\"time\", data=res)\n",
    "est = LinearRegression().fit(res[[\"n_dim\"]], res.time)\n",
    "plt.title(\n",
    "    f\"time[s] = {est.intercept_.round(3)} + {est.coef_[0].round(3)} * n_dim\"\n",
    ")\n",
    "plt.axline([0, est.intercept_], slope = est.coef_, linestyle=\"dashed\", color=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, también parece lineal en `n_dim`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los tiempos de `KernelDensity` son como la mitad de los de EuclideanKDE, pero no sé cuánto mejor/peor serían si implementase una DistanceMetric nueva para la distancia de Fermat. Por el momento, seguimos con la programacion habitual, y mantenemos los clasificadores de juguete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rang.random_sample((100, 3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import random as rnd\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = rnd.rand(100, 3)\n",
    "A.sort(axis=1)\n",
    "A = np.column_stack([np.zeros_like(A[:, 0]), A, np.ones_like(A[:, 0])])\n",
    "a = A[0]\n",
    "a, A[\n",
    "    :5,\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fermat_dist(x, alpha=1):\n",
    "    \"\"\"Fermat alpha-distance between `x_0` and `x_k`, in the line graph with nodes at `x = (x_1, ..., x_k)`.\"\"\"\n",
    "    return ((x[1:] - x[:-1]) ** alpha).sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.apply_along_axis(fermat_dist, axis=1, arr=X, alpha=3).mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size = 1000\n",
    "scales = [1 / 10, 1 / 2, 1, 2, 10]\n",
    "alphas = np.linspace(1, 4, 7)\n",
    "ks = [1, 2, 5, 10]\n",
    "results = []\n",
    "for k in ks:\n",
    "    A = rnd.rand(sample_size, k)\n",
    "    A.sort(axis=1)\n",
    "    A = np.column_stack([np.zeros_like(A[:, 0]), A, np.ones_like(A[:, 0])])\n",
    "    for scale in scales:\n",
    "        for alpha in alphas:\n",
    "            results.append(\n",
    "                dict(\n",
    "                    k=k,\n",
    "                    alpha=alpha,\n",
    "                    scale=scale,\n",
    "                    dists=np.apply_along_axis(\n",
    "                        fermat_dist, axis=1, arr=scale * A, alpha=alpha\n",
    "                    ),\n",
    "                )\n",
    "            )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"mean_dist\"] = df.dists.apply(np.mean)\n",
    "df[\"scaled_dist\"] = df.mean_dist / (df.scale**df.alpha)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.k == 1].pivot(\"alpha\", \"scale\", \"mean_dist\").round(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.k == 1].pivot(\"alpha\", \"scale\", \"scaled_dist\").round(3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Está claro que si $c$ es la constance de escale `scale`, las distancias de fermat escalan según $c^{-\\alpha}$. Ahora, cuánto cambian con el tamaño de muestra $k$? \n",
    "\n",
    "Para $k=1$, se puede calcular exactamente la esperanza de la longitud del camino. Si hay un único punto entre 0 y 1 elegido al azar según $X \\sim \\text{Unif}(0, 1)$, entonces la longitud del camino de Fermat cuando $alpha=2$ será $E\\left(X^2 + (1-X)^2\\right) = 2/3$, lo cual se ve en la tabla anterior. Para $k=1$ y otros valores de $\\alpha$, la expresión no será tan bella pero es computable sin mucha dificultad. Para otros valores de $k$, sin embargo, ya entran en juego la distribución de los estadísticos de orden y no me resulta para nada evidente una fórmula cerrada.\n",
    "\n",
    "Aproximémosla. Sean $X^{(0)} = 0, X^{(k+1)} = 1$ y $X^{(i)}, i=1,\\dots,k$ las v.a. que surgen de ordenar una muestra $X_i \\sim_{iid} \\text{Unif}(0, 1), \\ i\\in[k]$. Para todo $k$ se cumple que cada \"segmentito de recta\", $\\mathbb{E}\\left(X^{(i+1)}-X^{(i)}\\right) = 1 / (k + 1)$. Luego, esperaríamos que \n",
    "\n",
    "$$\n",
    "\\begin{align} dist_{\\alpha}^k(0, 1) &= \\mathbb{E}\\ \\left(\\sum_{i=0}^k \\left[X^{(i+1)}-X^{(i)} \\right]^{\\alpha}\\right) \\\\\n",
    " &=  \\sum_{i=0}^k\\ \\left( \\mathbb{E}\\left[X^{(i+1)}-X^{(i)} \\right]^{\\alpha}\\right) \\\\\n",
    " &\\approx (???) (k + 1) \\left(\\frac{1}{k+1}\\right)^{\\alpha}\n",
    "   \\end{align}\n",
    "\n",
    "$$\n",
    "\n",
    "Pero no vale que $E(X^k) = E(X)^k$! Qué se hace en su lugar? Hay que conocer la densidad de $X^{(i+1)}-X^{(i)}$ y calcularlo? Empíricamente, veamos como cambia la distancia con $k$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.scale == 1].pivot(\"alpha\", \"k\", \"scaled_dist\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"k_scaled_dist\"] = df.scaled_dist * ((df.k + 2) ** (df.alpha - 1))\n",
    "df[df.scale == 1].assign().pivot(\"alpha\", \"k\", \"k_scaled_dist\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size = 1000\n",
    "alphas = [1.5, 1.75, 2, 2.25, 3, 4, 5]\n",
    "ks = np.array([*range(1, 11), *rnd.choice(range(11, 2001), 50, replace=False)])\n",
    "\n",
    "results = []\n",
    "for k in ks:\n",
    "    A = rnd.rand(sample_size, k)\n",
    "    A.sort(axis=1)\n",
    "    A = np.column_stack([np.zeros_like(A[:, 0]), A, np.ones_like(A[:, 0])])\n",
    "    for alpha in alphas:\n",
    "        results.append(\n",
    "            dict(\n",
    "                k=k,\n",
    "                alpha=alpha,\n",
    "                dists=np.apply_along_axis(fermat_dist, axis=1, arr=A, alpha=alpha),\n",
    "            )\n",
    "        )\n",
    "\n",
    "df = pd.DataFrame(results)\n",
    "df[\"mean_dist\"] = df.dists.apply(np.mean)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.k < 50].pivot(\"alpha\", \"k\", \"mean_dist\").round(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 7))\n",
    "sns.lineplot(x=\"k\", y=\"mean_dist\", hue=\"alpha\", data=df, marker=\"o\")\n",
    "plt.xscale(\"log\")\n",
    "plt.yscale(\"log\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para la elección del ancho de banda $h$ es necesario saber la escala de las distancias! Con qué se come esto???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for i in range(10000):\n",
    "    s = rnd.rand(100)\n",
    "    s.sort()\n",
    "    dists = (np.array([s[0], *(s[1:] - s[:-1]), 1 - s[-1]])) ** 2\n",
    "    results.append((dists.mean(), dists.std()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean([std / mean for mean, std in results])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementación KDEClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "from numpy.linalg import norm as euclidean_norm\n",
    "from scipy.spatial import distance_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "\n",
    "scipy.spatial.distance.minkowski(A[0], A[2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repr(fermat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B = range(5)\n",
    "C = range(7)\n",
    "[[b * c for c in C] for b in B]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(A[:] == A[2]).all(1)[:5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A.shape, np.zeros(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WIP - ignore\n",
    "def euclidean_distance(x, y):\n",
    "    return euclidean_norm(x - y)\n",
    "\n",
    "\n",
    "class FermatDistance(Fermat):\n",
    "    def __init__(self, **kwargs):\n",
    "        super.__init__(**kwargs)\n",
    "\n",
    "    def _fit(self, X):\n",
    "        self.fit(X)\n",
    "        self.X_ = X\n",
    "        self.n_, self.d_ = X.shape\n",
    "        self.is_fitted_ = True\n",
    "        return self\n",
    "\n",
    "    def __call__(A, B):\n",
    "        if self.alpha == 1:\n",
    "            return distance_matrix(A, B)\n",
    "        else:\n",
    "            return [[self._get_distance(a, b) for a in A] for b in B]\n",
    "\n",
    "    def _get_distance(a, b):\n",
    "        if not self.is_fitted_:\n",
    "            self._fit()\n",
    "        if any((self.X_[:] == a).all(1)):  # `a` is a node from X_\n",
    "            to_X = np.zeros(self.n_)\n",
    "        else:\n",
    "            to_X = euclidean_distances(a.reshape(1, -1), verts)[0] ** self.alpha\n",
    "\n",
    "        if a_known := np.where((X[:] == 2 * X[2]).all(1)):\n",
    "            to_nodes = euclidean_distances(a.reshape(1, -1), verts)[0] ** alpha\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_pdf = scipy.stats.norm.pdf\n",
    "\n",
    "\n",
    "class EuclideanKDE:\n",
    "    def __init__(self, kernel=norm_pdf, bandwith=1):\n",
    "        self.kernel = kernel\n",
    "        self.bandwith = bandwith\n",
    "\n",
    "    def fit(self, X):\n",
    "        self.X_ = X\n",
    "        self.n_, self.dim_ = X.shape\n",
    "        return self\n",
    "\n",
    "    def density(self, X, log=True):\n",
    "        densities = (self.bandwith**-self.dim_) * self.kernel(\n",
    "            distance_matrix(X, self.X_) / self.bandwith\n",
    "        ).mean(axis=1)\n",
    "        return np.log(densities) if log else densities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.utils.validation import check_X_y  # , check_array, check_is_fitted\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "from scipy.special import softmax\n",
    "\n",
    "\n",
    "class EuclideanKDEClassifier(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, kernel=norm_pdf, bandwith=1):\n",
    "        self.kernel = kernel\n",
    "        self.bandwith = bandwith\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # Check that X and y have correct shape\n",
    "        X, y = check_X_y(X, y)\n",
    "        # Store the classes seen during fit\n",
    "        self.classes_ = unique_labels(y)\n",
    "        self.X_ = X\n",
    "        self.y_ = y\n",
    "        self.densities_ = {\n",
    "            cls: EuclideanKDE(self.kernel, self.bandwith).fit(X[y == cls])\n",
    "            for cls in self.classes_\n",
    "        }\n",
    "        return self\n",
    "\n",
    "    def decision_function(self, X):\n",
    "        return np.column_stack(\n",
    "            [self.densities_[cls].density(X) for cls in self.classes_]\n",
    "        )\n",
    "\n",
    "    def predict(self, X):\n",
    "        D = self.decision_function(X)\n",
    "        return self.classes_[np.argmax(D, axis=1)]\n",
    "\n",
    "    def predict_proba(self, X, temp=1):\n",
    "        D = self.decision_function(X)\n",
    "        return softmax(D / temp, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FermatKDE:\n",
    "    def __init__(self, kernel=norm_pdf, bandwith=1, alpha=2, path_method=\"FW\"):\n",
    "        self.kernel = kernel\n",
    "        self.bandwith = bandwith\n",
    "        self.alpha = alpha\n",
    "        self.path_method = path_method\n",
    "\n",
    "    def fit(self, X):\n",
    "        self.X_ = X\n",
    "        self.n_, self.dim_ = X.shape\n",
    "        self.fermat_ = Fermat(self.alpha, self.path_method).fit(distance_matrix(X, X))\n",
    "        self.distances_ = self.fermat_.get_distances()\n",
    "        return self\n",
    "\n",
    "    def density(self, X, log=True):\n",
    "        densities = (self.bandwith**-self.dim_) * self.kernel(\n",
    "            self.distances(X) / self.bandwith\n",
    "        ).mean(axis=1)\n",
    "        return np.log(densities) if log else densities\n",
    "\n",
    "    def distances(self, X):\n",
    "        distances_to_X = distance_matrix(X, self.X_) ** self.alpha\n",
    "        return np.vstack([(d + self.distances_).min(axis=1) for d in distances_to_X])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import softmax\n",
    "\n",
    "\n",
    "class FermatKDEClassifier(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, kernel=norm_pdf, bandwith=1, alpha=2, path_method=\"FW\"):\n",
    "        self.kernel = kernel\n",
    "        self.bandwith = bandwith\n",
    "        self.alpha = alpha\n",
    "        self.path_method = path_method\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # Check that X and y have correct shape\n",
    "        X, y = check_X_y(X, y)\n",
    "        # Store the classes seen during fit\n",
    "        self.classes_ = unique_labels(y)\n",
    "        self.X_ = X\n",
    "        self.y_ = y\n",
    "        self.densities_ = {\n",
    "            cls: FermatKDE(\n",
    "                self.kernel, self.bandwith, self.alpha, self.path_method\n",
    "            ).fit(X[y == cls])\n",
    "            for cls in self.classes_\n",
    "        }\n",
    "        return self\n",
    "\n",
    "    def decision_function(self, X):\n",
    "        return np.column_stack(\n",
    "            [self.densities_[cls].density(X) for cls in self.classes_]\n",
    "        )\n",
    "\n",
    "    def predict(self, X):\n",
    "        D = self.decision_function(X)\n",
    "        return self.classes_[np.argmax(D, axis=1)]\n",
    "\n",
    "    def predict_proba(self, X, temp=1):\n",
    "        D = self.decision_function(X)\n",
    "        return softmax(D / temp, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.array([[i] * 2 for i in range(3)])\n",
    "Fermat(2, \"FW\").fit(distance_matrix(A, A)).get_distances()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B = np.vstack([[-1, 0], [1, 0]])\n",
    "B\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = np.vstack([A, B])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full = Fermat(2, \"FW\").fit(distance_matrix(C, C))\n",
    "full_dists = full.get_distances()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train = 120\n",
    "train = FermatKDE(2, \"FW\").fit(A)\n",
    "test_dists = train.distances(B)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_dists, test_dists\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full = Fermat(2, \"FW\").fit(distance_matrix(X, X))\n",
    "full_dists = full.get_distances()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train = 120\n",
    "train = FermatKDE(2, \"FW\").fit(X[:n_train])\n",
    "test_dists = train.distances(X[n_train:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.where(full_dists[n_train:, :n_train] > test_dists + 0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_dists[n_train:, :n_train][22, 101], test_dists[22, 101]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series((full_dists[n_train:, :n_train] - test_dists).flatten()).describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fermat distnaces between train obss, with X=train osbs, must be >= than\n",
    "#   Fermat distances bw train obss, with X=(train obss + test obss)\n",
    "assert (train.distances_ >= full_dists[:n_train, :n_train]).all()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "preds = []\n",
    "fhats = []\n",
    "for x in X_test:\n",
    "    fhat = {}\n",
    "    for cls in classes.keys():\n",
    "        klass = classes[cls]\n",
    "        verts, dists = klass[\"verts\"], klass[\"dists\"]\n",
    "        n = verts.shape[0]\n",
    "        to_verts = euclidean_distances(x.reshape(1, -1), verts)[0] ** alpha\n",
    "        fmt_dists = [min(to_verts + dists[:, i]) for i in range(n)]\n",
    "        # print(cls, np.mean(fmt_dists))\n",
    "        fhat[cls] = (1 / h**D) * np.mean([kern(d / h) for d in fmt_dists])\n",
    "    fhats.append(fhat)\n",
    "    preds.append(pd.Series(fhat).argmax())\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.ensemble import RandomForestClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ".shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = load_digits(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.90)\n",
    "eucclf = EuclideanKDEClassifier().fit(X_train, y_train)\n",
    "fmtclf = FermatKDEClassifier(alpha=2, bandwith=200).fit(X_train, y_train)\n",
    "rfclf = RandomForestClassifier().fit(X_train, y_train)\n",
    "rfclf2 = RandomForestClassifier().fit(\n",
    "    np.concatenate([X_train, fmtclf.decision_function(X_train)], axis=1), y_train\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for clf in [eucclf, fmtclf, rfclf]:\n",
    "    print(repr(clf), \": \", accuracy_score(clf.predict(X_test), y_test))\n",
    "print(\n",
    "    repr(rfclf2),\n",
    "    \" - enhanced: \",\n",
    "    accuracy_score(\n",
    "        rfclf2.predict(\n",
    "            np.concatenate([X_test, fmtclf.decision_function(X_test)], axis=1)\n",
    "        ),\n",
    "        y_test,\n",
    "    ),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(eucclf.predict(X_test), y_test), accuracy_score(\n",
    "    fmtclf.predict(X_test), y_test\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[x.shape for x in [X_train, X_test, y_train, y_test]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp_softmax(D * 10, axis=1).round(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(np.argmax(preds, axis=1) == y_test).mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = load_iris(return_X_y=True)\n",
    "N = len(X)  # # observaciones\n",
    "K = len(set(y))  # # clases\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kde = KDEstimator()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kde.fit(X[y == 0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kde.predict(X[-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TemplateClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.utils.validation import check_X_y, check_array, check_is_fitted\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "from sklearn.metrics import euclidean_distances\n",
    "\n",
    "\n",
    "class TemplateClassifier(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, demo_param=\"demo\"):\n",
    "        self.demo_param = demo_param\n",
    "\n",
    "    def fit(self, X, y):\n",
    "\n",
    "        # Check that X and y have correct shape\n",
    "        X, y = check_X_y(X, y)\n",
    "        # Store the classes seen during fit\n",
    "        self.classes_ = unique_labels(y)\n",
    "\n",
    "        self.X_ = X\n",
    "        self.y_ = y\n",
    "        # Return the classifier\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "\n",
    "        # Check if fit has been called\n",
    "        check_is_fitted(self)\n",
    "\n",
    "        # Input validation\n",
    "        X = check_array(X)\n",
    "\n",
    "        closest = np.argmin(euclidean_distances(X, self.X_), axis=1)\n",
    "        return self.y_[closest]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_labels()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fermat",
   "language": "python",
   "name": "fermat"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
