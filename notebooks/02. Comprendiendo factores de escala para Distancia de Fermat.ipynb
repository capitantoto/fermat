{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02. Comprendiendo Factores de Escala\n",
    "`networkx` (o `igraph`, a tal fin) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.spatial import distance_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from fermat import Fermat\n",
    "from sklearn.datasets import load_iris\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KernelDensity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import random as rnd\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = rnd.rand(100, 3)\n",
    "A.sort(axis=1)\n",
    "A = np.column_stack([np.zeros_like(A[:, 0]), A, np.ones_like(A[:, 0])])\n",
    "a = A[0]\n",
    "a, A[:5,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fermat_dist(x, alpha=1):\n",
    "    \"\"\" Fermat alpha-distance between `x_0` and `x_k`, in the line graph with nodes at `x = (x_1, ..., x_k)`.\"\"\"\n",
    "    return ((x[1:] - x[:-1]) ** alpha).sum()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.apply_along_axis(fermat_dist, axis=1, arr=X, alpha=3).mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size = 1000\n",
    "scales = [1/10, 1/2, 1, 2, 10]\n",
    "alphas = np.linspace(1, 4, 7)\n",
    "ks = [1, 2, 5, 10]\n",
    "results = []\n",
    "for k in ks:\n",
    "    A = rnd.rand(sample_size, k)\n",
    "    A.sort(axis=1)\n",
    "    A = np.column_stack([np.zeros_like(A[:, 0]), A, np.ones_like(A[:, 0])])\n",
    "    for scale in scales:\n",
    "        for alpha in alphas:\n",
    "            results.append(\n",
    "                dict(\n",
    "                    k=k,\n",
    "                    alpha=alpha,\n",
    "                    scale=scale,\n",
    "                    dists=np.apply_along_axis(\n",
    "                        fermat_dist, axis=1, arr=scale * A, alpha=alpha\n",
    "                    ),\n",
    "                )\n",
    "            )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"mean_dist\"] = df.dists.apply(np.mean)\n",
    "df[\"scaled_dist\"] = df.mean_dist / (df.scale ** df.alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.k == 1].pivot(\"alpha\", \"scale\", \"mean_dist\").round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.k == 1].pivot(\"alpha\", \"scale\", \"scaled_dist\").round(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Está claro que si $c$ es la constance de escale `scale`, las distancias de fermat escalan según $c^{-\\alpha}$. Ahora, cuánto cambian con el tamaño de muestra $k$? \n",
    "\n",
    "Para $k=1$, se puede calcular exactamente la esperanza de la longitud del camino. Si hay un único punto entre 0 y 1 elegido al azar según $X \\sim \\text{Unif}(0, 1)$, entonces la longitud del camino de Fermat cuando $alpha=2$ será $E\\left(X^2 + (1-X)^2\\right) = 2/3$, lo cual se ve en la tabla anterior. Para $k=1$ y otros valores de $\\alpha$, la expresión no será tan bella pero es computable sin mucha dificultad. Para otros valores de $k$, sin embargo, ya entran en juego la distribución de los estadísticos de orden y no me resulta para nada evidente una fórmula cerrada.\n",
    "\n",
    "Aproximémosla. Sean $X^{(0)} = 0, X^{(k+1)} = 1$ y $X^{(i)}, i=1,\\dots,k$ las v.a. que surgen de ordenar una muestra $X_i \\sim_{iid} \\text{Unif}(0, 1), \\ i\\in[k]$. Para todo $k$ se cumple que cada \"segmentito de recta\", $\\mathbb{E}\\left(X^{(i+1)}-X^{(i)}\\right) = 1 / (k + 1)$. Luego, esperaríamos que \n",
    "\n",
    "$$\n",
    "\\begin{align} dist_{\\alpha}^k(0, 1) &= \\mathbb{E}\\ \\left(\\sum_{i=0}^k \\left[X^{(i+1)}-X^{(i)} \\right]^{\\alpha}\\right) \\\\\n",
    " &=  \\sum_{i=0}^k\\ \\left( \\mathbb{E}\\left[X^{(i+1)}-X^{(i)} \\right]^{\\alpha}\\right) \\\\\n",
    " &\\approx (???) (k + 1) \\left(\\frac{1}{k+1}\\right)^{\\alpha}\n",
    "   \\end{align}\n",
    "\n",
    "$$\n",
    "\n",
    "Pero no vale que $E(X^k) = E(X)^k$! Qué se hace en su lugar? Hay que conocer la densidad de $X^{(i+1)}-X^{(i)}$ y calcularlo? Empíricamente, veamos como cambia la distancia con $k$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.scale == 1].pivot(\"alpha\", \"k\", \"scaled_dist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"k_scaled_dist\"] = df.scaled_dist  * ((df.k + 2) ** (df.alpha - 1))\n",
    "df[df.scale == 1].assign().pivot(\"alpha\", \"k\", \"k_scaled_dist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size = 1000\n",
    "alphas = [1.5, 1.75, 2, 2.25, 3, 4, 5]\n",
    "ks = np.array([*range(1, 11), *rnd.choice(range(11, 2001), 50, replace=False)])\n",
    "\n",
    "results = []\n",
    "for k in ks:\n",
    "    A = rnd.rand(sample_size, k)\n",
    "    A.sort(axis=1)\n",
    "    A = np.column_stack([np.zeros_like(A[:, 0]), A, np.ones_like(A[:, 0])])\n",
    "    for alpha in alphas:\n",
    "        results.append(\n",
    "            dict(\n",
    "                k=k,\n",
    "                alpha=alpha,\n",
    "                dists=np.apply_along_axis(\n",
    "                    fermat_dist, axis=1, arr=A, alpha=alpha\n",
    "                ),\n",
    "            )\n",
    "        )\n",
    "\n",
    "df = pd.DataFrame(results)\n",
    "df[\"mean_dist\"] = df.dists.apply(np.mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.k < 50].pivot(\"alpha\", \"k\", \"mean_dist\").round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 7))\n",
    "sns.lineplot(x=\"k\", y=\"mean_dist\", hue=\"alpha\", data=df, marker=\"o\")\n",
    "plt.xscale(\"log\")\n",
    "plt.yscale(\"log\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para la elección del ancho de banda $h$ es necesario saber la escala de las distancias! Con qué se come esto???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for i in range(10000):\n",
    "    s = rnd.rand(100)\n",
    "    s.sort()\n",
    "    dists = (np.array([s[0], *(s[1:] - s[:-1]), 1 - s[-1]])) ** 2\n",
    "    results.append((dists.mean(), dists.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean([std / mean for mean, std in results])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementación KDEClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "from numpy.linalg import norm as euclidean_norm\n",
    "from scipy.spatial import distance_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "scipy.spatial.distance.minkowski(A[0], A[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repr(fermat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B = range(5)\n",
    "C = range(7)\n",
    "[[b * c for c in C] for b in B]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(A[:] == A[2]).all(1)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A.shape, np.zeros(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WIP - ignore\n",
    "def euclidean_distance(x, y):\n",
    "    return euclidean_norm(x - y)\n",
    "\n",
    "\n",
    "class FermatDistance(Fermat):\n",
    "    def __init__(self, **kwargs):\n",
    "        super.__init__(**kwargs)\n",
    "\n",
    "    def _fit(self, X):\n",
    "        self.fit(X)\n",
    "        self.X_ = X\n",
    "        self.n_, self.d_ = X.shape\n",
    "        self.is_fitted_ = True\n",
    "        return self\n",
    "\n",
    "    def __call__(A, B):\n",
    "        if self.alpha == 1:\n",
    "            return distance_matrix(A, B)\n",
    "        else:\n",
    "            return [[self._get_distance(a, b) for a in A] for b in B]\n",
    "\n",
    "    def _get_distance(a, b):\n",
    "        if not self.is_fitted_:\n",
    "            self._fit()\n",
    "        if any((self.X_[:] == a).all(1)):  # `a` is a node from X_\n",
    "            to_X = np.zeros(self.n_)\n",
    "        else:\n",
    "            to_X = euclidean_distances(a.reshape(1, -1), verts)[0] ** self.alpha\n",
    "\n",
    "        if a_known := np.where((X[:] == 2 * X[2]).all(1)):\n",
    "            to_nodes = euclidean_distances(a.reshape(1, -1), verts)[0] ** alpha\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_pdf = scipy.stats.norm.pdf\n",
    "\n",
    "class EuclideanKDE:\n",
    "    def __init__(self, kernel=norm_pdf, bandwith=1):\n",
    "        self.kernel = kernel\n",
    "        self.bandwith = bandwith\n",
    "\n",
    "    def fit(self, X):\n",
    "        self.X_ = X\n",
    "        self.n_, self.dim_ = X.shape\n",
    "        return self\n",
    "\n",
    "    def density(self, X, log=True):\n",
    "        densities = (self.bandwith**-self.dim_) * self.kernel(\n",
    "            distance_matrix(X, self.X_) / self.bandwith\n",
    "        ).mean(axis=1)\n",
    "        return np.log(densities) if log else densities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.utils.validation import check_X_y#, check_array, check_is_fitted\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "from scipy.special import softmax\n",
    "\n",
    "\n",
    "class EuclideanKDEClassifier(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, kernel=norm_pdf, bandwith=1):\n",
    "        self.kernel = kernel\n",
    "        self.bandwith = bandwith\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # Check that X and y have correct shape\n",
    "        X, y = check_X_y(X, y)\n",
    "        # Store the classes seen during fit\n",
    "        self.classes_ = unique_labels(y)\n",
    "        self.X_ = X\n",
    "        self.y_ = y\n",
    "        self.densities_ = {\n",
    "            cls: EuclideanKDE(self.kernel, self.bandwith).fit(X[y == cls])\n",
    "            for cls in self.classes_\n",
    "        }\n",
    "        return self\n",
    "\n",
    "    def decision_function(self, X):\n",
    "        return np.column_stack(\n",
    "            [self.densities_[cls].density(X) for cls in self.classes_]\n",
    "        )\n",
    "\n",
    "    def predict(self, X):\n",
    "        D = self.decision_function(X)\n",
    "        return self.classes_[np.argmax(D, axis=1)]\n",
    "\n",
    "    def predict_proba(self, X, temp=1):\n",
    "        D = self.decision_function(X)\n",
    "        return softmax(D / temp, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FermatKDE:\n",
    "    def __init__(self, kernel=norm_pdf, bandwith=1, alpha=2, path_method=\"FW\"):\n",
    "        self.kernel = kernel\n",
    "        self.bandwith = bandwith\n",
    "        self.alpha = alpha\n",
    "        self.path_method = path_method\n",
    "\n",
    "    def fit(self, X):\n",
    "        self.X_ = X\n",
    "        self.n_, self.dim_ = X.shape\n",
    "        self.fermat_ = Fermat(self.alpha, self.path_method).fit(distance_matrix(X, X))\n",
    "        self.distances_ = self.fermat_.get_distances()\n",
    "        return self\n",
    "\n",
    "    def density(self, X, log=True):\n",
    "        densities = (self.bandwith**-self.dim_) * self.kernel(\n",
    "            self.distances(X) / self.bandwith\n",
    "        ).mean(axis=1)\n",
    "        return np.log(densities) if log else densities\n",
    "\n",
    "    def distances(self, X):\n",
    "        distances_to_X = distance_matrix(X, self.X_) ** self.alpha\n",
    "        return np.vstack([(d + self.distances_).min(axis=1) for d in distances_to_X])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import softmax\n",
    "\n",
    "\n",
    "class FermatKDEClassifier(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, kernel=norm_pdf, bandwith=1, alpha=2, path_method=\"FW\"):\n",
    "        self.kernel = kernel\n",
    "        self.bandwith = bandwith\n",
    "        self.alpha = alpha\n",
    "        self.path_method = path_method\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # Check that X and y have correct shape\n",
    "        X, y = check_X_y(X, y)\n",
    "        # Store the classes seen during fit\n",
    "        self.classes_ = unique_labels(y)\n",
    "        self.X_ = X\n",
    "        self.y_ = y\n",
    "        self.densities_ = {\n",
    "            cls: FermatKDE(\n",
    "                self.kernel, self.bandwith, self.alpha, self.path_method\n",
    "            ).fit(X[y == cls])\n",
    "            for cls in self.classes_\n",
    "        }\n",
    "        return self\n",
    "\n",
    "    def decision_function(self, X):\n",
    "        return np.column_stack(\n",
    "            [self.densities_[cls].density(X) for cls in self.classes_]\n",
    "        )\n",
    "\n",
    "    def predict(self, X):\n",
    "        D = self.decision_function(X)\n",
    "        return self.classes_[np.argmax(D, axis=1)]\n",
    "\n",
    "    def predict_proba(self, X, temp=1):\n",
    "        D = self.decision_function(X)\n",
    "        return softmax(D / temp, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.array([[i] * 2 for i in range(3)])\n",
    "Fermat(2, \"FW\").fit(distance_matrix(A, A)).get_distances()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B = np.vstack([[-1, 0], [1, 0]])\n",
    "B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = np.vstack([A, B])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full = Fermat(2, \"FW\").fit(distance_matrix(C, C))\n",
    "full_dists = full.get_distances()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train = 120\n",
    "train = FermatKDE(2, \"FW\").fit(A)\n",
    "test_dists = train.distances(B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_dists, test_dists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full = Fermat(2, \"FW\").fit(distance_matrix(X, X))\n",
    "full_dists = full.get_distances()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train = 120\n",
    "train = FermatKDE(2, \"FW\").fit(X[:n_train])\n",
    "test_dists = train.distances(X[n_train:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.where(full_dists[n_train:,:n_train] > test_dists + 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_dists[n_train:,:n_train][22, 101],  test_dists[22, 101]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series((full_dists[n_train:,:n_train] - test_dists).flatten()).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fermat distnaces between train obss, with X=train osbs, must be >= than\n",
    "#   Fermat distances bw train obss, with X=(train obss + test obss)\n",
    "assert (train.distances_ >= full_dists[:n_train, :n_train]).all() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "preds = []\n",
    "fhats = []\n",
    "for x in X_test:\n",
    "    fhat = {}\n",
    "    for cls in classes.keys():\n",
    "        klass = classes[cls]\n",
    "        verts, dists = klass[\"verts\"], klass[\"dists\"]\n",
    "        n = verts.shape[0]\n",
    "        to_verts = euclidean_distances(x.reshape(1, -1), verts)[0] ** alpha\n",
    "        fmt_dists = [min(to_verts + dists[:, i]) for i in range(n)]\n",
    "        # print(cls, np.mean(fmt_dists))\n",
    "        fhat[cls] = (1 / h**D) * np.mean([kern(d / h) for d in fmt_dists])\n",
    "    fhats.append(fhat)\n",
    "    preds.append(pd.Series(fhat).argmax())\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ".shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = load_digits(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.90)\n",
    "eucclf = EuclideanKDEClassifier().fit(X_train, y_train)\n",
    "fmtclf = FermatKDEClassifier(alpha=2, bandwith=200).fit(X_train, y_train)\n",
    "rfclf = RandomForestClassifier().fit(X_train, y_train)\n",
    "rfclf2 = RandomForestClassifier().fit(np.concatenate([X_train, fmtclf.decision_function(X_train)], axis=1), y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for clf in [eucclf, fmtclf, rfclf]:\n",
    "    print(repr(clf), \": \", accuracy_score(clf.predict(X_test), y_test))\n",
    "print(repr(rfclf2), \" - enhanced: \", accuracy_score(rfclf2.predict(np.concatenate([X_test, fmtclf.decision_function(X_test)], axis=1)), y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(eucclf.predict(X_test), y_test), accuracy_score(fmtclf.predict(X_test), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[x.shape for x in [X_train, X_test, y_train, y_test]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp_softmax(D * 10, axis=1).round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(np.argmax(preds, axis=1) == y_test).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = load_iris(return_X_y=True)\n",
    "N = len(X)  # # observaciones\n",
    "K = len(set(y))  # # clases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kde = KDEstimator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kde.fit(X[y == 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kde.predict(X[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TemplateClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.utils.validation import check_X_y, check_array, check_is_fitted\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "from sklearn.metrics import euclidean_distances\n",
    "class TemplateClassifier(BaseEstimator, ClassifierMixin):\n",
    "\n",
    "    def __init__(self, demo_param='demo'):\n",
    "        self.demo_param = demo_param\n",
    "\n",
    "    def fit(self, X, y):\n",
    "\n",
    "        # Check that X and y have correct shape\n",
    "        X, y = check_X_y(X, y)\n",
    "        # Store the classes seen during fit\n",
    "        self.classes_ = unique_labels(y)\n",
    "\n",
    "        self.X_ = X\n",
    "        self.y_ = y\n",
    "        # Return the classifier\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "\n",
    "        # Check if fit has been called\n",
    "        check_is_fitted(self)\n",
    "\n",
    "        # Input validation\n",
    "        X = check_array(X)\n",
    "\n",
    "        closest = np.argmin(euclidean_distances(X, self.X_), axis=1)\n",
    "        return self.y_[closest]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_labels()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fermat",
   "language": "python",
   "name": "fermat"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
